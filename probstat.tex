\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\title{Thinking like a Frequentist
}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
\usepackage{blindtext}
\usepackage{mathrsfs}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{epigraph} 
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\author{Student}
\date{January 2026}
\begin{document}
\maketitle
\newpage
\begin{center}
	\textit{This page is intentionally left blank.}
\end{center}
\chapter*{Preface}
\epigraph{\textit{"Are you watching closely?"}}{Alfred Borden, The Prestige (2006)}
When I was learning Probability and Statistics (ProbStat for short), I didn't really understand why things were distributed \textit{normally}. Were they \textit{normal} because they were, or were \textit{we just assuming "Everything is normal for simplicity"}? Sometimes, I felt we were overusing this term and in many situations, we were separating our theory from the actual data due to the intital assumption of \textit{normal} distribution.
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{1.jpg}
	\caption{Distribution of Math scores in the 2021 National Entrance Exam}
	\label{fig:galaxy}
\end{figure}
\\ As you can see here, this graph \textbf{doesn't really resemble a bell-shaped curve}; and clearly, this data is not distributed normally. In fact, not much real-world statistical data fits a bell-shaped curve. But why are we still using it?
\\ In my opinion (as a junior CE student), I think the main reason is due to \textbf{Central Limit Theorem} (CLT for short), every sample in the same population (regradless of how the data is distributed) has a \textbf{mean} that converges to a \textit{normal distribution}, or \textbf{bell-shaped curve}. This topic will be explained in detail in \textbf{Chapter 8: Fundamentals of Statistics}.\\ \\  \\
\indent This book approaches the fundamentals of probability and statistics in a very mathematically rigourous way to ensure maximum accuracy. I encourage readers to prove all of theorems, corollaries; and you should create your own examples for each theorem to gain a deeper understading of their origins.
\begin{flushright}
	---Student---
\end{flushright}
\chapter*{Acknowledgements}
First of all, I want to thank you for reading this book. Although I am not a native English speaker, but I truly enjoy writing in English. I have tried my best to express my ideas in English, but minor grammatical or spelling errors are unavoidable. If you find any of them, I would be very happy to receive your feedback via my email address:
\url{tinvu1309@gmail.com}
\\ \\ \\
\indent Secondly, I am extremly grateful and would like to express my thanks to the authors of the textbook "Probability and Statistics for Engineers and Scientists", $9$th edition. This book truly saved my student life.
\\ \\ \\
\indent Finally, the idea of writing this book was inspired by Prof. Steve Brunton lectures on YouTube. You should check out his videos too!
\begin{flushright}
	---Student---
\end{flushright}
\tableofcontents
\listoffigures
\newpage
\chapter*{List of notations}
Since much of my work is handwritten, so I've modified some commonly used notations for probability distribution funcions using curved script for convenience. You should notice that my conventions are not the international standards.
\begin{enumerate}
	\item $\mathscr{I}$: Bernoulli distribution
	\item $\mathscr{B}$: Binomial distribution
	\item $\mathscr{B}^*$: Negative binomial distribution
	\item $\mathscr{H}$: Hypergeometric distribution
	\item $\mathscr{G}^*$: Geometric distribution
	\item $\mathscr{P}$: Poisson distribution
	\item $\mathscr{N}$: Normal distribution
	\item $\mathscr{G}$: Gamma distribution
	\item $\mathscr{E}$: Exponential distribution
	\item $\mathscr{C}$: Chi-squared distributon
	\item $\mathscr{T}$: t-distribution (or Student distribution)
\end{enumerate}
\chapter{Introduction to Probability and Statistics}
\section{Tossing a coin}
Imagine you have a coin, like this one. It's an ordinary coin that can be found everywhere.
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{2.jpg}
	\caption{Tossing a coin}
	\label{Figure 2}
\end{figure}
\\Everyone knows that the chance of heads (H) appearing after each toss is $50\%$, no one even doubts that. So, the \textbf{probability} of an \textbf{event} that heads appearing is $50\%$. But because you're a very curious person, you don't easily accept the truth like others, so you will find a way to test it. The fact "The probability of heads appearing is $50\%$" will be tested, and it can be called a \textbf{hypothesis}.
\\ \\ \\
\indent The simplest way to test your hypothesis is tossing a coin many times. You might toss a coin $N=100$ times and see heads appear $n=40$ times, so your actual probability is: $$p=\frac{n}{N}=0.4$$
But your previous assumption (or hypothesis) states that the ideal probability is: $$\hat p=0.5$$
Is there anything wrong here? Dissatisfied, you continue your experiment. This time, you toss a coin $N=500$ times and see heads appear $n=270$ times, so the new actual probability is:
$$p=\frac{n}{N}=0.54$$
This time the result is closer with intial hypothesis $\hat p$, but your hand now must be very tired after tossing a coin $600$ times. So do you think you have \textbf{enough} evidence to conclude that if $N\to+\infty$, then $p\to\hat p$? If so, your assumption is correct because \textit{can't be refuted}; and if not, our common sense might be wrong because the evidence \textit{strongly refutes it}.
\\ \\ \\
\indent Another strategy to toss a coin is fixing and dividing $N$. Instead of tossing $N=600$ times and only getting the final $p$ value, you can divide large number of coin tosses $N=600$ to smaller tosses $N_{1}=N_{2}=\cdots=N_{6}=100$ (but don't too small, at least each $N_{i}>30$), and get
6 values of $p_{i}$. Suppose that you would get:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$p_{1}$ & $p_{2}$ & $p_{3}$ & $p_{4}$ & $p_{5}$ & $p_{6}$ \\
		\hline
		0.43    & 0.46    & 0.56    & 0.53    & 0.49    & 0.51    \\
		\hline
	\end{tabular}
\end{center}
You might see that the value $\hat p=0.5$ too idealistic to occur in our experiment, and $p$ constantly changes. Therefore instead of determining the \textbf{exact value} of $p$, we \textbf{predict} that the $p$ value lies inside a \textbf{closed interval} with a \textbf{certainty} of $(1-\alpha)100\%$. For example, we can confidently conclude that the value of $p$ lies within the range (0.48,0.51) with $95\%$ accuracy.
\section{Weather forecasting}
You don't need to know anything about geography to predict what the weather will be tomorrow. Everything you need is just knowledge about Poisson, exponential distribution and the \textbf{average number} of rainy days per month where you are living.
\\ For example, in December there are \textbf{average} 5 rainy days. What are the probabilities of:
\begin{enumerate}
	\item There will be 3 rainy days this month.
	\item Tomorrow will be a rainy day, if today is the $10$th and you haven't seen any rain since the beginning of the month.
\end{enumerate}
This problem will be covered in detail in \textbf{Chapter 5: Discrete Random Variables} and \textbf{Chapter 6: Continuous Random Variables}, but if you do have experience with random variables, you can try solving it!
\\ The answer for the first question is:
\begin{equation*}
	P(X=3)=\frac{e^{-5}5^3}{3!}\approx 14.03\%
\end{equation*}
\\ The answer for the second question is:
\begin{equation*}
	P(X<11|X>10)=1-P(X>11|X>10)=1-e^{\frac{-5}{31}}\approx 14.89\%
\end{equation*}
Since $14.89\%$ is quite low, so tomorrow you don't have to bring an umbrella.
\section{Relationship between Probability and Statistics}
The formal definitions of \textbf{probability} and \textbf{statistics} will be discussed later in \textbf{Chapter 2: Fundamentals of Probability} and \textbf{Chapter 8: Fundamentals of Statistics}, but now let's focus on the general model below:
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=15cm]{3.jpg}
	\caption{General model of simple ProbStat problems}
	\label{Figure 3}
\end{figure}
You want to know some attributes of a very large \textbf{population} such as the probabilites, means,$\cdots$ of the quantities. But there's no way you can observe the entire population (in many cases, $N\to +\infty$)
, so you have to take some \textbf{samples} ($n$ elements) and use \textbf{probability rules} to process them. After that, you can try applying \textbf{statistical inference rules} to draw conclusions about the original population. That's how it works!
\\ \\ \\
\indent Referring to the previous example, you want to check if the probability of heads appearing is $\hat p=50\%$, so you toss a coin multiple times. But since you can only toss a coin for $600$ times consecutively, so you conclude based on your final result $p=0.54$ (or closed interval with a certainty) that might be if $N\to +\infty$ then $p\to \hat p=0.5$. You took the \textbf{samples} ($N=600$ or $N_{i}=100$), used \textbf{probability rule} to calculate the actual $p$ value, and then applied \textbf{statistical inference rules} (hypothesis or closed interval) to draw conclusions respectively.
\chapter{Fundamentals of Probability}
\section{Sample Space and Events}
\subsection{Sample Space}
If you toss a coin, you will see that there are $2$ possible outcomes: heads and tails, denoted by capital letters H and T; or if you roll a die, you will see that there are $6$ possible outcomes, from $1$ to $6$. Tossing a coin, or rolling a die are typical examples of \textbf{experiments}.
\begin{definition}
	An \textbf{experiment} is the process that generates a set of outcomes (or data).
\end{definition}
All of possible outcomes are collected into a single set.
$$S_{1}=\{H,T\}$$
$$S_{2}=\{1,2,3,4,5,6\}$$
\begin{definition}
	The \textbf{sample space} is the set of all possible outcomes of an \textbf{experiment}.
\end{definition}
The sample space is usually represented by the symbol $S$ or $\Omega$. You can easily see that $S$ is not always a countable or finite set. For instance, $S_{3}$ is the set of all random numbers you can choose within the range $(0,1)$:
$$S_{3}=\{x\;|\;0<x<1\}$$
$S_{4}$ is the set of all number of coin tosses until first heads appear:
$$S_{4}=\{1,2,3,\cdots\}$$
It's possible that you toss a coin forever, and heads never appear.
\begin{definition}
	A \textbf{sample point} is a single outcome of the sample space $S$.
\end{definition}
$S_{1},S_{2}$ have $2$ and $6$ sample points respectively, while $S_{3},S_{4}$ have infinite sample points.
\subsection{Events}
For any given experiment, we are often interested in the occurence of certain \textbf{events} rather than a specific element (or \textbf{sample point}) in the sample space. For example, in the die roll experiment, you may want to know when the outcome is an even number. This will happen if the result is an element of the subset $E_{2}$ of the sample space $S_{2}$:
$$E_{2}=\{2,4,6\}$$
\begin{definition}
	An \textbf{event} is the subset of a sample space.
\end{definition}
Events are always denoted by capital letters like $A,B,C,\cdots$. Similarly, an event is not always a countable or finite subset.
\\ The \textbf{complement} of an event $E_{2}$ with respect to $S_{2}$ is the set of all \textit{odd outcomes}, and can be represented as follows: $$\overline{E}_{2}=\{1,3,5\}$$
There are many ways to denote the \textbf{complement} of an event: $\overline{E},E',E^c$, but in this book I choose overline notation for convenience and make it easy to relate with Boolean algebra.
\begin{definition}
	The \textbf{complement} of an event $E$ with respect to $S$ is the subset of all elements of $S$ that are not in $E$, and can be denoted by the symbol $\overline{E}$.
\end{definition}
Applying set theory, we can perform many set operations like joint, disjoint, union,$\cdots$
\begin{definition}
	The \textbf{intersection} of two events $A$ and $B$, denoted by the symbol $A\cap B$ is the event containing all elements that are common to $A$ and $B$.
\end{definition}
For example, if $A$ and $B$ are the subset of $S_{3}$ and defined as:
\begin{equation*}
	\begin{split}
		A&=\{x\;|\;0<x<0.7\}\\
		B&=\{x\;|\;0.2<x<0.9\}\\
		\Rightarrow A\cap B&=\{x\;|\;0.2<x<0.7\}
	\end{split}
\end{equation*}
\begin{definition}
	Two events $A$ and $B$ are \textbf{mutually exclusive}, or \textbf{disjoint}, if $A\cap B=\varnothing$, that is $A$ and $B$ have nothing in common.
\end{definition}
For example, $E_{2}$ and $\overline{E}_{2}$ are mutually exclusive.
\begin{definition}
	The \textbf{union} of the two events $A$ and $B$, denoted by the symbol $A\cup B$ is the event containing all of the elements that belong to $A$ or $B$ or both.
\end{definition}
For example, $A\cup B=\{x\;|\;0<x<0.9\}$, and $(A\cup B)\subset S_{3}$. Note that $E_{2}\cup\overline{E}_{2}=S_{2}$ is an useful result and can be generalized to corollary below:
\begin{corollary}
	If $A$ is an event with respect to sample space $S$, then $A\cup\overline{A}=S$
\end{corollary}
De Morgan's laws can also be applied to set theory.
\begin{corollary}
	If $A$ and $B$ are events with respect to sample space $S$, then $$\overline{A\cap B}=\overline{A}\cup\overline{B}$$ $$\overline{A\cup B}=\overline{A}\cap\overline{B}$$
\end{corollary}
These results above are really useful in many cases, but you don't have prove them since they are pretty simple. Proving them rigourously is not the focus of Probability and Statistics book.
\section{Counting Sample Points}
In this section, we develop some \textit{counting techniques} to count the number of points in the sample space $S$ and its event subset $E$ without actually listing each elements. These techniques play an important role in solving some simple probability problems. Notice that these techniques can only be applied when your sample space is finite and countable.
\\ Obviously, you can't count the number of elements of $S_{3}$ and $S_{4}$, since they are infinite and uncountable sets.
\subsection{Rule of Product}
You toss a pair of coins. How many sample points are there in the sample space? First you can try to list all of the possible outcomes: $$S=\{HH,HT,TH,TT\}$$
There are total $4$ sample points in $S$. But listing all of the elements might be not so clever idea, so you use \textbf{rule of product}.
\\ The first coin can land heads or tails, so can the second coin. We multiply the number of possible outcomes of the two coins:
$$n_{1}n_{2}=2.2=4\text{ (possible outcomes)}$$
\begin{definition}
	If an operation can be performed in $n_{1}$ ways, and if for each of these a second operation can be performed in $n_{2}$ ways, and for each of the first two a third operation can be performed in $n_{3}$ ways, and so fourth, the the sequence of $k$ operations can be performed in:
	$$\prod_{i=1}^k n_{i}\text{ (ways)}$$
\end{definition}
\begin{figure}[h]
	\centering
	\includegraphics[width=13cm]{4.jpg}
	\caption{Tree diagram for rule of product}
	\label{Figure 4}
\end{figure}
\subsection{Permutations}
\begin{definition}
	A \textbf{permutation} is an arrangement of \textbf{all} or \textbf{part} of a set of objects.
\end{definition}
For instance, the number of permutations of $n$ distinct objects can be counted by using rule of product:
$$n(n-1)(n-2)\cdots3.2.1$$
We introduce a new notation for such a number.
\begin{definition}
	For any non-negative integer $n$, $n!$, called "$n$ factorial", is defined as:
	$$n!=n(n-1)(n-2)\cdots 3.2.1$$
	with special case $0!=1$
\end{definition}
\begin{theorem}
	The number of \textbf{permutations} of $n$ distinct objects is $n!$
\end{theorem}
In general, the number of permutations of $n$ distinct objects taken $r$ at a time can also be counted by using rule of product:
$$n(n-1)(n-2)\cdots(n-r+2)(n-r+1)$$
This product can be represented by the new symbol $nPr$.
\begin{theorem}
	The number of \textbf{permutations} of $n$ distinct objects taken $r$ at a time is:
	$$nPr=\frac{n!}{(n-r)!}$$
\end{theorem}
But how about our $n$ objects are not distinct? Assume that $n_{1}$ are of one kind, $n_{2}$ are of a second kind,$\cdots$ and $n_{k}$ of a $k$th kind.
\begin{theorem}
	The number of \textbf{permutations} of $n$ things of which $n_{1}$ are of one kind, $n_{2}$ of a second kind, and so forth is:
	$$\frac{n!}{n_{1}!n_{2}!\cdots n_{k}!}$$
	with $\sum_{i=1}^{k}n_{i}=n$.
\end{theorem}
For example, from the digits $1$ to $9$, we can form $9!$ numbers made up of $9$ distinct digits, or we can form $9P5$ five-digit numbers such that all the digits are different. If we allow repetation, five-digits numbers are now formed by $2$ digits 1, $2$ digits 2 and $1$ digit 3, then the number of permutation that satisfy is:
$$\frac{5!}{2!2!1!}=30$$
\subsection{Combinations}
Consider another problem, now you have a set of $n$ elements. How many ways you can partition it into $k$ cells with $n_{1}$ elements in the first cell, $n_{2}$ elements in the second cell, and so forth? Coincidentally, the equation in \textbf{Theorem 2.2.3} appears again.
\begin{theorem}
	The number of ways of partitioning a set of $n$ objects into $k$ cells with $n_{1}$ elements in the first cell, $n_{2}$ elements in the second, and so forth, is:
	$$\binom{n}{n_{1},n_{2},\cdots,n_{k}}=\frac{n!}{n_{1}!n_{2}!\cdots n_{k}!}$$
	with $\sum_{i=1}^{k}n_{i}=n$.
\end{theorem}
In many problems, we are interested in the number of ways of selecting $k$ objects from $n$ without regrad to order. These selections are called \textbf{combinations}. It's not too hard to realize that a \textbf{combination} is just a partition with $2$ cells, one cell containing the $k$ objects and the other containing the $(n-k)$ objects.
\begin{equation*}
	\binom{n}{k,n-k} \text{ is often shortened to } \binom{n}{k}
\end{equation*}
\begin{definition}
	A \textbf{combination} is a selection of items from a set that has distinct elements, such that \textbf{the order of selection does not matter}.
\end{definition}
\begin{theorem}
	The number of combinations of $n$ distinct objects taken $k$ at a time is:
	$$\binom{n}{k}=\frac{n!}{k!(n-k)!}$$
\end{theorem}
For example, the number of subsets of $3$ elements that can be obtained from an original set of $10$ elements is: $$\binom{10}{3}=120$$
For the rest of this book, we mainly focus on \textbf{combinations}, and derive many probability distribution functions based on them. Like set theory, we don't go deeply into \textbf{counting techniques} since they are not the main concern of ProbStat.
\section{Probability of an Event}
Everyone knows the basic idea of probability. If I toss a \textbf{fair} coin once, and I want to know the probability of getting heads; how can I determine it? Strictly, I have to define $S$ is the sample space of this experiment and $A$ is the event "Getting heads after one toss".
$$S=\{H,T\}$$
$$A=\{H\}$$
So, the probability of event $A$ occurring is:
$$P(A)=\frac{\text{number of sample points inside } A}{\text{number of sample points inside } S}=\frac{1}{2}$$
Very easy and intuitive. But how about tossing 4 \textbf{fair} coins once, and determining the probability of getting $2$ heads? Now sample space $S$ and its event set can be represented as:
$$S=\{HHHH,HHHT,HHTH,\cdots\}$$
$$A=\{TTHH,THTH,\cdots\}$$
Now we change our strategy using \textbf{counting techniques}:
$$P(A)=\frac{\text{number of sample points inside } A}{\text{number of sample points inside } S}=\frac{\binom{4}{2}}{2^4}=\frac{3}{8}=0.375$$
In fact, the chance of getting heads is slightly greater than tails (you know, because coins are asymmetrical). Assume that probability of heads appearing is $60\%$, and tails appearing is $40\%$. Since the role of \textbf{sample points} inside set $S$ and subset $A$ are not \textbf{equal} anymore, so we can't use \textbf{counting techniques} blindly.
$$P(A)=\binom{4}{2}0.6^2 0.4^2=0.3456$$
Coin tossing is a typical example of \textbf{Bernoulli trial}, and the experiment tossing unfair coins is \textbf{Bernoulli process}. The probability $P(A)$ can be calculated by using \textbf{Binomial distribution} formula. You don't have to worry about these terms, we will cover them in \textbf{Chapter 5: Discrete Random Variables} very carefully.
\\ \\ \\
\indent Consider another problem, what's the probability of getting $0.7$ when randomly choosing a number (assume that the role of every numbers are equal) inside the interval $(0,1)$?
$$S=\{x\;|\;0<x<1\}$$
$$A=\{0.7\}$$
$$P(A)=\frac{\text{number of sample points inside } A}{\text{number of sample points inside } S}=\frac{1}{+\infty}=0\;(?????)$$
Since $P(A)=0$, so may we conclude 0.7 will never be chosen? Absolutely incorrect, even in common thinking. From previous examples, now we see the \textbf{limitation} of our "common definition" of probability in real life. Mathematically, our definition is just a very special case of the formal one.
\begin{definition}
	The \textbf{probability} of an event $A$ is the sum of the weights (or probabilities) of all sample points in $A$. Therefore:
	\begin{equation*}
		0\leq P(A)\leq 1,\quad P(\varnothing)=0,\quad P(S)=1
	\end{equation*}
	If $A$ and $B$ are \textbf{mutually exclusive (or disjoint)}, then $P(A\cup B)=P(A)+P(B)$
\end{definition}
This definition can also be called Kolmogorov's axioms. An intuitive way to understand it is sketching a sample space with some events inside.
\begin{figure}[h]
	\centering
	\includegraphics[width=14cm]{5.jpg}
	\caption{Visualize definition of probability}
	\label{Figure 5}
\end{figure}
\\Literally, probability is just a number that describes how likely an event can occur. I often relate it with "weight" quantity. As you can see in the sample space, I can assign arbitrarily the "weights" (or probabilites) of sample points as follows:
\begin{itemize}
	\item Each blue point is $0.1$
	\item Each orange point is $0.2$
	\item Each pink point is $0.066$
\end{itemize}
Using \textbf{Definition 2.3.1}, now we can obtain these resuls:
$$P(A)=3.0.2=0.6$$
$$P(B)=2.0.1=0.2$$
$$P(A\cup B)=P(A)+P(B)=0.6+0.2=0.8$$
$$P(\overline{A\cup B})=3.0.066\approx 0.2$$
$$P(S)=2.0.1+3.0.2+3.0.066\approx 1$$
You should verify that our "special definition" of probabilty above satisfies the axioms of probability, and can be formalized by a theorem.
\begin{theorem}
	If an experiment can result in any one of $N$ different \textbf{equally} likely outcomes, and if exactly $n$ of these outcomes correspond to event $A$, then the probability of event $A$ is:
	$$P(A)=\frac{n}{N}$$
\end{theorem}
Logically, you can view probability as a mapping from a set to a closed interval $[0,1]$, then you can freely define the mapping $P$ by yourself as long as it satisfies Kolmogorov's axioms.
\begin{equation*}
	\begin{split}
		P:\text{Set}&\to[0,1]\\
		A&\xrightarrow{P} P(A)
	\end{split}
\end{equation*}
By applying set theory, we can derive several extremely useful theorems and corollaries:
\begin{theorem}
	If $A$ and $B$ are two events, then:
	$$P(A\cup B)=P(A)+P(B)-P(A\cap B)$$
\end{theorem}
\begin{theorem}
	If $A$ is an event of sample space $S$, then:
	$$P(S)=P(A\cup\overline{A})=P(A)+P(\overline{A})=1$$
\end{theorem}
\begin{corollary}
	If A is an event of sample space $S$, then:
	$$P(\overline{A})=1-P(A)$$
\end{corollary}
Again, De Morgan's laws can also be applied:
\begin{corollary}
	If $A$ and $B$ are events of sample space $S$, then:
	\begin{equation*}
		\begin{split}
			P(\overline{A\cup B})&=P(\overline{A}\cap\overline{B})\\
			P(\overline{A\cap B})&=P(\overline{A}\cup\overline{B})\\
		\end{split}
	\end{equation*}
\end{corollary}
\section{Conditional Probability}
\subsection{Conditional Probability}
Imagine you now have a perfectly fair coin. Obviously if you define events $A$ and $B$ are heads and tails appearing after one toss, respectively, you will conclude: $$P(A)=P(B)=0.5$$
But I might ask you "If event $A$ \textbf{did} occur, might event $B$ would have any chance to occur?"
. Since there's no way heads and tails can simultaneously appear, so your answer must be "No!". Now we can form an equation to represent the probability of a "special event" (or formally, conditional event) that "If $A$ occured, then $B$ would occur.":
$$P(B|A)=0$$
Now we shift our attention to a more complex problem. I give you a die, and you roll it. Events $E_{1}$ and $E_{2}$ can be defined as: "Landing a number that greater than 3" and "Landing a number that divisible by 2":
$$P(E_{1})=P(E_{2})=\frac{1}{3}$$
Sample points of $2$ events can be listed: $$E_{1}=\{4,5,6\}$$ $$E_{2}=\{2,4,6\}$$
If a number greater than 3 landed, what's the probability that it could be divisible by 2? You can count the number of sample points inside $E_{1}$ and draw a result:
$$P(E_{2}|E_{1})=\frac{2}{3}$$
Conversely, if a number divisible by 2 landed, what's the probability that it could be greater than $3$?
$$P(E_{1}|E_{2})=\frac{2}{3}$$
As you can see here, if one event occurs before another event, probability will be completely \textbf{changed}. So we call the pairs of events $A$ and $B$, $E_{1}$ and $E_{2}$ \textbf{dependent events} because they depend on each other. The "special" probabilities $P(B|A), P(E_{2}|E_{1}),P(E_{1}|E_{2})$ are called \textbf{conditional probability}.
\begin{definition}
	The \textbf{conditional probability} of $B$, given $A$, denoted by $P(B|A)$, is defined:
	$$P(B|A)=\frac{P(A\cap B)}{P(A)}$$
\end{definition}
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=13cm]{6.jpg}
	\caption{Visualize how conditional probability is calculated}
	\label{Figure 6}
\end{figure}
Now back to our previous problems, these conditional probabilities can easily be determined using \textbf{Definition 2.4.1} above:
$$P(E_{1}|E_{2})=\frac{P(E_{1}\cap E_{2})}{P(E_{2})}=\frac{2/6}{1/2}=\frac{2}{3}$$
$$P(E_{2}|E_{1})=\frac{P(E_{1}\cap E_{2})}{P(E_{1})}=\frac{2/6}{1/2}=\frac{2}{3}$$
\subsection{Independent Events}
Intuitively, we can see that if events $A$ and $B$ don't influence each other, then:
$$P(B|A)=P(B)$$
$$P(B|A)=P(B)\Leftrightarrow \frac{P(B\cap A)}{P(A)}=P(B)\Leftrightarrow P(A\cap B)=P(B)(A)\;(\text{if }P(A)>0 )$$
\begin{definition}
	Two events $A$ and $B$ are independent if and only if: $$P(B|A)=P(B)$$
	assuming $P(A)>0$
\end{definition}
\begin{theorem}
	Two events $A$ and $B$ are independent if and only if $$P(A\cap B)=P(A)P(B)$$
\end{theorem}
There are many examples of independent events, like if we define 2 events $E_{1}$: "Getting heads on the first toss." and $E_{2}$: "Getting heads on the second toss.". Intuitively you can see that $E_{1}$ and $E_{2}$ are unrelated, so they are \textbf{independent events}. You can also verify this fact:
$$P(E_{1}\cap E_{2})=\frac{1}{2}.\frac{1}{2}=\frac{1}{4}=P(E_{1})P(E_{2})$$
It's very important to note that determining the independence of events is completely \textbf{unrelated} to their mutual exclusion. Events are considered independent if and only if they satisfy \textbf{Theorem 2.4.1}.
\newpage
\section{Total Probability and Bayes's rule}
\subsection{Total Probability}
In many situations, we don't know directly the information of $P(A)$ in \textbf{Definition 2.4.1}(or denomirator part); so in this section, we will develop a simple formula to handle this problem.
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{7.jpg}
	\caption{Total probability}
	\label{Figure 7}
\end{figure}
\begin{equation*}
	\begin{split}
		A=A\cap S=A\cap(B\cup \overline{B})=(A\cap B)\cup (A\cap \overline{B})
	\end{split}
\end{equation*}
Because $A\cap B$ and $A\cap \overline{B}$ are independent events, so:
\begin{equation*}
	P(A)=P(A\cap B)+P(A\cap \overline{B})=P(A|B)P(B)+P(A|\overline{B})P(\overline{B})
\end{equation*}
\begin{theorem}
	If $A$ and $B$ are two events of sample space $S$, then:
	\begin{equation*}
		P(A)=P(A|B)P(B)+P(A|\overline{B})P(\overline{B})
	\end{equation*}
\end{theorem}
Total probability is an useful formula, especially when you're conducting surveys in practice. For example, in my university, there are $2$ types of students: those who "studied the whole semester" and those who "studied only one night before the exam". After the final exam, I asked everyone in my class about their scores and totaled the results. Event $A$: "Got $A+$" and event $\overline{A}$: "Didn't get $A+$"; event $B$: "Studied the whole semester" and event $\overline{B}$: "Studied only one night".
\begin{equation*}
	P(A|B)=0.8,\;P(A|\overline{B})=0.3,\;P(B)=0.4
\end{equation*}
Using total probability formula, I obtained the probability of $A$:
$$P(A)=P(A|B)P(B)+P(A|\overline{B})P(\overline{B})=0.8.0.4+0.3.(1-0.4)=0.5$$
Wow, that was an impressive ratio. Half of a class received perfect score. Was this course too easy?
\subsection{Bayes's rule}
If I studied hard, I would get $A+$. But how about me, who wasn't keen on studying boring courses like Computer Architecture but \textit{still survived after final test and even got "A+"}? I could questioned myself "Was I too lucky?". To answer myself, I had to calculate $P(\overline{B}|A)$, if the result is not so high, perhaps I was lucky. Bayes's rule was what I needed.
\begin{theorem}
	If $A$ and $B$ are two events of sample space $S$, then:
	\begin{equation*}
		P(B|A)=\frac{P(B\cap A)}{P(A)}=\frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|\overline{B})P(\overline{B})}
	\end{equation*}
\end{theorem}
Bayes's rule seems very simple. Indeed an average high school student has no difficulty finding it, but its idea is brilliant. Before Bayes, we typically only reasoned about problems in terms of cause first, then effect. But Bayes's rule opens up a completely new way of thinking for us: knowing the effect beforehand, then understanding how cause influences it.
\\ \\ \\
\indent Back to my previous problem, I applied Bayes's rule:
\begin{equation*}
	P(B|A)=\frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|\overline{B})P(\overline{B})}=\frac{0.8.0.4}{0.5}=0.64\Rightarrow P(\overline{B}|A)=1-P(B|A)=0.36
\end{equation*}
Since $0.36$ was not a high number, indeed I was incredibly lucky and the course was not as easy as I thought.
\chapter{Random Variables and Probability Distributions}
\section{Definition of Random Variables}
Back to our coin tossing game, now you toss a fair coin three times. This experiment has sample space $S$:
$$S=\{HHH,HHT,HTH,HTT,THH,THT,TTH,TTT\}$$
Since writing all of the sample points is quite lengthly and time-consuming, sometimes unnecessarily, so how about we assign each point with a \textbf{numerical value}? Because the assignment of values is entirely based on our own conventions, there are no constraints whatsover. But for this reason, we should choose the \textbf{smartest} and \textbf{most convenient} way to assign values to suit our concern. For example, if I define an event $A$: "Getting $2$ heads", then the cleverest way is assigning each sample point with its number of heads.
\begin{equation*}
	\begin{split}
		HHH&\to3\\
		HHT,HTH,THH&\to2\\
		HTT,TTH,THT&\to1\\
		TTT&\to0\\
	\end{split}
\end{equation*}
Or event $B$: "Getting both tails and heads":
\begin{equation*}
	\begin{split}
		HHT,HTH,HTT,THH,THT,TTH,TTT&\to1\quad\text{(Yes)}\\
		HHH,TTT&\to0\quad\text{(No)}\\
	\end{split}
\end{equation*}
These values may be viewed as values assumed by the \textbf{random variable} $X$, $X$ can be "number of heads appearing" or "getting both tails and heads state", but \textit{not both}.
\begin{definition}
	A \textbf{random variable} is a function that associates a real number with each element in the sample space.
\end{definition}
We shall use a capital letter $X$, to denote the random variable and its corresponding smaller letter $x$, for \textbf{one of its values}. A typical mistake is forgetting to define the meaning of random variable $X$, so \textit{please don't forget it in your exam}. Let's continue by looking at some examples of random variables and sample spaces.
\\ Roll the die and observe the landing value. $X$ is the random variable defined as the value we observed:
$$S_{1}=\{1,2,3,4,5,6\}$$
Roll the die repeatedly until the sixth appears $6$ times. $Y$ is the random varibale defined as the number of rolling:
$$S_{2}=\{1,2,3,\cdots\}$$
Use \textbf{exponential distribution} to predict if tomorrow will be a rainy day. $Z$ is the random variable defined as the probability of the event "Tomorrow will be a rainy day":
$$S_{3}=\{z\;|\;0< z<1\}$$
The random variable $X$ can take one of the values $x_{1}=1,x_{2}=2,\cdots,x_{6}=6$, and similarly with $Y$ and $Z$ can take one of their own values in their sample spaces $S_{2}$, $S_{3}$ respectively. Now we interested in classifying $2$ types of sample spaces and their random variables.
\begin{definition}
	If a sample space contains a finite number of possibilities or an unending sequence with as many elements as there are whole numbers, it's called a \textbf{discrete sample space}.
\end{definition}
\begin{definition}
	If a sample space contains an infinite number of possibilities equal to the number of points on a line segment, it's called a \textbf{continuous sample space}.
\end{definition}
So $S_{1}$ and $S_{2}$ are \textbf{discrete sample spaces} and $X$, $Y$ are called \textbf{discrete random variables}; $S_{3}$ is \textbf{continuous sample space} and $Z$ is called \textbf{continuous random variable}.
\section{Discrete Probability Distributions}
Consider coin tossing experiment, now if we assign random variable $X$ as the number of heads appearing, I can obtain some useful results:
\begin{equation*}
	\begin{split}
		P(X=3)&=P(\{HHH\})=\frac{1}{8}=0.125\\
		P(X=2)&=P(\{HHT,HTH,THH\})=\frac{3}{8}=0.375\\
		P(X=1)&=P(\{TTH,THT,HTT\})=\frac{3}{8}=0.375\\
		P(X=0)&=P(\{TTT\})=\frac{1}{8}=0.125\\
	\end{split}
\end{equation*}
Or in table form:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$x$      & $0$     & $1$     & $2$     & $3$     \\
		\hline
		$P(X=x)$ & $0.125$ & $0.375$ & $0.375$ & $0.125$ \\
		\hline
	\end{tabular}
\end{center}
Frequently, it's much convenient to represent all of the probabilities of a random variable $X$ by a \textbf{formula}.
\begin{definition}
	The function $f(x)$ is a \textbf{probability density function} (pdf) of the discrete random variable $X$ if, for each possible outcome $X$:
	\begin{equation*}
		\begin{cases}
			f(x)\geq0      \\
			\sum_{x}f(x)=1 \\
			P(X=x)=f(x)    \\
		\end{cases}
	\end{equation*}
\end{definition}
You can derive that the pdf of coin tossing experiment above is:
$$f(x)=P(X=x)=\frac{\binom{3}{x}}{2^3}\quad(x=0,1,2,3)$$
In many cases, we want to know the probability of $(X\leq x)$ (you will see it clearly in the next section). For instance, I want to know the probability of heads appearing a maximum of $2$ times.
$$P(X\leq2)=P(X=0)+P(X=1)+P(X=2)=0.125+0.375+0.375=0.875$$
In general, we define a new function $F(x)$ to handle these cases as follows:
\begin{definition}
	The \textbf{cumulative distribution function} (cdf) $F(x)$ of a discrete random variable $X$ with probability distribution $f(x)$ is:
	$$F(x)=P(X\leq x)=\sum_{t\leq x}f(t)\quad {(-\infty<x<+\infty)}$$
\end{definition}
For example, in the above experiment:
\begin{equation*}
	F(x)=
	\begin{cases}
		0\quad(x<0)           \\
		0.125\quad(0\leq x<1) \\
		0.5\quad(1\leq x<2)   \\
		0.875\quad(2\leq x<3) \\
		1\quad(x\geq 3)       \\
	\end{cases}
\end{equation*}
\section{Continuous Probability Distributions}
Choosing randomly a number within the range $(0,1)$. $X$ is the random variable is defined as the chosen one. In the previous chapter, we've discussed that the probability of getting \textbf{a single number} in the range $(0,1)$ is $0$.
$$P(X=0.7)=0$$
So $0.7$ will never be chosen since $P(X=0.7)=0$? Now think carefully about the reasons why the probability of an event might be zero. Recall the \textbf{Theorem 2.3.1}:
$$P(A)=\frac{n}{N}$$
There are $2$ main reasons that could explain why $P(A)$ can be zero; the first one is \textbf{$n=0$ and $N$ is a finite number}, and the second one is $\textbf{$n$ is a finite number and $N$ is an infinite number}$. This might be the big misconception, since people always claim that the only reason for $P(A)=0$ is the first one, and forgot the second. But now you can clearly see that if $n>0$, event will always have a chance of happening. So now we conclude certainly: "$P(A)=0$ doesn't mean event $A$ will never happen."\\
If sample space $S$ is \textit{continuous sample space}, which contains \textit{an infinite number of possibilities equal to the number of points on a line segment}, we don't care about the probability of \textbf{a single sample point} occurring (because it's always equal $0$). We  shift our attention to the probability of \textbf{the interval that our concern sample point may be fallen  inside} occurring.
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{8.jpg}
	\caption{What's the probability that a number will fall within this range?}
	\label{Figure 8}
\end{figure}
Intuitively you can conclude that: $$P(0.6<X<0.78)=0.78-0.6=0.18$$
Similarly with the discrete random variables, we can also define:
\begin{definition}
	The function $f(x)$ is a \textbf{probability distribution (or density) function} (pdf) for the continuous random variable $X$, defined over the set of real numbes, if:
	\begin{equation*}
		\begin{cases}
			f(x)\geq0,\text{ for all }x\in R \\
			\int_{-\infty}^{+\infty}f(x)dx=1 \\
			P(a<X<b)=\int_{a}^{b}f(x)dx
		\end{cases}
	\end{equation*}
\end{definition}
Verifying yourself that the pdf function of number choosing experiment is:
\begin{equation*}
	f(x)=
	\begin{cases}
		1\quad(0<x<1)            \\
		0\quad(\text{elsewhere}) \\
	\end{cases}
\end{equation*}
This pdf function is the simplest case of \textbf{uniform distribution function}. As I mentioned before, in the continuous sample space case, concerning on the probability of $(X=x)$ doesn't not make any sense since it's equal $0$ and we can't use any information about it. Instead, we turn our focus to the probability of $(X<x)$ and define the \textbf{cumulative distribution function} $F(x)$ as follows:
\begin{definition}
	The \textbf{cumulative distribution function} (cdf) $F(x)$ of a continuous random variable $X$ with pdf $f(x)$ is:
	\begin{equation*}
		F(x)=P(X<x)=\int_{-\infty}^{x}f(t)dt\quad(-\infty<x<+\infty)
	\end{equation*}
\end{definition}
The cdf function of number choosing experiment is:
\begin{equation*}
	F(x)=
	\begin{cases}
		0\quad(x<0)       \\
		x\quad(0\leq x<1) \\
		1\quad(x\geq 1)   \\
	\end{cases}
\end{equation*}
\section{Joint Probability Distributions}
\subsection{Case of Discrete Random Variables}
In the previous sections, we've considered \textbf{single random variable} and its \textbf{probability distribution function} case, and restricted ourshelves to \textbf{one-dimension sample space}. But now we're interested in observing  the \textbf{simultaneous outcomes} of \textbf{several random variables}. For example, you toss $2$ fair coins simultaneously and observe their appearing faces. You define $2$ random variables, $X$ for the first coin face, and $Y$ for the second one. The heads is assigned value $1$, and the tails is assigned value $0$. These are some results obtained from this experiment:
$$P(X=0,Y=0)=\frac{1}{2}.\frac{1}{2}=\frac{1}{4}=0.25$$
$$P(X=0,Y=1)=\frac{1}{2}.\frac{1}{2}=\frac{1}{4}=0.25$$
$$P(X=1,Y=0)=\frac{1}{2}.\frac{1}{2}=\frac{1}{4}=0.25$$
$$P(X=1,Y=1)=\frac{1}{2}.\frac{1}{2}=\frac{1}{4}=0.25$$
These results can be written in table form:
\begin{center}
	\begin{tabular}{ |c|c|c| }
		\hline
		$P(X=x,Y=y)$ & $Y=0$ & $Y=1$ \\
		\hline
		$X=0$        & 0.25  & 0.25  \\
		\hline
		$X=1$        & 0.25  & 0.25  \\
		\hline
	\end{tabular}
\end{center}
Another classic example of \textbf{discrete joint probability distribution} is the problem of picking balls from a basket. Now you have a basket with many colorful balls inside; there are $3$ red, $4$ green and $5$ blue balls. You choose randomly $3$ balls from the basket, and you define $2$ random variables $X$ and $Y$; $X$ is the number of red balls and $Y$ is the number of green balls. Using counting techniques and combinations, you can write the \textbf{joint probability distribution function}:
$${f(x,y)}=P(X=x,Y=y)=\frac{\binom{3}{x}\binom{4}{y}\binom{5}{3-x-y}}{\binom{12}{3}}$$
Or in table form:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c| }
		\hline
		$P(X=x,Y=y)$ & $Y=0$           & $Y=1$          & $Y=2$           & $Y=3$          \\
		\hline
		$X=0$        & $\frac{1}{22}$  & $\frac{2}{11}$ & $\frac{3}{22}$  & $\frac{1}{55}$ \\
		\hline
		$X=1$        & $\frac{3}{22}$  & $\frac{3}{11}$ & $\frac{9}{110}$ & -              \\
		\hline
		$X=2$        & $\frac{3}{44}$  & $\frac{3}{55}$ & -               & -              \\
		\hline
		$X=3$        & $\frac{1}{220}$ & -              & -               & -              \\
		\hline
	\end{tabular}
\end{center}
\begin{definition}
	The function $f(x,y)$ is a \textbf{joint probability distribution function} of the \textbf{discrete random variables} $X$ and $Y$ if:
	\begin{equation*}
		\begin{cases}
			f(x,y)\geq 0             \\
			\sum_{x}\sum_{y}f(x,y)=1 \\
			P(X=x,Y=y)=f(x,y)        \\
		\end{cases}
	\end{equation*}
\end{definition}
\begin{corollary}
	For any region $A$ in the $xy$ plane:
	$$P[(X,Y)\in A]=\sum\sum_{A}f(x,y)$$
\end{corollary}
After considering the case where both variables $X$ and $Y$ are both varying, what if we \textbf{fix} one variable and vary the other? You can see this idea appearing very naturally in the process of finding the values of the above table using a calculator. We introduce the new functions called \textbf{marginal distributions} of $X$ and $Y$ alone.
\begin{definition}
	The \textbf{marginal distributions} of $X$ alone and of $Y$ alone for the \textbf{discrete case} are:
	$$g(x)=\sum_{y}f(x,y);\quad h(y)=\sum_{x}f(x,y)$$
\end{definition}
Since the general form of $g(x)$ and $h(y)$ are not easy to be generalized, and the range of discrete random variables $X$ and $Y$ is very narrow, so we should reuse the table above to obtain values of these functions.
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$P(X=x,Y=y)$ & $Y=0$                & $Y=1$                & $Y=2$                & $Y=3$               & $g(x)$                \\
		\hline
		$X=0$        & $\frac{1}{22}$       & $\frac{2}{11}$       & $\frac{3}{22}$       & $\frac{1}{55}$      & $g(0)=\frac{21}{55}$  \\
		\hline
		$X=1$        & $\frac{3}{22}$       & $\frac{3}{11}$       & $\frac{9}{110}$      & -                   & $g(1)=\frac{27}{55}$  \\
		\hline
		$X=2$        & $\frac{3}{44}$       & $\frac{3}{55}$       & -                    & -                   & $g(2)=\frac{27}{220}$ \\
		\hline
		$X=3$        & $\frac{1}{220}$      & -                    & -                    & -                   & $g(3)=\frac{1}{220}$  \\
		\hline
		$h(y)$       & $h(0)=\frac{14}{55}$ & $h(1)=\frac{28}{55}$ & $h(2)=\frac{12}{55}$ & $h(3)=\frac{1}{55}$ & $1$                   \\
		\hline
	\end{tabular}
\end{center}
\begin{corollary}
	If $g(x)$ and $h(y)$ are marginal distributions of $X$ alone and $Y$ alone for the \textbf{discrete case}, then:
	$$\sum_{x}g(x)=\sum_{y}h(y)=1$$
\end{corollary}
\textbf{Corollary 3.4.0.2} is directly derived from the \textbf{Definition 3.4.1}, and you can verify them by using the probability distribution table.
\\ After defining the \textbf{marginal distribution}, now we can see clearly the connection between them and regular pdf functions are:
\begin{equation*}
	P(X=x)=g(x);\quad P(Y=y)=h(y)
\end{equation*}
Now if I choose $3$ balls from the basket, and I know two of them are green, what's the probability that the remaining ball is red?
\begin{equation*}
	P(X=1|Y=2)=\frac{P(X=1,Y=2)}{P(Y=2)}=\frac{\frac{9}{110}}{\frac{12}{55}}=\frac{3}{8}=0.375
\end{equation*}
In general, it's not hard to deduce these formulas:
\begin{equation*}
	\begin{split}
		f(x|y)&=P(X=x|Y=y)=\frac{P(X=x,Y=y)}{P(Y=y)}=\frac{f(x,y)}{h(y)}\\
		f(y|x)&=P(Y=y|X=x)=\frac{P(Y=y,X=x)}{P(X=x)}=\frac{f(x,y)}{g(x)}\\
	\end{split}
\end{equation*}
\begin{definition}
	Let $X$ and $Y$ be two \textbf{discrete random variables}. The \textbf{conditional distribution} of the variable $Y$ given that $(X=x)$ is:
	$$f(y|x)=\frac{f(x,y)}{g(x)}$$
	Similarly for the reverse case:
	$$f(x|y)=\frac{f(x,y)}{h(y)}$$
\end{definition}
From the previous section \textbf{Chapter 2.4: Conditional Probability}, you already knew how to identify two independent events, by checking this condition:  $$P(A\cap B)=P(A)P(B)$$
Now we use it again, with random variables $X$ and $Y$:
\begin{equation*}
	P(X=x,Y=y)=P(X=x)P(Y=y)\Leftrightarrow f(x,y)=g(x)h(y)
\end{equation*}
\begin{definition}
	Let $X$ and $Y$ be two \textbf{discrete random variables}, with joint probability distribution function $f(x,y)$ and marginal distributions $g(x)$, $h(y)$, respectively. They are said to be \textbf{statistically independent} if and only if: $$f(x,y)=g(x)h(y)$$
\end{definition}
If we check the case $(X=1,Y=1)$: $$\left(f(1,1)=\frac{3}{11}\right)\neq \left(g(1)h(1)=\frac{27}{55}.\frac{28}{55}\right)$$
So $X$ and $Y$ are \textbf{not} statistically independent, they are interdependent. But how interdependent are they? Are they highly or minimally interdependent? This question will be answered at the end of the next chapter.
\subsection{Case of Continuous Random Variables}
Similarly, the joint probability distribution function of \textbf{continuous random variable} can also be defined as follows:
\begin{definition}
	The function $f(x,y)$ is a \textbf{joint probability distribution function} of the \textbf{continuous random variables} $X$ and $Y$ if:
	\begin{equation*}
		\begin{cases}
			f(x,y)\geq 0                                                 \\
			\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x,y)dxdy=1 \\
			P(a<X<b,c<Y<d)=\int_{c}^{d}\int_{a}^{b}f(x,y)dxdy            \\
		\end{cases}
	\end{equation*}
\end{definition}
\begin{corollary}
	For any region $A$ in the $xy$ plane:
	$$P[(X,Y)\in A]=\iint_{A}f(x,y)dxdy$$
\end{corollary}
Almost all the formulas in the above section are redefined exactly the same, with a slight difference in the notation for the integral (you can read more about Riemann sum to see the relationship between integrals and infinite discrete sums).
\begin{definition}
	The \textbf{marginal distributions} of $X$ alone and of $Y$ alone for the \textbf{continuous case} are:
	$$g(x)=\int_{-\infty}^{+\infty}f(x,y)dy;\quad h(y)=\int_{-\infty}^{+\infty}f(x,y)dx$$
\end{definition}
\begin{corollary}
	If $g(x)$ and $h(y)$ are marginal distributions of $X$ alone and $Y$ alone for the \textbf{continuous case}, then:
	$$\int_{-\infty}^{+\infty}g(x)dx=\int_{-\infty}^{+\infty}h(y)dy=1$$
\end{corollary}
\begin{definition}
	Let $X$ and $Y$ be two \textbf{continuous random variables}. The \textbf{conditional distribution} of the variable $Y$ given that $(X=x)$ is:
	$$f(y|x)=\frac{f(x,y)}{g(x)}$$
	Similarly for the reverse case:
	$$f(x|y)=\frac{f(x,y)}{h(y)}$$
\end{definition}
But notice that if $X$ and $Y$ are continuous random variables, then $P(X=x|Y=y)$ is always equal zero! The correct way to obtain conditional probability value is shown below:
\begin{corollary}
	If $X$ and $Y$ be two \textbf{continuous random variables}, then:
	$$P(a<X<b|Y=y)=\int_{a}^{b}f(x|y)dx$$
\end{corollary}
\begin{definition}
	Let $X$ and $Y$ be two \textbf{continuous random variables}, with joint probability distribution function $f(x,y)$ and marginal distributions $g(x)$, $h(y)$, respectively. They are said to be \textbf{statistically independent} if and only if: $$f(x,y)=g(x)h(y)$$
\end{definition}
\chapter{Mathematical Expectation}
Due to the relatively high degree of similarity between the formulas in the case of discrete and continuous random variables, this chapter approaches them \textbf{in parallel}, rather than seperating them like previous chapter.
\section{Mean of a Random Variable}
After conducting experiment, now it's time to process your obtained results. Toss a fair coin for $3$ times, and define the discrete random variable $X$ as the number of heads appearing. The pdf  of this experiment is:
$$f(x)=P(X=x)=\frac{\binom{3}{x}}{2^3}\quad(x=0,1,2,3)$$
Or in table form:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$x$      & $0$     & $1$     & $2$     & $3$     \\
		\hline
		$P(X=x)$ & $0.125$ & $0.375$ & $0.375$ & $0.125$ \\
		\hline
	\end{tabular}
\end{center}
What's the \textbf{mean} or \textbf{expected value} of $X$? Intuitively, you will use the formula:
$$\mu_{X}=E(X)=\sum_{x}xf(x)=0.0.125+1.0.375+2.0.375+3.0.125=1.5$$
By illustrating the above results with a graph, the position of $\mu$ is shown.
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{9.jpg}
	\caption{Pdf of fair coin tossing experiment with its mean}
	\label{Figure 9}
\end{figure}
\\If our coin is unfair, assume that the probability of getting heads is $p=0.3$, then the probability of getting tails is $q=1-p=0.7$. Now the pdf is:
$$f(x)=P(X=x)=\binom{3}{x}p^{x}q^{3-x}=\binom{3}{x}0.3^{x}0.7^{3-x}\quad(x=0,1,2,3)$$
Or in table form:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$x$      & $0$     & $1$     & $2$     & $3$     \\
		\hline
		$P(X=x)$ & $0.343$ & $0.441$ & $0.189$ & $0.027$ \\
		\hline
	\end{tabular}
\end{center}
In this experiment, we \textbf{expect} to get the \textbf{average value} (or mean) of $X$:
$$\mu_{X}=E(X)=\sum_{x}xf(x)=0.0.343+1.0.441+2.0.189+3.0.027=0.9$$
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{10.jpg}
	\caption{Pdf of unfair coin tossing experiment with its mean}
	\label{Figure 10}
\end{figure}
\begin{definition}
	Let $X$ be a random variable with pdf function $f(x)$. The \textbf{expected value} or \textbf{mean} of $X$ is:
	$$\mu_{X}=E(X)=\sum_{x}xf(x)\quad(\text{if $X$ is discrete})$$
	$$\mu_{X}=E(X)=\int_{-\infty}^{+\infty}{x}f(x)dx\quad(\text{if $X$ is continuous})$$
\end{definition}
Now imagine you're playing coin tossing game (don't treat it like an experiment). As I mention the rule above, you have $3$ turns to toss a coin. If you see $3$ heads appear, you're extremely lucky today; but if you don't see any, don't be sad because life is long. A very natural thought occurred to me that we should quantify a player's "luck level" with a quantitative value. Random variable $Y$ takes this role and can be defined as:
$$Y=\frac{X}{3}$$
Now we are intersted in average "luck level" of coin tossing game, so reuse the previous table that we've created:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$x$             & $0$     & $1$     & $2$     & $3$     \\
		\hline
		$y=\frac{x}{3}$ & $0$     & $0.333$ & $0.666$ & $1$     \\
		\hline
		$P(X=x)$        & $0.343$ & $0.441$ & $0.189$ & $0.027$ \\
		\hline
	\end{tabular}
\end{center}
The \textbf{expected value} or \textbf{mean} of "luck level" is:
$$\mu_{Y}=E(Y)=\sum_{y}yf(x)=0.0.343+0.333.0.441+0.666.0.189+1.0.027=0.299\approx 0.3$$
Since $0.3$ is not so high value, so the \textbf{mean} of "luck level" is pretty small and players shouldn't look forward to their chances in this game. Furthermore, you should notice the subtle connection between two random variables $X$ and $Y$ as:
$$Y=\frac{X}{3}\Leftrightarrow E(Y)=\frac{E(X)}{3}$$
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{11.jpg}
	\caption{The mean value of "luck level"}
	\label{Figure 11}
\end{figure}
\\ If $Y$ is defined as \textbf{function of random variable $X$} or $(Y=g(X))$, then the \textbf{expected value} of it can be calculated using the theorem below:
\begin{theorem}
	Let $X$ be a random variable with pdf $f(x)$, and the \textbf{expected value} of the random variable $g(X)$ is:
	$$E(g(X))=\mu_{g(X)}=\sum_{x}g(x)f(x)\quad\text{(if X is discrete)}$$
	$$E(g(X))=\mu_{g(X)}=\int_{-\infty}^{+\infty}g(x)f(x)dx\quad\text{(if X is continuous)}$$
\end{theorem}
Now I want to make our game funnier because just tossing a coin is too boring, so I came up with an idea. How about toss  $3$ unfair coins and the bottle cap \textbf{at the same time}? The random variable $X$ is defined as the number of heads appearing with the probability of getting heads is $p=0.3$, and getting tails is $q=1-p=0.7$, so the pdf is:
$$g(x)=P(X=x)=\binom{3}{x}p^{x}q^{3-x}=\binom{3}{x}0.3^{x}0.7^{3-x}\quad(x=0,1,2,3)$$
A 'PEPSI' bottle cap has just been founded, so I define a new random variable $Y$ as its face after tossing. The 'PEPSI' face is assigned the value $1$, and the other is assigned the value $0$. Assume that:
\begin{equation*}
	\begin{split}
		P(Y=1)&=p'=0.8\\
		P(Y=0)&=q'=1-p'=0.2\\
	\end{split}
\end{equation*}
So the pdf of bottle cap tossing experiment is: $$h(y)=P(Y=y)=(p')^{y}(q')^{1-y}=0.8^{y}0.2^{1-y}\quad(y=0,1)$$
Intuitively, you can certainly conclude that $X$ and $Y$ are \textbf{statistically independent} since coins and bottle cap tossing process don't affect each other. So the \textbf{joint probability distribution} of them is:
$$f(x,y)=g(x)h(y)=\binom{3}{x}0.3^{x}0.7^{3-x}0.8^{y}0.2^{1-y}\quad(x=0,1,2,3;\;y=0,1)$$
Or in table form:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$P(X=x,Y=y)$ & $X=0$        & $X=1$        & $X=2$        & $X=3$        & $h(y)$     \\
		\hline
		$Y=0$        & $0.0686$     & $0.0882$     & $0.0378$     & $0.0054$     & $h(0)=0.2$ \\
		\hline
		$Y=1$        & $0.2744$     & $0.3528$     & $0.1512$     & $0.0216$     & $h(1)=0.8$ \\
		\hline
		$g(x)$       & $g(0)=0.343$ & $g(1)=0.441$ & $g(2)=0.189$ & $g(3)=0.027$ & $1$        \\
		\hline
	\end{tabular}
\end{center}
Now "luck level" can be defined as the value of $(X+Y)$. Thinking according to the same logic, the \textbf{mean} value of $(X+Y)$ is:
\begin{equation*}
	\begin{split}
		\mu_{(X+Y)}&=E(X+Y)=\sum_{x}\sum_{y}(x+y)f(x,y)=\sum_{x}\left(\sum_{y}(x+y)f(x,y))\right)\\
		&=\sum_{x}(xf(x,0)+xf(x,1)+0f(x,0)+1f(x,1))\\
		&=\sum_{x}\left(xf(x,0)+xf(x,1)+f(x,1)\right)\\
		&=0f(0,0)+1f(1,0)+2f(2,0)+3f(3,0)+0f(0,1)+1f(1,1)\\&+2f(2,1)+3f(3,1)+f(0,1)+f(1,1)+f(2,1)+f(3,1)\\
		&=0.0882+2.0.0378+3.0.0054+0.3528+2.0.1512\\&+3.0.00216+0.2744+0.3528+0.1512+0.0216\\
		&=1.7
	\end{split}
\end{equation*}
So now, the players can confidently look forward for their opportunity in this game!
\begin{definition}
	Let $X$ and $Y$ be random variables with \textbf{joint probability distribution} $f(x,y)$. The \textbf{mean} or \textbf{expected value} of the random variable function $g(X,Y)$ is:
	$$\mu_{g(X,Y)}=E(g(X,Y))=\sum_{x}\sum_{y}g(x,y)f(x,y)\quad\text{(if $X$ and $Y$ are discrete)}$$
	$$\mu_{g(X,Y)}=E(g(X,Y))=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} g(x,y)f(x,y)dxdy\quad\text{(if $X$ and $Y$ are continuous)}$$
\end{definition}
If $g(X,Y)=X$, then we obtain some useful results and can be represented as corollaries:
\begin{corollary}
	Let $X$ and $Y$ be discrete random variables with \textbf{joint probability distribution} $f(x,y)$, the \textbf{mean} of $X$ is:
	$$\mu_{X}=E(X)=\sum_{x}\sum_{y}xf(x,y)=\sum_{x}x\left(\sum_{y}f(x,y)\right)=\sum_{x}xg(x)$$
	Similarly, the mean of $Y$ is: $$\mu_{Y}=E(Y)=\sum_{y}yh(y)$$
\end{corollary}
\begin{corollary}
	Let $X$ and $Y$ be continuous random variables with \textbf{joint probability distribution} $f(x,y)$, the \textbf{mean} of $X$ is:
	$$\mu_{X}=E(X)=\int_{-\infty}^{+\infty}xg(x)dx$$
	Similarly, the mean of $Y$ is: $$\mu_{Y}=E(Y)=\int_{-\infty}^{+\infty}yh(y)dy$$
\end{corollary}
\section{Variance and Covariance of Random Variables}
The concept of \textbf{mean} or \textbf{expected value} is very important in ProbStat. In any probability distribution graph, we use the \textbf{mean value} as \textbf{main reference point} (although the mean isn't always in the center of the graph). We are very interested in how data is distributed \textbf{around the mean value}. Are they distributed far or close to it?
\end{document}

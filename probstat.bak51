\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\title{Thinking like a Frequentist
}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
\usepackage{blindtext}
\usepackage{mathrsfs}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{epigraph} 
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{comment}
\usepackage[english]{babel}
\setcounter{chapter}{-1}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\author{Tin Vu}
\date{January 2026}
\begin{document}
\maketitle
\newpage
\begin{center}
	\textit{This page is intentionally left blank.}
\end{center}
\chapter*{Preface}
\epigraph{\textit{"Are you watching closely?"}}{Alfred Borden, The Prestige (2006)}
When I was learning Probability and Statistics (ProbStat for short), I did not really understand why things were distributed \textit{normally}. Were they \textit{normal} because they were, or were \textit{we just assuming "Everything is normal for simplicity"}? Sometimes, I felt we were overusing this term and in many situations, we were separating our theory from the actual data due to the intital assumption of \textit{normal} distribution.
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{1.jpg}
	\caption{Distribution of Math scores in the 2021 National Entrance Exam}
	\label{fig:galaxy}
\end{figure}
\\ As you can see here, this graph \textbf{does not really resemble a bell-shaped curve}; and clearly, this data is not distributed normally. In fact, not much real-world statistical data fits a bell-shaped curve. But why are we still using it?
\\ In my opinion, I think the main reason is due to \textbf{Central Limit Theorem} (CLT for short), every sample in the same population (regardless of how the data is distributed) has a \textbf{mean} that converges to a \textit{normal distribution}, or \textbf{bell-shaped curve}. This topic will be explained in detail in \textbf{Chapter 7: Fundamentals of Statistics}.\\ \\  \\
\indent This book approaches the fundamentals of probability and statistics in a very mathematically rigorous way to ensure maximum accuracy. I encourage readers to prove all of theorems, corollaries; and you should create your own examples for each theorem to gain a deeper understading of their origins.

\chapter*{Acknowledgements}
First of all, I want to thank you for reading this book. Although I am not a native English speaker, but I truly enjoy writing in English. I have tried my best to express my ideas in English, but minor grammatical or spelling errors are unavoidable. If you find any of them, I would be very happy to receive your feedback via my email address:
\url{tinvu1309@gmail.com}
\\ \\ \\
\indent Secondly, I am extremely grateful and would like to express my thanks to the authors of the textbook "Probability and Statistics for Engineers and Scientists", $9$th edition. This book truly saved my student life.
\\ \\ \\
\indent Finally, the idea of writing this book was inspired by Prof. Steve Brunton lectures on YouTube. You should check out his videos too!
\begin{flushright}
	---Tin Vu---
\end{flushright}
\tableofcontents
\listoffigures
\newpage
\chapter*{List of notations}
Since much of my work is handwritten, so I have modified some commonly used notations for probability distribution funcions using curved script for convenience. You should notice that my conventions are not the international standards.
\begin{enumerate}
	\item $\mathscr{I}$: Bernoulli distribution
	\item $\mathscr{B}$: Binomial distribution
	\item $\mathscr{B}^*$: Negative binomial distribution
	\item $\mathscr{H}$: Hypergeometric distribution
	\item $\mathscr{G}^*$: Geometric distribution
	\item $\mathscr{P}$: Poisson distribution
	\item $\mathscr{U}$: Uniform distribution
	\item $\mathscr{N}$: Normal distribution
	\item $\mathscr{G}$: Gamma distribution
	\item $\mathscr{E}$: Exponential distribution
	\item $\mathscr{C}$: Chi-squared distributon
	\item $\mathscr{T}$: t-distribution (or Student distribution)
\end{enumerate}
\chapter{Introduction to Probability and Statistics}
\section{Coin Tossing}
Imagine you have a coin, like this one. It is an ordinary coin that can be found everywhere.
\\Everyone knows that the chance of heads (H) appearing after each toss is $50\%$, no one even doubts that. So, the \textbf{probability} of an \textbf{event} that heads appearing is $50\%$. But because you are a very curious person, you do not easily accept the truth like others, so you will find a way to test it. The fact "The probability of heads appearing is $50\%$" will be tested, and it can be called a \textbf{hypothesis}.
\\ \\ \\
\indent The simplest way to test your hypothesis is tossing a coin many times. You might toss a coin $N=100$ times and see heads appear $n=40$ times, so your actual probability is: $$\hat p=\frac{n}{N}=0.4$$
But your previous assumption (or hypothesis) states that the ideal probability is: $$p=0.5$$
Is there anything wrong here? Dissatisfied, you continue your experiment. This time, you toss a coin $N=500$ times and see heads appear $n=270$ times, so the new actual probability is:
$$\hat p=\frac{n}{N}=0.54$$
This time the result is closer with initial hypothesis $p$, but your hand now must be very tired after tossing a coin $600$ times. So do you think you have \textbf{enough} evidence to conclude that if $N\to+\infty$, then $\hat p\to p$? If so, your assumption is correct because \textit{can not be refuted}; and if not, our common sense might be wrong because the evidence \textit{strongly refutes it}.
\\ \\ \\
\indent Another strategy to toss a coin is fixing and dividing $N$. Instead of tossing $N=600$ times and only getting the final $p$ value, you can divide large number of coin tosses $N=600$ to smaller tosses $N_{1}=N_{2}=\cdots=N_{6}=100$ (but do not too small, at least each $N_{i}>30$), and get
6 values of $\hat p_{i}$. Suppose that you would get:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$\hat p_{1}$ & $\hat p_{2}$ & $\hat p_{3}$ & $\hat p_{4}$ & $\hat p_{5}$ & $\hat p_{6}$ \\
		\hline
		0.43         & 0.46         & 0.56         & 0.53         & 0.49         & 0.51         \\
		\hline
	\end{tabular}
\end{center}
You might see that the value $p=0.5$ too idealistic to occur in our experiment, and $\hat p$ constantly changes. Therefore instead of determining the \textbf{exact value} of $p$, we \textbf{predict} that the $p$ value lies inside a \textbf{closed interval} with a \textbf{certainty} of $(1-\alpha)100\%$. For example, we can confidently conclude that the value of $p$ lies within the range (0.48,0.51) with $95\%$ accuracy.
\section{Weather Forecasting}
You do not need to know anything about geography to predict what the weather will be tomorrow. Everything you need is just knowledge about Poisson, exponential distributions and the \textbf{average number} of rainy days per month where you are living.
\\ For example, in December there are \textbf{average} 2 rainy days. What are the probabilities of:
\begin{enumerate}
	\item There will be 3 rainy days this month.
	\item Tomorrow will be a rainy day, if today is the $10$th and you have not seen any rain since the beginning of the month.
\end{enumerate}
This problem will be covered in detail in \textbf{Chapter 4: Some Discrete Probability Distributions} and \textbf{Chapter 5: Continuous Probability Distributions}, but if you do have experience with random variables, you can try solving it!
\\ The answer for the first question is:
\begin{equation*}
	P(X=3)=\frac{e^{-2}2^3}{3!}=0.1804=18.04\%
\end{equation*}
\\ The answer for the second question is:
\begin{equation*}
	P(X<11|X>10)=1-P(X>11|X>10)=1-e^{\frac{-2}{31}}=0.0624=6.24\%
\end{equation*}
Since $6.24\%$ is quite low, so tomorrow you do not have to bring an umbrella.
\section{Relationship between Probability and Statistics}
The formal definitions of \textbf{probability} and \textbf{statistics} will be discussed later in \textbf{Chapter 1: Fundamentals of Probability} and \textbf{Chapter 7: Fundamentals of Statistics}, but now let's focus on the general model below:
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=15cm]{3.jpg}
	\caption{General model of simple ProbStat problems}
	\label{Figure 3}
\end{figure}
You want to know some attributes of a very large \textbf{population} such as the probabilites, means,$\cdots$ of the quantities. But there is no way you can observe the entire population (in many cases, $N\to +\infty$)
, so you have to take some \textbf{samples} ($n$ elements) and use \textbf{probability rules} to process them. After that, you can try applying \textbf{statistical inference rules} to draw conclusions about the original population. That is how it works!
\\ \\ \\
\indent Referring to the previous example, you want to check if the probability of heads appearing is $p=50\%$, so you toss a coin multiple times. But since you can only toss a coin for $600$ times consecutively, so you conclude based on your final result $\hat p=0.54$ (or closed interval with a certainty) that might be if $N\to +\infty$ then $\hat p\to p=0.5$. You took the \textbf{samples} ($N=600$ or $N_{i}=100$), used \textbf{probability rule} to calculate the actual $p$ value, and then applied \textbf{statistical inference rules} (hypothesis or closed interval) to draw conclusions respectively.
\chapter{Fundamentals of Probability}
\section{Sample Space and Events}
\subsection{Sample Space}
If you toss a coin, you will see that there are $2$ possible outcomes: heads and tails, denoted by capital letters H and T; or if you roll a die, you will see that there are $6$ possible outcomes, from $1$ to $6$. Tossing a coin, or rolling a die are typical examples of \textbf{experiments}.
\begin{definition}
	An \textbf{experiment} is the process that generates a set of outcomes (or data).
\end{definition}
All of possible outcomes are collected into a single set.
$$S_{1}=\{H,T\}$$
$$S_{2}=\{1,2,3,4,5,6\}$$
\begin{definition}
	The \textbf{sample space} is the set of all possible outcomes of an \textbf{experiment}.
\end{definition}
The sample space is usually represented by the symbol $S$ or $\Omega$. You can easily see that $S$ is not always a countable or finite set. For instance, $S_{3}$ is the set of all random numbers you can choose within the range $(0,1)$:
$$S_{3}=\{x\;|\;0<x<1\}$$
$S_{4}$ is the set of all number of coin tosses until first heads appear:
$$S_{4}=\{1,2,3,\cdots\}$$
It is possible that you toss a coin forever, and heads never appear.
\begin{definition}
	A \textbf{sample point} is a single outcome of the sample space $S$.
\end{definition}
$S_{1},S_{2}$ have $2$ and $6$ sample points respectively, while $S_{3},S_{4}$ have infinite sample points.
\subsection{Events}
For any given experiment, we are often interested in the occurrence of certain \textbf{events} rather than a specific element (or \textbf{sample point}) in the sample space. For example, in the die roll experiment, you may want to know when the outcome is an even number. This will happen if the result is an element of the subset $E_{2}$ of the sample space $S_{2}$:
$$E_{2}=\{2,4,6\}$$
\begin{definition}
	An \textbf{event} is the subset of a sample space.
\end{definition}
Events are always denoted by capital letters like $A,B,C,\cdots$. Similarly, an event is not always a countable or finite subset.
\\ The \textbf{complement} of an event $E_{2}$ with respect to $S_{2}$ is the set of all \textit{odd outcomes}, and can be represented as follows: $$\overline{E}_{2}=\{1,3,5\}$$
There are many ways to denote the \textbf{complement} of an event: $\overline{E},E',E^c$, but in this book I choose overline notation for convenience and make it easy to relate with Boolean algebra.
\begin{definition}
	The \textbf{complement} of an event $E$ with respect to $S$ is the subset of all elements of $S$ that are not in $E$, and can be denoted by the symbol $\overline{E}$.
\end{definition}
Applying set theory, we can perform many set operations like joint, disjoint, union,$\cdots$
\begin{definition}
	The \textbf{intersection} of two events $A$ and $B$, denoted by the symbol $A\cap B$ is the event containing all elements that are common to $A$ and $B$.
\end{definition}
For example, if $A$ and $B$ are the subset of $S_{3}$ and defined as:
\begin{equation*}
	\begin{split}
		A&=\{x\;|\;0<x<0.7\}\\
		B&=\{x\;|\;0.2<x<0.9\}\\
		\Rightarrow A\cap B&=\{x\;|\;0.2<x<0.7\}
	\end{split}
\end{equation*}
\begin{definition}
	Two events $A$ and $B$ are \textbf{mutually exclusive}, or \textbf{disjoint}, if $A\cap B=\varnothing$, that is $A$ and $B$ have nothing in common.
\end{definition}
For example, $E_{2}$ and $\overline{E}_{2}$ are mutually exclusive.
\begin{definition}
	The \textbf{union} of the two events $A$ and $B$, denoted by the symbol $A\cup B$ is the event containing all of the elements that belong to $A$ or $B$ or both.
\end{definition}
For example, $A\cup B=\{x\;|\;0<x<0.9\}$, and $(A\cup B)\subset S_{3}$. Note that $E_{2}\cup\overline{E}_{2}=S_{2}$ is an useful result and can be generalized to corollary below:
\begin{corollary}
	If $A$ is an event with respect to sample space $S$, then $A\cup\overline{A}=S$
\end{corollary}
De Morgan's laws can also be applied to set theory.
\begin{corollary}
	If $A$ and $B$ are events with respect to sample space $S$, then $$\overline{A\cap B}=\overline{A}\cup\overline{B}$$ $$\overline{A\cup B}=\overline{A}\cap\overline{B}$$
\end{corollary}
These results above are really useful in many cases, but you do not have prove them since they are pretty simple. Proving them rigorously is not the focus of Probability and Statistics book.
\section{Counting Sample Points}
In this section, we develop some \textit{counting techniques} to count the number of points in the sample space $S$ and its event subset $E$ without actually listing each elements. These techniques play an important role in solving some simple probability problems. Notice that these techniques can only be applied when your sample space is finite and countable.
\\ Obviously, you can not count the number of elements of $S_{3}$ and $S_{4}$, since they are infinite and uncountable sets.
\subsection{Rule of Product}
You toss a pair of coins. How many sample points are there in the sample space? First you can try to list all of the possible outcomes: $$S=\{HH,HT,TH,TT\}$$
There are total $4$ sample points in $S$. But listing all of the elements might be not so clever idea, so you use \textbf{rule of product}.
\\ The first coin can land heads or tails, so can the second coin. We multiply the number of possible outcomes of the two coins:
$$n_{1}n_{2}=2.2=4\text{ (possible outcomes)}$$
\begin{definition}
	If an operation can be performed in $n_{1}$ ways, and if for each of these a second operation can be performed in $n_{2}$ ways, and for each of the first two a third operation can be performed in $n_{3}$ ways, and so fourth, the the sequence of $k$ operations can be performed in:
	$$\prod_{i=1}^k n_{i}\text{ (ways)}$$
\end{definition}
\begin{figure}[h]
	\centering
	\includegraphics[width=13cm]{4.jpg}
	\caption{Tree diagram for rule of product}
	\label{Figure 4}
\end{figure}
\subsection{Permutations}
\begin{definition}
	A \textbf{permutation} is an arrangement of \textbf{all} or \textbf{part} of a set of objects.
\end{definition}
For instance, the number of permutations of $n$ distinct objects can be counted by using rule of product:
$$n(n-1)(n-2)\cdots3.2.1$$
We introduce a new notation for such a number.
\begin{definition}
	For any non-negative integer $n$, $n!$, called "$n$ factorial", is defined as:
	$$n!=n(n-1)(n-2)\cdots 3.2.1$$
	with special case $0!=1$
\end{definition}
\begin{theorem}
	The number of \textbf{permutations} of $n$ distinct objects is $n!$
\end{theorem}
In general, the number of permutations of $n$ distinct objects taken $r$ at a time can also be counted by using rule of product:
$$n(n-1)(n-2)\cdots(n-r+2)(n-r+1)$$
This product can be represented by the new symbol $nPr$.
\begin{theorem}
	The number of \textbf{permutations} of $n$ distinct objects taken $r$ at a time is:
	$$nPr=\frac{n!}{(n-r)!}$$
\end{theorem}
But how about our $n$ objects are not distinct? Assume that $n_{1}$ are of one kind, $n_{2}$ are of a second kind,$\cdots$ and $n_{k}$ of a $k$th kind.
\begin{theorem}
	The number of \textbf{permutations} of $n$ things of which $n_{1}$ are of one kind, $n_{2}$ of a second kind, and so forth is:
	$$\frac{n!}{n_{1}!n_{2}!\cdots n_{k}!}$$
	with $\sum_{i=1}^{k}n_{i}=n$.
\end{theorem}
For example, from the digits $1$ to $9$, we can form $9!$ numbers made up of $9$ distinct digits, or we can form $9P5$ five-digit numbers such that all the digits are different. If we allow repetation, five-digits numbers are now formed by $2$ digits 1, $2$ digits 2 and $1$ digit 3, then the number of permutation that satisfy is:
$$\frac{5!}{2!2!1!}=30$$
\subsection{Combinations}
Consider another problem, now you have a set of $n$ elements. How many ways you can partition it into $k$ cells with $n_{1}$ elements in the first cell, $n_{2}$ elements in the second cell, and so forth? Coincidentally, the equation in \textbf{Theorem 1.2.3} appears again.
\begin{theorem}
	The number of ways of partitioning a set of $n$ objects into $k$ cells with $n_{1}$ elements in the first cell, $n_{2}$ elements in the second, and so forth, is:
	$$\binom{n}{n_{1},n_{2},\cdots,n_{k}}=\frac{n!}{n_{1}!n_{2}!\cdots n_{k}!}$$
	with $\sum_{i=1}^{k}n_{i}=n$.
\end{theorem}
In many problems, we are interested in the number of ways of selecting $k$ objects from $n$ without regard to order. These selections are called \textbf{combinations}. It is not too hard to realize that a \textbf{combination} is just a partition with $2$ cells, one cell containing the $k$ objects and the other containing the $(n-k)$ objects.
\begin{equation*}
	\binom{n}{k,n-k} \text{ is often shortened to } \binom{n}{k}
\end{equation*}
\begin{definition}
	A \textbf{combination} is a selection of items from a set that has distinct elements, such that \textbf{the order of selection does not matter}.
\end{definition}
\begin{theorem}
	The number of combinations of $n$ distinct objects taken $k$ at a time is:
	$$\binom{n}{k}=\frac{n!}{k!(n-k)!}$$
\end{theorem}
For example, the number of subsets of $3$ elements that can be obtained from an original set of $10$ elements is: $$\binom{10}{3}=120$$
For the rest of this book, we mainly focus on \textbf{combinations}, and derive many probability distribution functions based on them. Like set theory, we do not go deeply into \textbf{counting techniques} since they are not the main concern of ProbStat.
\section{Probability of an Event}
Everyone knows the basic idea of probability. If I toss a \textbf{fair} coin once, and I want to know the probability of getting heads; how can I determine it? Strictly, I have to define $S$ is the sample space of this experiment and $A$ is the event "Getting heads after one toss".
$$S=\{H,T\}$$
$$A=\{H\}$$
So, the probability of event $A$ occurring is:
$$P(A)=\frac{\text{number of sample points inside } A}{\text{number of sample points inside } S}=\frac{1}{2}$$
Very easy and intuitive. But how about tossing 4 \textbf{fair} coins once, and determining the probability of getting $2$ heads? Now sample space $S$ and its event set can be represented as:
$$S=\{HHHH,HHHT,HHTH,\cdots\}$$
$$A=\{TTHH,THTH,\cdots\}$$
Now we change our strategy using \textbf{counting techniques}:
$$P(A)=\frac{\text{number of sample points inside } A}{\text{number of sample points inside } S}=\frac{\binom{4}{2}}{2^4}=\frac{3}{8}=0.375$$
In fact, the chance of getting heads is slightly greater than tails (you know, because coins are asymmetrical). Assume that the probability of heads appearing is $60\%$, and tails appearing is $40\%$. Since the role of \textbf{sample points} inside set $S$ and subset $A$ are not \textbf{equal} anymore, so we can not use \textbf{counting techniques} blindly.
$$P(A)=\binom{4}{2}0.6^2 0.4^2=0.3456$$
Coin tossing is a typical example of \textbf{Bernoulli trial}, and the experiment tossing unfair coins is a \textbf{Bernoulli process}. The probability $P(A)$ can be calculated by using \textbf{Binomial distribution} formula. You do not have to worry about these terms, we will cover them in \textbf{Chapter 4: Some Discrete Probability Distributions} very carefully.
\\ \\ \\
\indent Consider another problem, what is the probability of getting $0.7$ when randomly choosing a number (assume that the role of every numbers are equal) inside the interval $(0,1)$?
$$S=\{x\;|\;0<x<1\}$$
$$A=\{0.7\}$$
$$P(A)=\frac{\text{number of sample points inside } A}{\text{number of sample points inside } S}=\frac{1}{+\infty}=0\;(?????)$$
Since $P(A)=0$, so may we conclude 0.7 will never be chosen? Absolutely incorrect, even in common thinking. From previous examples, now we see the \textbf{limitation} of our "common definition" of probability in real life. Mathematically, our definition is just a very special case of the formal one.
\begin{definition}
	The \textbf{probability} of an event $A$ is the sum of the weights (or probabilities) of all sample points in $A$. Therefore:
	\begin{equation*}
		0\leq P(A)\leq 1,\quad P(\varnothing)=0,\quad P(S)=1
	\end{equation*}
	If $A$ and $B$ are \textbf{mutually exclusive (or disjoint)}, then $P(A\cup B)=P(A)+P(B)$
\end{definition}
This definition can also be called Kolmogorov's axioms. An intuitive way to understand it is sketching a sample space with some events inside.
\begin{figure}[h]
	\centering
	\includegraphics[width=14cm]{5.jpg}
	\caption{Visualizing definition of probability}
	\label{Figure 5}
\end{figure}
\\Literally, probability is just a number that describes how likely an event can occur. I often relate it with "weight" quantity. As you can see in the sample space, I can assign arbitrarily the "weights" (or probabilites) of sample points as follows:
\begin{itemize}
	\item Each blue point is $0.1$
	\item Each orange point is $0.2$
	\item Each pink point is $0.066$
\end{itemize}
Using \textbf{Definition 1.3.1}, now we can obtain these resuls:
$$P(A)=3.0.2=0.6$$
$$P(B)=2.0.1=0.2$$
$$P(A\cup B)=P(A)+P(B)=0.6+0.2=0.8$$
$$P(\overline{A\cup B})=3.0.066=0.2$$
$$P(S)=2.0.1+3.0.2+3.0.066=1$$
You should verify that our "special definition" of probability above satisfies the axioms of probability, and can be formalized by a theorem.
\begin{theorem}
	If an experiment can result in any one of $N$ different \textbf{equally} likely outcomes, and if exactly $n$ of these outcomes correspond to event $A$, then the probability of event $A$ is:
	$$P(A)=\frac{n}{N}$$
\end{theorem}
Logically, you can view probability as a mapping from a set to a closed interval $[0,1]$, then you can freely define the mapping $P$ by yourself as long as it satisfies Kolmogorov's axioms.
\begin{equation*}
	\begin{split}
		P:\text{Set}&\to[0,1]\\
		A&\xrightarrow{P} P(A)
	\end{split}
\end{equation*}
By applying set theory, we can derive several extremely useful theorems and corollaries:
\begin{theorem}
	If $A$ and $B$ are two events, then:
	$$P(A\cup B)=P(A)+P(B)-P(A\cap B)$$
\end{theorem}
\begin{theorem}
	If $A$ is an event of sample space $S$, then:
	$$P(S)=P(A\cup\overline{A})=P(A)+P(\overline{A})=1$$
\end{theorem}
\begin{corollary}
	If A is an event of sample space $S$, then:
	$$P(\overline{A})=1-P(A)$$
\end{corollary}
Again, De Morgan's laws can also be applied:
\begin{corollary}
	If $A$ and $B$ are events of sample space $S$, then:
	\begin{equation*}
		\begin{split}
			P(\overline{A\cup B})&=P(\overline{A}\cap\overline{B})\\
			P(\overline{A\cap B})&=P(\overline{A}\cup\overline{B})\\
		\end{split}
	\end{equation*}
\end{corollary}
\section{Conditional Probability}
\subsection{Conditional Probability}
Imagine you now have a perfectly fair coin. Obviously if you define events $A$ and $B$ are heads and tails appearing after one toss, respectively, you will conclude: $$P(A)=P(B)=0.5$$
But I might ask you "If event $A$ \textbf{did} occur, might event $B$ would have any chance to occur?"
. Since there is no way heads and tails can simultaneously appear, so your answer must be "No!". Now we can form an equation to represent the probability of a "special event" (or formally, conditional event) that "If $A$ occured, then $B$ would occur.":
$$P(B|A)=0$$
Now we shift our attention to a more complex problem. I give you a die, and you roll it. Events $E_{1}$ and $E_{2}$ can be defined as: "Landing a number that greater than 3" and "Landing a number that divisible by 2":
$$P(E_{1})=P(E_{2})=\frac{1}{3}$$
Sample points of $2$ events can be listed: $$E_{1}=\{4,5,6\}$$ $$E_{2}=\{2,4,6\}$$
If a number greater than 3 landed, what is the probability that it could be divisible by 2? You can count the number of sample points inside $E_{1}$ and draw a result:
$$P(E_{2}|E_{1})=\frac{2}{3}$$
Conversely, if a number divisible by 2 landed, what is the probability that it could be greater than $3$?
$$P(E_{1}|E_{2})=\frac{2}{3}$$
As you can see here, if one event occurs before another event, probability will be completely \textbf{changed}. So we call the pairs of events $A$ and $B$, $E_{1}$ and $E_{2}$ \textbf{dependent events} because they depend on each other. The "special" probabilities $P(B|A), P(E_{2}|E_{1}),P(E_{1}|E_{2})$ are called \textbf{conditional probability}.
\begin{definition}
	The \textbf{conditional probability} of $B$, given $A$, denoted by $P(B|A)$, is defined:
	$$P(B|A)=\frac{P(A\cap B)}{P(A)}$$
\end{definition}
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=13cm]{6.jpg}
	\caption{Visualizing how conditional probability is calculated}
	\label{Figure 6}
\end{figure}
Now back to our previous problems, these conditional probabilities can easily be determined using \textbf{Definition 1.4.1} above:
$$P(E_{1}|E_{2})=\frac{P(E_{1}\cap E_{2})}{P(E_{2})}=\frac{2/6}{1/2}=\frac{2}{3}$$
$$P(E_{2}|E_{1})=\frac{P(E_{1}\cap E_{2})}{P(E_{1})}=\frac{2/6}{1/2}=\frac{2}{3}$$
\subsection{Independent Events}
Intuitively, we can see that if events $A$ and $B$ do not influence each other, then:
$$P(B|A)=P(B)$$
$$P(B|A)=P(B)\Leftrightarrow \frac{P(B\cap A)}{P(A)}=P(B)\Leftrightarrow P(A\cap B)=P(B)(A)\;(\text{if }P(A)>0 )$$
\begin{definition}
	Two events $A$ and $B$ are independent if and only if: $$P(B|A)=P(B)$$
	assuming $P(A)>0$
\end{definition}
\begin{theorem}
	Two events $A$ and $B$ are independent if and only if $$P(A\cap B)=P(A)P(B)$$
\end{theorem}
There are many examples of independent events, like if we define 2 events $E_{1}$: "Getting heads on the first toss." and $E_{2}$: "Getting heads on the second toss.". Intuitively you can see that $E_{1}$ and $E_{2}$ are unrelated, so they are \textbf{independent events}. You can also verify this fact:
$$P(E_{1}\cap E_{2})=\frac{1}{2}.\frac{1}{2}=\frac{1}{4}=P(E_{1})P(E_{2})$$
It is very important to note that determining the independence of events is completely \textbf{unrelated} to their mutual exclusion. Events are considered independent if and only if they satisfy \textbf{Theorem 1.4.1}.
\newpage
\section{Total Probability and Bayes' rule}
\subsection{Total Probability}
In many situations, we do not know directly the information of $P(A)$ in \textbf{Definition 1.4.1} (or denomirator part); so in this section, we will develop a simple formula to handle this problem.
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{7.jpg}
	\caption{Total probability}
	\label{Figure 7}
\end{figure}
\begin{equation*}
	\begin{split}
		A=A\cap S=A\cap(B\cup \overline{B})=(A\cap B)\cup (A\cap \overline{B})
	\end{split}
\end{equation*}
Because $A\cap B$ and $A\cap \overline{B}$ are independent events, so:
\begin{equation*}
	P(A)=P(A\cap B)+P(A\cap \overline{B})=P(A|B)P(B)+P(A|\overline{B})P(\overline{B})
\end{equation*}
\begin{theorem}
	If $A$ and $B$ are two events of sample space $S$, then:
	\begin{equation*}
		P(A)=P(A|B)P(B)+P(A|\overline{B})P(\overline{B})
	\end{equation*}
\end{theorem}
Total probability is an useful formula, especially when you are conducting surveys in practice. For example, in my university, there are $2$ types of students: those who "studied the whole semester" and those who "studied only one night before the exam". After the final exam, I asked everyone in my class about their scores and totaled the results. Event $A$: "Got $A+$" and event $\overline{A}$: "Did not get $A+$"; event $B$: "Studied the whole semester" and event $\overline{B}$: "Studied only one night".
\begin{equation*}
	P(A|B)=0.8,\;P(A|\overline{B})=0.3,\;P(B)=0.4
\end{equation*}
Using total probability formula, I obtained the probability of $A$:
$$P(A)=P(A|B)P(B)+P(A|\overline{B})P(\overline{B})=0.8.0.4+0.3.(1-0.4)=0.5$$
Wow, that was an impressive ratio. Half of a class received perfect score. Was this course too easy?
\subsection{Bayes' rule}
If I studied hard, I would get $A+$. But how about me, who was not keen on studying boring courses like Computer Architecture but \textit{still survived after final test and even got "A+"}? I could questioned myself "Was I too lucky?". To answer myself, I had to calculate $P(\overline{B}|A)$, if the result is not so high, perhaps I was lucky. Bayes' rule was what I needed.
\begin{theorem}
	If $A$ and $B$ are two events of sample space $S$, then:
	\begin{equation*}
		P(B|A)=\frac{P(B\cap A)}{P(A)}=\frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|\overline{B})P(\overline{B})}
	\end{equation*}
\end{theorem}
Bayes' rule seems very simple. Indeed an average high school student has no difficulty finding it, but its idea is brilliant. Before Bayes, we typically only reasoned about problems in terms of cause first, then effect. But Bayes' rule opens up a completely new way of thinking for us: knowing the effect beforehand, then understanding how cause influences it.
\\ \\ \\
\indent Back to my previous problem, I applied Bayes' rule:
\begin{equation*}
	P(B|A)=\frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|\overline{B})P(\overline{B})}=\frac{0.8.0.4}{0.5}=0.64\Rightarrow P(\overline{B}|A)=1-P(B|A)=0.36
\end{equation*}
Since $0.36$ was not a high number, indeed I was incredibly lucky and the course was not as easy as I thought.
\chapter{Random Variables and Probability Distributions}
\section{Definition of Random Variables}
Back to our coin tossing game, now you toss a fair coin three times. This experiment has sample space $S$:
$$S=\{HHH,HHT,HTH,HTT,THH,THT,TTH,TTT\}$$
Since writing all of the sample points is quite lengthy and time-consuming, sometimes unnecessarily, so how about we assign each point with a \textbf{numerical value}? Because the assignment of values is entirely based on our own conventions, there are no constraints whatsoever. But for this reason, we should choose the \textbf{smartest} and \textbf{most convenient} way to assign values to suit our concern. For example, if I define an event $A$: "Getting $2$ heads", then the cleverest way is assigning each sample point with its number of heads.
\begin{equation*}
	\begin{split}
		HHH&\to3\\
		HHT,HTH,THH&\to2\\
		HTT,TTH,THT&\to1\\
		TTT&\to0\\
	\end{split}
\end{equation*}
Or event $B$: "Getting both tails and heads":
\begin{equation*}
	\begin{split}
		HHT,HTH,HTT,THH,THT,TTH,TTT&\to1\quad\text{(Yes)}\\
		HHH,TTT&\to0\quad\text{(No)}\\
	\end{split}
\end{equation*}
These values may be viewed as values assumed by the \textbf{random variable} $X$, $X$ can be "number of heads appearing" or "getting both tails and heads state", but \textit{not both}.
\begin{definition}
	A \textbf{random variable} is a function that associates a real number with each element in the sample space.
\end{definition}
We shall use a capital letter $X$, to denote the random variable and its corresponding smaller letter $x$, for \textbf{one of its values}. A typical mistake is forgetting to define the meaning of random variable $X$, so \textit{please do not forget it in your exam}. Let's continue by looking at some examples of random variables and sample spaces.
\\ Roll the die and observe the landing value. $X$ is the random variable defined as the value we observed:
$$S_{1}=\{1,2,3,4,5,6\}$$
Roll the die repeatedly until the sixth appears $6$ times. $Y$ is the random varibale defined as the number of rolling:
$$S_{2}=\{1,2,3,\cdots\}$$
Use \textbf{exponential distribution} to predict if tomorrow will be a rainy day. $Z$ is the random variable defined as the probability of the event "Tomorrow will be a rainy day":
$$S_{3}=\{z\;|\;0< z<1\}$$
The random variable $X$ can take one of the values $x_{1}=1,x_{2}=2,\cdots,x_{6}=6$, and similarly with $Y$ and $Z$ can take one of their own values in their sample spaces $S_{2}$, $S_{3}$ respectively. Now we interested in classifying $2$ types of sample spaces and their random variables.
\begin{definition}
	If a sample space contains a finite number of possibilities or an unending sequence with as many elements as there are whole numbers, it is called a \textbf{discrete sample space}.
\end{definition}
\begin{definition}
	If a sample space contains an infinite number of possibilities equal to the number of points on a line segment, it is called a \textbf{continuous sample space}.
\end{definition}
So $S_{1}$ and $S_{2}$ are \textbf{discrete sample spaces} and $X$, $Y$ are called \textbf{discrete random variables}; $S_{3}$ is \textbf{continuous sample space} and $Z$ is called \textbf{continuous random variable}.
\section{Discrete Probability Distributions}
Consider coin tossing experiment, now if we assign random variable $X$ as the number of heads appearing, I can obtain some useful results:
\begin{equation*}
	\begin{split}
		P(X=3)&=P(\{HHH\})=\frac{1}{8}=0.125\\
		P(X=2)&=P(\{HHT,HTH,THH\})=\frac{3}{8}=0.375\\
		P(X=1)&=P(\{TTH,THT,HTT\})=\frac{3}{8}=0.375\\
		P(X=0)&=P(\{TTT\})=\frac{1}{8}=0.125\\
	\end{split}
\end{equation*}
Or in table form:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$x$      & $0$     & $1$     & $2$     & $3$     \\
		\hline
		$P(X=x)$ & $0.125$ & $0.375$ & $0.375$ & $0.125$ \\
		\hline
	\end{tabular}
\end{center}
Frequently, it is much convenient to represent all of the probabilities of a random variable $X$ by a \textbf{formula}.
\begin{definition}
	The function $f(x)$ is a \textbf{probability density function} (pdf) of the discrete random variable $X$ if, for each possible outcome $X$:
	\begin{equation*}
		\begin{cases}
			f(x)\geq0      \\
			\sum_{x}f(x)=1 \\
			P(X=x)=f(x)    \\
		\end{cases}
	\end{equation*}
\end{definition}
You can derive that the pdf of coin tossing experiment above is:
$$f(x)=P(X=x)=\frac{\binom{3}{x}}{2^3}\quad(x=0,1,2,3)$$
In many cases, we want to know the probability of $(X\leq x)$ (you will see it clearly in the next section). For instance, I want to know the probability of heads appearing a maximum of $2$ times.
$$P(X\leq2)=P(X=0)+P(X=1)+P(X=2)=0.125+0.375+0.375=0.875$$
In general, we define a new function $F(x)$ to handle these cases as follows:
\begin{definition}
	The \textbf{cumulative distribution function} (cdf) $F(x)$ of a discrete random variable $X$ with probability distribution $f(x)$ is:
	$$F(x)=P(X\leq x)=\sum_{t\leq x}f(t)\quad {(-\infty<x<+\infty)}$$
\end{definition}
For example, in the above experiment:
\begin{equation*}
	F(x)=
	\begin{cases}
		0\quad(x<0)           \\
		0.125\quad(0\leq x<1) \\
		0.5\quad(1\leq x<2)   \\
		0.875\quad(2\leq x<3) \\
		1\quad(x\geq 3)       \\
	\end{cases}
\end{equation*}
\section{Continuous Probability Distributions}
Choosing randomly a number within the range $(0,1)$. $X$ is the random variable, defined as the chosen one. In the previous chapter, we have discussed that the probability of getting \textbf{a single number} in the range $(0,1)$ is $0$.
$$P(X=0.7)=0$$
So $0.7$ will never be chosen since $P(X=0.7)=0$? Now think carefully about the reasons why the probability of an event might be zero. Recall the \textbf{Theorem 1.3.1}:
$$P(A)=\frac{n}{N}$$
There are $2$ main reasons that could explain why $P(A)$ can be zero; the first one is \textbf{$n=0$ and $N$ is a finite number}, and the second one is $\textbf{$n$ is a finite number and $N$ is an infinite number}$. This might be the big misconception, since people always claim that the only reason for $P(A)=0$ is the first one, and forgot the second. But now you can clearly see that if $n>0$, event will always have a chance of happening. So now we conclude certainly: "$P(A)=0$ does not mean event $A$ will never happen."\\
If sample space $S$ is \textit{continuous sample space}, which contains \textit{an infinite number of possibilities equal to the number of points on a line segment}, we do not care about the probability of \textbf{a single sample point} occurring (because it is always equal $0$). We  shift our attention to the probability of \textbf{the interval that our concern sample point may be fallen  inside} occurring.
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{8.jpg}
	\caption{What is the probability that a number will fall within this range?}
	\label{Figure 8}
\end{figure}
Intuitively you can conclude that: $$P(0.6<X<0.78)=0.78-0.6=0.18$$
Similarly with the discrete random variables, we can also define:
\begin{definition}
	The function $f(x)$ is a \textbf{probability distribution (or density) function} (pdf) for the continuous random variable $X$, defined over the set of real numbes, if:
	\begin{equation*}
		\begin{cases}
			f(x)\geq0,\text{ for all }x\in R \\
			\int_{-\infty}^{+\infty}f(x)dx=1 \\
			P(a<X<b)=\int_{a}^{b}f(x)dx
		\end{cases}
	\end{equation*}
\end{definition}
Verifying yourself that the pdf of number choosing experiment is:
\begin{equation*}
	f(x)=
	\begin{cases}
		1\quad(0<x<1)            \\
		0\quad(\text{elsewhere}) \\
	\end{cases}
\end{equation*}
This pdf is the simplest case of \textbf{uniform distribution function}. As I mentioned before, in the continuous sample space case, concerning on the probability of $(X=x)$ does not not make any sense since it is equal $0$ and we can not use any information about it. Instead, we turn our focus to the probability of $(X<x)$ and define the \textbf{cumulative distribution function} $F(x)$ as follows:
\begin{definition}
	The \textbf{cumulative distribution function} (cdf) $F(x)$ of a continuous random variable $X$ with pdf $f(x)$ is:
	\begin{equation*}
		F(x)=P(X<x)=\int_{-\infty}^{x}f(t)dt\quad(-\infty<x<+\infty)
	\end{equation*}
\end{definition}
The cdf of number choosing experiment is:
\begin{equation*}
	F(x)=
	\begin{cases}
		0\quad(x<0)       \\
		x\quad(0\leq x<1) \\
		1\quad(x\geq 1)   \\
	\end{cases}
\end{equation*}
\section{Joint Probability Distributions}
\subsection{Case of Discrete Random Variables}
In the previous sections, we have considered \textbf{single random variable} and its \textbf{probability distribution function} case, and restricted ourshelves to \textbf{one-dimension sample space}. But now we are interested in observing  the \textbf{simultaneous outcomes} of \textbf{several random variables}. For example, you toss $2$ fair coins simultaneously and observe their appearing faces. You define $2$ random variables, $X$ for the first coin face, and $Y$ for the second one. The heads is assigned value $1$, and the tails is assigned value $0$. These are some results obtained from this experiment:
$$P(X=0,Y=0)=\frac{1}{2}.\frac{1}{2}=\frac{1}{4}=0.25$$
$$P(X=0,Y=1)=\frac{1}{2}.\frac{1}{2}=\frac{1}{4}=0.25$$
$$P(X=1,Y=0)=\frac{1}{2}.\frac{1}{2}=\frac{1}{4}=0.25$$
$$P(X=1,Y=1)=\frac{1}{2}.\frac{1}{2}=\frac{1}{4}=0.25$$
These results can be written in table form:
\begin{center}
	\begin{tabular}{ |c|c|c| }
		\hline
		$P(X=x,Y=y)$ & $Y=0$ & $Y=1$ \\
		\hline
		$X=0$        & 0.25  & 0.25  \\
		\hline
		$X=1$        & 0.25  & 0.25  \\
		\hline
	\end{tabular}
\end{center}
Another classic example of \textbf{discrete joint probability distribution} is the problem of picking balls from a basket. Now you have a basket with many colorful balls inside; there are $3$ red, $4$ green and $5$ blue balls. You choose randomly $3$ balls from the basket, and you define $2$ random variables $X$ and $Y$; $X$ is the number of red balls and $Y$ is the number of green balls. Using counting techniques and combinations, you can write the \textbf{joint pdf}:
$${f(x,y)}=P(X=x,Y=y)=\frac{\binom{3}{x}\binom{4}{y}\binom{5}{3-x-y}}{\binom{12}{3}}$$
Or in table form:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c| }
		\hline
		$P(X=x,Y=y)$ & $Y=0$           & $Y=1$          & $Y=2$           & $Y=3$          \\
		\hline
		$X=0$        & $\frac{1}{22}$  & $\frac{2}{11}$ & $\frac{3}{22}$  & $\frac{1}{55}$ \\
		\hline
		$X=1$        & $\frac{3}{22}$  & $\frac{3}{11}$ & $\frac{9}{110}$ & -              \\
		\hline
		$X=2$        & $\frac{3}{44}$  & $\frac{3}{55}$ & -               & -              \\
		\hline
		$X=3$        & $\frac{1}{220}$ & -              & -               & -              \\
		\hline
	\end{tabular}
\end{center}
\begin{definition}
	The function $f(x,y)$ is a \textbf{joint pdf} of the \textbf{discrete random variables} $X$ and $Y$ if:
	\begin{equation*}
		\begin{cases}
			f(x,y)\geq 0             \\
			\sum_{x}\sum_{y}f(x,y)=1 \\
			P(X=x,Y=y)=f(x,y)        \\
		\end{cases}
	\end{equation*}
\end{definition}
\begin{corollary}
	For any region $A$ in the $xy$ plane:
	$$P[(X,Y)\in A]=\sum\sum_{A}f(x,y)$$
\end{corollary}
After considering the case where both variables $X$ and $Y$ are both varying, what if we \textbf{fix} one variable and vary the other? You can see this idea appearing very naturally in the process of finding the values of the above table using a calculator. We introduce the new functions called \textbf{marginal distributions} of $X$ and $Y$ alone.
\begin{definition}
	The \textbf{marginal distributions} of $X$ alone and of $Y$ alone for the \textbf{discrete case} are:
	$$g(x)=\sum_{y}f(x,y);\quad h(y)=\sum_{x}f(x,y)$$
\end{definition}
Since the general form of $g(x)$ and $h(y)$ are not easy to be generalized, and the range of discrete random variables $X$ and $Y$ is very narrow, so we should reuse the table above to obtain values of these functions.
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$P(X=x,Y=y)$ & $Y=0$                & $Y=1$                & $Y=2$                & $Y=3$               & $g(x)$                \\
		\hline
		$X=0$        & $\frac{1}{22}$       & $\frac{2}{11}$       & $\frac{3}{22}$       & $\frac{1}{55}$      & $g(0)=\frac{21}{55}$  \\
		\hline
		$X=1$        & $\frac{3}{22}$       & $\frac{3}{11}$       & $\frac{9}{110}$      & -                   & $g(1)=\frac{27}{55}$  \\
		\hline
		$X=2$        & $\frac{3}{44}$       & $\frac{3}{55}$       & -                    & -                   & $g(2)=\frac{27}{220}$ \\
		\hline
		$X=3$        & $\frac{1}{220}$      & -                    & -                    & -                   & $g(3)=\frac{1}{220}$  \\
		\hline
		$h(y)$       & $h(0)=\frac{14}{55}$ & $h(1)=\frac{28}{55}$ & $h(2)=\frac{12}{55}$ & $h(3)=\frac{1}{55}$ & $1$                   \\
		\hline
	\end{tabular}
\end{center}
\begin{corollary}
	If $g(x)$ and $h(y)$ are marginal distributions of $X$ alone and $Y$ alone for the \textbf{discrete case}, then:
	$$\sum_{x}g(x)=\sum_{y}h(y)=1$$
\end{corollary}
\textbf{Corollary 2.4.0.2} is directly derived from the \textbf{Definition 2.4.1}, and you can verify them by using the probability distribution table.
\\ After defining the \textbf{marginal distribution}, now we can see clearly the connection between them and regular pdfs are:
\begin{equation*}
	P(X=x)=g(x);\quad P(Y=y)=h(y)
\end{equation*}
Now if I choose $3$ balls from the basket, and I know two of them are green, what is the probability that the remaining ball is red?
\begin{equation*}
	P(X=1|Y=2)=\frac{P(X=1,Y=2)}{P(Y=2)}=\frac{\frac{9}{110}}{\frac{12}{55}}=\frac{3}{8}=0.375
\end{equation*}
In general, it is not hard to deduce these formulas:
\begin{equation*}
	\begin{split}
		f(x|y)&=P(X=x|Y=y)=\frac{P(X=x,Y=y)}{P(Y=y)}=\frac{f(x,y)}{h(y)}\\
		f(y|x)&=P(Y=y|X=x)=\frac{P(Y=y,X=x)}{P(X=x)}=\frac{f(x,y)}{g(x)}\\
	\end{split}
\end{equation*}
\begin{definition}
	Let $X$ and $Y$ be two \textbf{discrete random variables}. The \textbf{conditional distribution} of the variable $Y$ given that $(X=x)$ is:
	$$f(y|x)=\frac{f(x,y)}{g(x)}$$
	Similarly for the reverse case:
	$$f(x|y)=\frac{f(x,y)}{h(y)}$$
\end{definition}
From the previous section \textbf{Chapter 1.4: Conditional Probability}, you already knew how to identify two independent events, by checking this condition:  $$P(A\cap B)=P(A)P(B)$$
Now we use it again, with random variables $X$ and $Y$:
\begin{equation*}
	P(X=x,Y=y)=P(X=x)P(Y=y)\Leftrightarrow f(x,y)=g(x)h(y)
\end{equation*}
\begin{definition}
	Let $X$ and $Y$ be two \textbf{discrete random variables}, with joint pdf $f(x,y)$ and marginal distributions $g(x)$, $h(y)$, respectively. They are said to be \textbf{statistically independent} if and only if: $$f(x,y)=g(x)h(y)$$
\end{definition}
If we check the case $(X=1,Y=1)$: $$\left(f(1,1)=\frac{3}{11}\right)\neq \left(g(1)h(1)=\frac{27}{55}.\frac{28}{55}\right)$$
So $X$ and $Y$ are \textbf{not} statistically independent, they are interdependent. But how interdependent are they? Are they highly or minimally interdependent? This question will be answered at the end of the next chapter.
\subsection{Case of Continuous Random Variables}
Similarly, the joint pdf of \textbf{continuous random variable} can also be defined as follows:
\begin{definition}
	The function $f(x,y)$ is a \textbf{joint pdf} of the \textbf{continuous random variables} $X$ and $Y$ if:
	\begin{equation*}
		\begin{cases}
			f(x,y)\geq 0                                                 \\
			\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x,y)dxdy=1 \\
			P(a<X<b,c<Y<d)=\int_{c}^{d}\int_{a}^{b}f(x,y)dxdy            \\
		\end{cases}
	\end{equation*}
\end{definition}
\begin{corollary}
	For any region $A$ in the $xy$ plane:
	$$P[(X,Y)\in A]=\iint_{A}f(x,y)dxdy$$
\end{corollary}
Almost all the formulas in the above section are redefined exactly the same, with a slight difference in the notation for the integral (you can read more about Riemann sum to see the relationship between integrals and infinite discrete sums).
\begin{definition}
	The \textbf{marginal distributions} of $X$ alone and of $Y$ alone for the \textbf{continuous case} are:
	$$g(x)=\int_{-\infty}^{+\infty}f(x,y)dy;\quad h(y)=\int_{-\infty}^{+\infty}f(x,y)dx$$
\end{definition}
\begin{corollary}
	If $g(x)$ and $h(y)$ are marginal distributions of $X$ alone and $Y$ alone for the \textbf{continuous case}, then:
	$$\int_{-\infty}^{+\infty}g(x)dx=\int_{-\infty}^{+\infty}h(y)dy=1$$
\end{corollary}
\begin{definition}
	Let $X$ and $Y$ be two \textbf{continuous random variables}. The \textbf{conditional distribution} of the variable $Y$ given that $(X=x)$ is:
	$$f(y|x)=\frac{f(x,y)}{g(x)}$$
	Similarly for the reverse case:
	$$f(x|y)=\frac{f(x,y)}{h(y)}$$
\end{definition}
But notice that if $X$ and $Y$ are continuous random variables, then $P(X=x|Y=y)$ is always equal zero! The correct way to obtain conditional probability value is shown below:
\begin{corollary}
	If $X$ and $Y$ be two \textbf{continuous random variables}, then:
	$$P(a<X<b|Y=y)=\int_{a}^{b}f(x|y)dx$$
\end{corollary}
\begin{definition}
	Let $X$ and $Y$ be two \textbf{continuous random variables}, with joint pdf $f(x,y)$ and marginal distributions $g(x)$, $h(y)$, respectively. They are said to be \textbf{statistically independent} if and only if: $$f(x,y)=g(x)h(y)$$
\end{definition}
\chapter{Mathematical Expectation}
Due to the relatively high degree of similarity between the formulas in the case of discrete and continuous random variables, this chapter approaches them \textbf{in parallel}, rather than separating them like previous chapter.
\section{Mean of a Random Variable}
After conducting experiment, now it is time to process your obtained results. Toss a fair coin for $3$ times, and define the discrete random variable $X$ as the number of heads appearing. The pdf  of this experiment is:
$$f(x)=P(X=x)=\frac{\binom{3}{x}}{2^3}\quad(x=0,1,2,3)$$
Or in table form:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$x$      & $0$     & $1$     & $2$     & $3$     \\
		\hline
		$P(X=x)$ & $0.125$ & $0.375$ & $0.375$ & $0.125$ \\
		\hline
	\end{tabular}
\end{center}
What is the \textbf{mean} or \textbf{expected value} of $X$? Intuitively, you will use the formula:
$$\mu_{X}=E(X)=\sum_{x}xf(x)=0.0.125+1.0.375+2.0.375+3.0.125=1.5$$
By illustrating the above results with a graph, the position of $\mu$ is shown.
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{9.jpg}
	\caption{Pdf of fair coin tossing experiment with its mean}
	\label{Figure 9}
\end{figure}
\\If our coin is unfair, assume that the probability of getting heads is $p=0.3$, then the probability of getting tails is $q=1-p=0.7$. Now the pdf is:
$$f(x)=P(X=x)=\binom{3}{x}p^{x}q^{3-x}=\binom{3}{x}0.3^{x}0.7^{3-x}\quad(x=0,1,2,3)$$
Or in table form:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$x$      & $0$     & $1$     & $2$     & $3$     \\
		\hline
		$P(X=x)$ & $0.343$ & $0.441$ & $0.189$ & $0.027$ \\
		\hline
	\end{tabular}
\end{center}
In this experiment, we \textbf{expect} to get the \textbf{average value} (or mean) of $X$:
$$\mu_{X}=E(X)=\sum_{x}xf(x)=0.0.343+1.0.441+2.0.189+3.0.027=0.9$$
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{10.jpg}
	\caption{Pdf of unfair coin tossing experiment with its mean}
	\label{Figure 10}
\end{figure}
\begin{definition}
	Let $X$ be a random variable with pdf $f(x)$. The \textbf{expected value} or \textbf{mean} of $X$ is:
	$$\mu_{X}=E(X)=\sum_{x}xf(x)\quad(\text{if $X$ is discrete})$$
	$$\mu_{X}=E(X)=\int_{-\infty}^{+\infty}{x}f(x)dx\quad(\text{if $X$ is continuous})$$
\end{definition}
Now imagine you are playing coin tossing game (do not treat it like an experiment). As I mention the rule above, you have $3$ turns to toss a coin. If you see $3$ heads appear, you are extremely lucky today; but if you do not see any, do not be sad because life is long. A very natural thought occurred to me that we should quantify a player's "luck level" with a quantitative value. Random variable $Y$ takes this role and can be defined as:
$$Y=\frac{X}{3}$$
Now we are intersted in average "luck level" of coin tossing game, so reuse the previous table that we have created:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$x$             & $0$     & $1$     & $2$     & $3$     \\
		\hline
		$y=\frac{x}{3}$ & $0$     & $0.333$ & $0.666$ & $1$     \\
		\hline
		$P(X=x)$        & $0.343$ & $0.441$ & $0.189$ & $0.027$ \\
		\hline
	\end{tabular}
\end{center}
The \textbf{expected value} or \textbf{mean} of "luck level" is:
$$\mu_{Y}=E(Y)=\sum_{y}yf(x)=0.0.343+0.333.0.441+0.666.0.189+1.0.027=0.3$$
Since $0.3$ is not so high value, so the \textbf{mean} of "luck level" is pretty small and players should not look forward to their chances in this game. Furthermore, you should notice the subtle connection between two random variables $X$ and $Y$ as:
$$Y=\frac{X}{3}\Leftrightarrow E(Y)=\frac{E(X)}{3}$$
\begin{figure}[h]
	\centering
	\includegraphics[width=8cm]{11.jpg}
	\caption{The mean value of "luck level"}
	\label{Figure 11}
\end{figure}
\\ If $Y$ is defined as \textbf{function of random variable $X$} or $(Y=g(X))$, then the \textbf{expected value} of it can be calculated using the theorem below:
\begin{theorem}
	Let $X$ be a random variable with pdf $f(x)$, and the \textbf{expected value} of the random variable $g(X)$ is:
	$$\mu_{g(X)}=E(g(X))=\sum_{x}g(x)f(x)\quad\text{(if X is discrete)}$$
	$$\mu_{g(X)}=E(g(X))=\int_{-\infty}^{+\infty}g(x)f(x)dx\quad\text{(if X is continuous)}$$
\end{theorem}
Now I want to make our game funnier because just tossing a coin is too boring, so I came up with an idea. How about toss  $3$ unfair coins and the bottle cap \textbf{at the same time}? The random variable $X$ is defined as the number of heads appearing with the probability of getting heads is $p=0.3$, and getting tails is $q=1-p=0.7$, so the pdf is:
$$g(x)=P(X=x)=\binom{3}{x}p^{x}q^{3-x}=\binom{3}{x}0.3^{x}0.7^{3-x}\quad(x=0,1,2,3)$$
A 'PEPSI' bottle cap has just been founded, so I define a new random variable $Y$ as its face after tossing. The 'PEPSI' face is assigned the value $1$, and the other is assigned the value $0$. Assume that:
\begin{equation*}
	\begin{split}
		P(Y=1)&=p'=0.8\\
		P(Y=0)&=q'=1-p'=0.2\\
	\end{split}
\end{equation*}
So the pdf of bottle cap tossing experiment is: $$h(y)=P(Y=y)=(p')^{y}(q')^{1-y}=0.8^{y}0.2^{1-y}\quad(y=0,1)$$
Intuitively, you can certainly conclude that $X$ and $Y$ are \textbf{statistically independent} since coins and bottle cap tossing process do not affect each other. So the \textbf{joint pdf} of them is:
$$f(x,y)=g(x)h(y)=\binom{3}{x}0.3^{x}0.7^{3-x}0.8^{y}0.2^{1-y}\quad(x=0,1,2,3;\;y=0,1)$$
Or in table form:
\begin{center}
	\begin{tabular}{ |c|c|c|c|c|c| }
		\hline
		$P(X=x,Y=y)$ & $X=0$        & $X=1$        & $X=2$        & $X=3$        & $h(y)$     \\
		\hline
		$Y=0$        & $0.0686$     & $0.0882$     & $0.0378$     & $0.0054$     & $h(0)=0.2$ \\
		\hline
		$Y=1$        & $0.2744$     & $0.3528$     & $0.1512$     & $0.0216$     & $h(1)=0.8$ \\
		\hline
		$g(x)$       & $g(0)=0.343$ & $g(1)=0.441$ & $g(2)=0.189$ & $g(3)=0.027$ & $1$        \\
		\hline
	\end{tabular}
\end{center}
Now "luck level" can be defined as the value of $(X+Y)$. Thinking according to the same logic, the \textbf{mean} value of $(X+Y)$ is:
\begin{equation*}
	\begin{split}
		\mu_{(X+Y)}&=E(X+Y)=\sum_{x}\sum_{y}(x+y)f(x,y)=\sum_{x}\left(\sum_{y}(x+y)f(x,y))\right)\\
		&=\sum_{x}(xf(x,0)+xf(x,1)+0f(x,0)+1f(x,1))\\
		&=\sum_{x}\left(xf(x,0)+xf(x,1)+f(x,1)\right)\\
		&=0f(0,0)+1f(1,0)+2f(2,0)+3f(3,0)+0f(0,1)+1f(1,1)\\&+2f(2,1)+3f(3,1)+f(0,1)+f(1,1)+f(2,1)+f(3,1)\\
		&=0.0882+2.0.0378+3.0.0054+0.3528+2.0.1512\\&+3.0.00216+0.2744+0.3528+0.1512+0.0216\\
		&=1.7
	\end{split}
\end{equation*}
So now, the players can confidently look forward for their opportunity in this game!
\begin{definition}
	Let $X$ and $Y$ be random variables with \textbf{joint pdf} $f(x,y)$. The \textbf{mean} or \textbf{expected value} of the random variable function $g(X,Y)$ is:
	$$\mu_{g(X,Y)}=E(g(X,Y))=\sum_{x}\sum_{y}g(x,y)f(x,y)\quad\text{(if $X$ and $Y$ are discrete)}$$
	$$\mu_{g(X,Y)}=E(g(X,Y))=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} g(x,y)f(x,y)dxdy\quad\text{(if $X$ and $Y$ are continuous)}$$
\end{definition}
If $g(X,Y)=X$, then we obtain some useful results and can be represented as corollaries:
\begin{corollary}
	Let $X$ and $Y$ be discrete random variables with \textbf{joint pdf} $f(x,y)$, the \textbf{mean} of $X$ is:
	$$\mu_{X}=E(X)=\sum_{x}\sum_{y}xf(x,y)=\sum_{x}x\left(\sum_{y}f(x,y)\right)=\sum_{x}xg(x)$$
	Similarly, the mean of $Y$ is: $$\mu_{Y}=E(Y)=\sum_{y}yh(y)$$
\end{corollary}
\begin{corollary}
	Let $X$ and $Y$ be continuous random variables with \textbf{joint pdf} $f(x,y)$, the \textbf{mean} of $X$ is:
	$$\mu_{X}=E(X)=\int_{-\infty}^{+\infty}xg(x)dx$$
	Similarly, the mean of $Y$ is: $$\mu_{Y}=E(Y)=\int_{-\infty}^{+\infty}yh(y)dy$$
\end{corollary}
\section{Variance and Covariance of Random Variables}
\subsection{Variance of Random Variables}
The concept of \textbf{mean} or \textbf{expected value} is very important in ProbStat. In any probability distribution graph, we use the \textbf{mean value} as \textbf{main reference point} (although the mean is not always in the center of the graph). We are very interested in how the data is distributed \textbf{around the mean value}. Is it distributed far or close to it? How does it spread?
\begin{figure}[h]
	\centering
	\includegraphics[width=12cm]{12.jpg}
	\caption{How is the data distributed around the \textbf{mean}?}
	\label{Figure 12}
\end{figure}
\\The sums of total squares of the distances from the mean are:
\begin{equation*}
	\begin{split}
		\sum&=(0-1.5)^2+(1-1.5)^2+(2-1.5)^2+(3-1.5)^2=5\text{ (fair coin case)}\\
		\sum&=(0-0.9)^2+(1-0.9)^2+(2-0.9)^2+(3-0.9)^2=6.44\text{ (unfair coin case)}\\
	\end{split}
\end{equation*}
Because $6.44>5$, and you can also see on the graph, the data from the unfair coin toss experiment is more \textbf{widely distributed} compare to the fair coin case. For simplicity, we often take \textbf{the average of sums} and call it \textbf{variance}.
\begin{definition}
	Let $X$ be a random variable with pdf $f(x)$ and mean $\mu$. The \textbf{variance} of $X$ is:
	$$\sigma_{X}^2=E((X-\mu_{X})^2)=\sum_{x}(x-\mu_{X})^2f(x)\quad\text{(if $X$ is discrete)}$$
	$$\sigma_{X}^2=E((X-\mu_{X})^2)=\int_{-\infty}^{+\infty}(x-\mu_{X})^2f(x)dx\quad\text{(if $X$ is continuous)}$$
	The positive square root of the variance, $\sigma_{X}$, is called the \textbf{standard deviation} of $X$.
\end{definition}
\begin{theorem}
	The \textbf{variance} of a random variable $X$ is:
	$$\sigma_{X}^2=E(X^2)-\mu^2_{X}$$
\end{theorem}
\begin{proof} We only prove for the discrete case because proving for the continuous case using exactly the same logic:
	\begin{equation*}
		\begin{split}
			\sigma_{X}^2&=E((X-\mu_{X})^2)=\sum_{x}(x-\mu_{X})^2f(x)=\sum_{x}x^2f(x)-2\mu_{X}\sum_{x}x f(x)+\mu_{X}^2\sum_{x} f(x)\\
			&=E(X^2)-2\mu_{X}^2+\mu_{X}^2=E(X^2)-\mu_{X}^2
		\end{split}
	\end{equation*}
\end{proof}
At this point you may wonder "Why do not we care about the sum of total \textbf{distances} itself?" Well, there are $3$ main reasons; the first is absolutely avoiding \textbf{negative values} from miscalculation, the second is working with $\sigma_{X}^2$ is much easier than $\sigma_{X}$ and you will understand the final reason after reading \textbf{Chapter 10: Simple Linear Regression}. Using a completely similar line of thinking as before, we can also develop the following formulas:
\begin{theorem}
	Let $X$ be a random variable with pdf $f(X)$. The \textbf{variance} of the random variable $g(X)$ is:
	$$\sigma^{2}_{g(X)}=E((g(X)-\mu_{g(X)})^2)=\sum_{x}(g(x)-\mu_{g(X)})^2f(x)\quad\text{(if $X$ is discrete)}$$
	$$\sigma^{2}_{g(X)}=E((g(X)-\mu_{g(X)})^2)=\int_{-\infty}^{+\infty}(g(x)-\mu_{g(X)})^2f(x)dx\quad\text{(if $X$ is continuous)}$$
\end{theorem}
\begin{corollary}
	The \textbf{variance} of random variable $g(X)$ is:
	$$\sigma^2_{g(X)}=E(g(X)^2)-\mu^2_{g(X)}$$
\end{corollary}
\begin{proof} We only prove for the discrete case:
	\begin{equation*}
		\begin{split}
			\sigma_{g(X)}^2&=E((g(X)-\mu_{g(X)})^2)=\sum_{x}(g(x)-\mu_{g(X)})^2f(x)\\&=\sum_{x}g(x)^2f(x)-2\mu_{g(X)}\sum_{x}g(x) f(x)+\mu_{g(X)}^2\sum_{x} f(x)\\
			&=E(g(X)^2)-2\mu_{g(X)}^2+\mu_{g(X)}^2=E(g(X)^2)-\mu_{g(X)}^2
		\end{split}
	\end{equation*}
\end{proof}
\subsection{Covariance of Random Variables}
In this subsection, we will answer the question: "How are $2$ random variables $X$ and $Y$ interdependent?" after checking the \textbf{statistically independent criterion}:
$$f(x,y)=g(x)h(y)$$
We define a new concept called \textbf{covariance}, which is \textbf{a measure of the nature of the association between the two}.
\begin{definition}
	Let $X$ and $Y$ be random variables with joint pdf $f(x,y)$. The covariance of $X$ and $Y$ is:
	$$\sigma_{XY}=E((X-\mu_{X})(Y-\mu_{Y}))=\sum_{x}\sum_{y}(x-\mu_{X})(y-\mu_{Y})f(x,y)\quad\text{(if $X$ and $Y$ are discrete)}$$

	$$\sigma_{XY}=E((X-\mu_{X})(Y-\mu_{Y}))=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}(x-\mu_{X})(y-\mu_{Y})f(x,y)dxdy\quad\text{(if $X$ and $Y$ are continuous)}$$
\end{definition}
Applying directly \textbf{Definition 3.2.2} is not convenient in many situations, so we should rewrite its formula:
\begin{corollary}
	The \textbf{covariance} of random variable $X$ and $Y$ is:
	\begin{equation*}
		\sigma_{XY}=E(XY)-\mu_{X}\mu_{Y}
	\end{equation*}
\end{corollary}
\begin{proof} We only prove for the discrete case:
	\begin{equation*}
		\begin{split}
			\sigma_{XY}&=\sum_{x}\sum_{y}(x-\mu_{X})(y-\mu_{Y})f(x,y)\\
			&=\sum_{x}\sum_{y}xyf(x,y)-\mu_{Y}\sum_{x}\sum_{y}xf(x,y)-\mu_{X}\sum_{x}\sum_{y}yf(x,y)+\mu_{X}\mu_{Y}\sum_{x}\sum_{y}f(x,y)\\
			&=E(XY)-\mu_{Y}\sum_{x}xg(x)-\mu_{X}\sum_{y}yh(y)+\mu_{X}\mu_{Y}\\
			&=E(XY)-\mu_{Y}\mu_{X}-\mu_{X}\mu_{Y}+\mu_{X}\mu_{Y}\\
			&=E(XY)-\mu_{X}\mu_{Y}
		\end{split}
	\end{equation*}
\end{proof}
Although the \textbf{covariance} between two random variables does provide information regarding the nature of the relationship, the magnitude of $\sigma_{XY}$ does not indicate anything regarding the strength of the relationship, since $\sigma_{XY}$ is not scale-free quantity. There is a scale-free version of the covariance called the \textbf{correlation coefficient} that is widely used in ProbStat.
\begin{definition}
	Let $X$ and $Y$ be random variables with \textbf{covariance} $\sigma_{XY}$ and \textbf{standard deviations} $\sigma_{X}$ and $\sigma_{Y}$, respectively. The \textbf{correlation coefficient} of $X$ and $Y$ is: $$\rho_{XY}=\frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}}$$
\end{definition}
\begin{theorem}
	The \textbf{absolute value} of \textbf{correlation coefficient} $\rho_{XY}$ is always smaller or equal $1$.
	$$|\rho_{XY}|\leq 1$$
\end{theorem}
\begin{proof} Proving this theorem above is quiet difficult and requires knowledge of \textbf{Linear Algebra}. I recommend skipping this proof and moving on to the next section to learn some \textbf{useful theorems} first, and then back here later.
	\\ In the \textbf{random variables vector space} sharing the same joint pdf, we now define the \textbf{inner product} is:
	$$\left<X,Y\right>=E(XY)$$
	With $X$, $Y$ and $W$ are three vectors of this vector space and for any constant $c\in\mathbb{R}$, now we check if our \textbf{definition} satisfies $5$ axioms of inner product.
	\begin{equation*}
		\begin{split}
			\left<X,Y\right>=\left<Y,X\right>&\leftrightarrow E(XY)=E(YX)\\
			c\left<X,Y\right>=\left<cX,Y\right>&\leftrightarrow cE(XY)=E(cXY)\\
			\left<X,Y+W\right>=\left<X,Y\right>+\left<X,W\right>&\leftrightarrow E(X(Y+W))=E(XY)+E(XW)\\
			\left<X,X\right>\geq0&\leftrightarrow E(X^2)\geq0\\
			\left<X,X\right>=0\Leftrightarrow X=0&\leftrightarrow E(X^2)=0\Leftrightarrow X=0\\
		\end{split}
	\end{equation*}
	The original inequality is equivalent to:
	\begin{equation*}
		\begin{split}
			|\rho_{XY}|&\leq 1 \Leftrightarrow \sigma^2_{XY}\leq(\sigma_{X}\sigma_{Y})^2\\
			\Leftrightarrow E^2((X-\mu_{X})(Y-\mu_{Y}))&\leq E((X-\mu_{X})^2)E((Y-\mu_{Y})^2)\\
		\end{split}
	\end{equation*}
	Define $2$ new random variables $A$ and $B$ (or vectors):
	$$A=X-\mu_{X}$$ $$B=Y-\mu_{Y}$$
	So we have to prove: $$E^{2}(AB)\leq E(A^2)E(B^2)\Leftrightarrow |E(AB)|\leq \sqrt{E(A^2)E(B^2)}$$
	Rewrite in vector form, this is exactly Cauchy-Schwarz inequality:
	$$|\left<A,B\right>|\leq ||A||.||B||$$
	Now \textbf{Theorem 3.2.3} has been completely proven. The equality of the inequality occurs if and only if $B=kA$, where $k$ is any real constant.
	\begin{equation*}
		B=kA\Leftrightarrow Y-\mu_{Y}=k(X-\mu_{X})\Leftrightarrow Y=kX+(\mu_{Y}-k\mu_{X})
	\end{equation*}
	Since $k$ is an arbitrary value in $\mathbb{R}$, so we can conclude that if and only if $|\rho_{XY}|=1$, then $Y=aX+b\;(a,b\in\mathbb{R})$ or in other words, they have a linear relationship.
\end{proof}
\begin{corollary}
	If $\rho_{XY}=0$, then $X$ and $Y$ are \textbf{statistically independent}.
\end{corollary}
\begin{proof} $\rho_{XY}=0\Leftrightarrow \sigma_{XY}=E(XY)-\mu_{X}\mu_{Y}=0\Leftrightarrow E(XY)=\mu_{X}\mu_{Y}$. Or in equivalent:
	\begin{equation*}
		\begin{split}
			E(XY)&=\mu_{X}\mu_{Y}\\
			\Leftrightarrow \sum_{x}\sum_{y}xyf(x,y)&=\sum_{x}xg(x)\sum_{y}yh(y)\\
			\Leftrightarrow \sum_{x}\sum_{y}xyf(x,y)&=\sum_{x}\sum_{y}xyg(x)h(y)\\
		\end{split}
	\end{equation*}
	Finally, $f(x,y)=g(x)h(y)$, and we conclude $X$ and $Y$ are statistically independent.
\end{proof}
From the equality condition of the Cauchy-Schwarz inequality, we can deduce:
\begin{corollary}
	If $\rho_{XY}=1$, then: $$Y=aX+b\quad(a>0)$$
	And if $\rho_{XY}=-1$, then: $$Y=aX+b\quad(a<0)$$
\end{corollary}
\section{Means and Variances of Linear Combinations of Random Variables}
In this section, we will derive some extremely useful theorems and corollaries. You should note that all results obtained from this section will be reused multiple times throughout the rest of this book.
\begin{theorem}
	If $a,b$ are $2$ constants, then: $E(aX+b)=aE(X)+b$
\end{theorem}
\begin{proof} By definition of the \textbf{mean}:
	$$E(aX+b)=\sum_{x}(ax+b)f(x)=a\sum_{x}f(x)+b\sum_{x}f(x)=aE(X)+b$$
\end{proof}
\begin{corollary}
	Setting $a=0$, we see that $E(b)=b$
\end{corollary}
\begin{corollary}
	Setting $b=0$, we see that $E(aX)=0$
\end{corollary}
\begin{theorem}
	If $g(X)$ and $h(X)$ are two functions of random variable $X$, then:
	$$E(g(X)+h(X))=E(g(X))+E(h(X))$$
\end{theorem}
\begin{proof} Also by the definition of the \textbf{expected value}:
	$$E(g(X)+h(X))=\sum_{x}(g(x)+h(x))f(x)=\sum_{x}g(x)f(x)+\sum_{x}h(x)f(x)=E(g(X))+E(h(X))$$
\end{proof}
\begin{theorem}
	If $g(X,Y)$ and $h(X,Y)$ are two functions of random variable $(X,Y)$, then:
	$$E(g(X,Y)+h(X,Y))=E(g(X,Y))+E(h(X,Y))$$
\end{theorem}
\begin{proof} Applying directy the definition of the \textbf{expected value}, we have:
	\begin{equation*}
		\begin{split}
			E(g(X,Y)+h(X,Y))&=\sum_{x}\sum_{y}(g(x,y)+h(x,y))f(x,y)\\&=\sum_{x}\sum_{y}g(x,y)f(x,y)+\sum_{x}\sum_{y}h(x,y)f(x,y)\\&=E(g(X,Y))+E(h(X,Y))
		\end{split}
	\end{equation*}
\end{proof}
\begin{corollary}
	Setting $g(X,Y)=g(X)$ and $h(X,Y)=h(Y)$, then:
	$$E(g(X)+h(Y))=E(g(X))+E(h(Y))$$
\end{corollary}
\begin{corollary}
	Setting $g(X,Y)=X$ and $h(X,Y)=Y$, then:
	$$E(X+Y)=E(X)+E(Y)$$
\end{corollary}
\begin{theorem}
	If $X$ and $Y$ are statistically independent, then: $$E(XY)=\mu_{X}\mu_{Y}$$
\end{theorem}
\begin{proof} We already have $f(x,y)=g(x)h(y)$.
	\begin{equation*}
		E(XY)=\sum_{x}\sum_{y}xyf(x,y)=\sum_{x}\sum_{y}xyg(x)h(y)=\left(\sum_{x}xg(x)\right)\left(\sum_{y}yh(y)\right)=\mu_{X}\mu_{Y}
	\end{equation*}
\end{proof}
You should note that $X$ and $Y$ are independent random variables $\Leftrightarrow$ $E(XY)=\mu_{X}\mu_{Y}$
\begin{theorem}
	If $X$ and $Y$ are two \textbf{independent random variables} with the joint pdf $f(x,y)$ with three constants $a,b,c$, then: $$\sigma^2_{aX+bY+c}=a^2\sigma^2_{X}+b^2\sigma^2_{Y}$$
\end{theorem}
\begin{proof} Using definition, we have:
	\begin{equation*}
		\begin{split}
			\sigma^2_{aX+bY+c}&=E((aX+bY+c-\mu_{aX+bY+c})^2)\\
			&=E((aX+bY+c-a\mu_{X}-b\mu_{Y}-c)^2)\\
			&=E((a(X-\mu_{X})+b(Y-\mu_{Y}))^2)\\
			&=E(a^2(X-\mu_{X})^2)+2abE(X-\mu_{X})(Y-\mu_Y)+E(b^2(Y-\mu_{Y})^2)\\
			&=a^2\sigma^2_{X}+2ab\sigma_{XY}+b^2\sigma^2_{Y}\\
			&=a^2\sigma^2_{X}+b^2\sigma^2_{Y}\quad(\sigma_{XY}=0)
		\end{split}
	\end{equation*}
\end{proof}
\begin{corollary} Setting $b=0$, we see that: $\sigma^2_{aX+c}=a^2\sigma^2_{X}$
\end{corollary}
\begin{corollary} Setting both $b=c=0$, we see that: $\sigma^2_{aX}=a^2\sigma^2_{X}$
\end{corollary}
\begin{corollary} Setting $c=0$, we see that: $$\sigma^2_{aX+bY}=a^2\sigma^2_{X}+b^2\sigma^2_{Y}$$
	This corollary is only true when $X$ and $Y$ are \textbf{statistically independent}.
\end{corollary}
\section{Markov's and Chebyshev's Inequalities}
\subsection{Markov's Inequality}
\begin{theorem}
	If $X$ is a \textbf{non-negative} random variable with pdf $f(x)$, then:
	$$P(X\geq a)\leq\frac{E(X)}{a}$$
\end{theorem}
\begin{proof} From the intitial condition $X\geq0$:
	\begin{equation*}
		E(X)=\sum_{x}xf(x)\geq\sum_{x\geq a}xf(x)\geq\sum_{x\geq a}af(x)=aP(X\geq a)\Rightarrow P(X\geq a)\leq\frac{E(x)}{a}
	\end{equation*}
\end{proof}
\subsection{Chebyshev's Inequality}
\begin{theorem} If $X$ is any random variables with pdf $f(x)$, then:
	$$P(\mu_{X}-k\sigma_{X}<X<\mu_{X}+k\sigma_{X})\geq 1-\frac{1}{k^2}$$
\end{theorem}
\begin{proof} Applying Markov's inequality with $(X-\mu_{X})^2$ as non-negative random variable, now we obtain:
	\begin{equation*}
		\begin{split}
			&P((X-\mu_{X})^2\geq a^2)\leq\frac{E((X-\mu_{X})^2)}{a^2}=\frac{\sigma^2_{X}}{a^2}\\
			\Rightarrow& P((X-\mu_{X})^2<a^2)\geq 1-\frac{\sigma^2_{X}}{a^2}\\
		\end{split}
	\end{equation*}
	Using the positive scale factor $k$: $a=k\sigma_{X}$ from the \textbf{standard deviation}, now we obtain:
	$$P(\mu_{X}-k\sigma_{X}<X<\mu_{X}+k\sigma_{X})\geq 1-\frac{1}{k^2}$$
\end{proof}
Chebyshev's Inequality can also be written as:
$$P(|X-\mu_{X}|\geq k\sigma_{X})\leq \frac{1}{k^2}$$
This result is very important, especially in \textbf{case of continuous random variable}, the range of $X$ usually extends to infinity; it shows us that the first and last ends of any pdf are \textbf{bounded}. In general, the shape of a valid pdf graph is always \textbf{flattened} at both ends according to the inequality above.
\begin{figure}[h]
	\centering
	\includegraphics[width=12cm]{13.jpg}
	\caption{Example of a valid pdf graph}
	\label{Figure 13}
\end{figure}
\chapter{Some Discrete Probability Distributions}
Form this chapter to the end of this book, if $X$ is a random variable with pdf $f(x)$, we can denote it as:
$$X\sim f(x)$$
\section{Bernoulli, Binomial and Poisson Distributions}
\subsection{Bernoulli Distribution}
Because tossing a coin many times might be so boring, so in this chapter we will begin with more vivid examples. Let's plan to plant some mung bean plants (or maybe just imagine it)! Now you have to buy a packet of mung bean seeds from the agriculture store.
\\All seed packets clearly state the germination rate of the seeds. Assume that the germination rate is $p=0.8$. Now if we define the random variable $X$ as the germination state of \textbf{a single seed}, then the pdf of $X$ can be represented as follows, where $(X=1)$ describes the germinating state of the seed, and $(X=0)$ describes the opposite state:
\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		$x$      & $0$   & $1$   \\
		\hline
		$P(X=x)$ & $0.2$ & $0.8$ \\
		\hline
	\end{tabular}
\end{center}
Or in equation form: $$f(x)=P(X=x)=0.8^{x}0.2^{1-x}\quad(x=0,1)$$
The experiment to test the germination state of \textbf{a single mung bean seed} is a typical exmaple of \textbf{Bernoulli trial}. Formally, a \textbf{Bernoulli trial} is a random experiment with exactly \textbf{two possible outcomes}: "success" and "failure". The success rate is always denoted by the letter $p$, and failure rate is $q$. Because they complement each other, so: $q=1-p$.
\begin{definition} The \textbf{Bernoulli trial} is a random experiment with exactly \textbf{two} possible outcomes: "success" and "failure".
\end{definition}
\begin{theorem}
	The pdf of a \textbf{Bernoulli trial} is: $$\mathscr{I}(x;1,p)=p^{x}q^{1-x}\quad(x=0,1)$$
\end{theorem}
\begin{corollary} If $X\sim\mathscr{I}(x;1,p)$, then:
	\begin{equation*}
		\begin{split}
			\mu_{X}&=p\\
			\sigma^2_{X}&=pq\\
		\end{split}
	\end{equation*}
\end{corollary}
\begin{proof} For the mean:
	\begin{equation*}
		\mu_{X}=\sum_{x}xf(x)=p^{1}q^{0}=p
	\end{equation*}
	For the variance:
	$$\sigma^2_{X}=E(X^2)-\mu^2_{X}=\sum_{x}x^2 f(x)-p^2=p-p^2=pq$$
\end{proof}
\begin{figure}[h]
	\centering
	\includegraphics[width=12cm]{15.jpg}
	\caption{The pdf of Bernoulli distribution}
	\label{Figure 15}
\end{figure}
\newpage
\subsection{Binomial Distribution}
We know that the germination rate is $p=0.8$. Now you pick randomly $10$ mung bean seeds from the packet and put them on the wet tissue.
\\ Perhaps after waiting $3$ days, you will find out that only $6$ seeds have germinated, while the rest will start to smell. You might ask "But why did only $6$ seeds germinated? I thought it would be $8$?". Now let me explain.
\\ Firstly I change our previous definition of the random variable $X$. $X$ is now defined as the number of seeds that successfully germinated. The probability of $6$ seeds germinating is:
$$P(X=6)=\binom{10}{6}0.8^{6}0.2^{4}=0.088$$
And the probability of $8$ seeds germinating is:
$$P(X=8)=\binom{10}{8}0.8^{8}0.2^{2}=0.301$$
Although $0.301>0.088$, it does not mean the event "6 seeds germinated" will never happen (because $0.088>0$); but $1>0.301$, so the event "8 seeds germinated" will not always happen. This is the paradox of ProbStat, we evaluate everything through the question \textbf{"How likely will it occur?"}, and our certainty is never absolute.
\\ The pdf of our experiment is:
$$f(x)=P(X=x)=\binom{10}{x}0.8^{x}0.2^{10-x}\quad(x\in\{0,1,2\cdots,10\})$$
Or in table form:
\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		$x$      & $0$       & $1$         & $2$           & $3$           & $4$           \\
		\hline
		$P(X=x)$ & $10^{-7}$ & $4.10^{-6}$ & $7.3.10^{-5}$ & $7.8.10^{-4}$ & $5.5.10^{-3}$ \\
		\hline
	\end{tabular}
\end{center}
\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		$x$      & $5$     & $6$     & $7$     & $8$     & $9$     & $10$    \\
		\hline
		$P(X=x)$ & $0.026$ & $0.088$ & $0.201$ & $0.301$ & $0.268$ & $0.107$ \\
		\hline
	\end{tabular}
\end{center}
From the data table above, you can easily see that even if $P(X=8)$ reaches its maximum value, the probability of occurrence is only about $30\%$. Testing the germination ability of $10$ seeds experiment is a classic example of the \textbf{Bernoulli process}, which consists of \textbf{repeated Bernoulli trials}.
\begin{definition} The \textbf{Bernoulli process} is the process that consists \textbf{repeated Bernoulli trials}.
\end{definition}
\begin{theorem} A single Bernoulli trial can result is a sucess with probability $p$ and a failure with probability $q=1-p$. Then the pdf of \textbf{binomial random variable} $X$, the \textbf{number of successes} in $n$ independent trials is:
	$$\mathscr{B}(x;n,p)=\binom{n}{x}p^{x}q^{n-x}\quad(x=0,1,2,\cdots,n)$$
\end{theorem}
\begin{corollary}
	If $X\sim\mathscr{B}(x;n,p)$, then:
	\begin{equation*}
		\begin{split}
			\mu_{X}&=np\\
			\sigma^2_{X}&=npq\\
		\end{split}
	\end{equation*}
\end{corollary}
\begin{proof} For the mean:
	\begin{equation*}
		\begin{split}
			\mu_{X}&=\sum_{x}xf(x)=\sum_{x=0}^{n}x\binom{n}{x}p^{x}q^{n-x}=\sum_{x=1}^{n}\frac{n!}{(x-1)!(n-x)!}p^{x}q^{n-x}\\
			&=np\sum_{x=1}^{n}\frac{(n-1)!}{(x-1)!(n-x)!}p^{x-1}q^{n-x}=np\sum_{x=1}^{n}\binom{n-1}{x-1}p^{x-1}q^{n-x}\\
			&=np(p+q)^{n-1}=np
		\end{split}
	\end{equation*}
	Finding the variance directly is not easy, so we have to perform small trick here:
	\begin{equation*}
		\begin{split}
			E(X(X-1))&=\sum_{x}x(x-1)f(x)=\sum_{x=1}^{n}x(x-1)\binom{n}{x}p^{x}q^{n-x}\\&=p^{2}n(n-1)\sum_{x=2}^{n}\frac{(n-2)!}{(n-x)!(x-2)!}p^{x-2}q^{n-x}\\&=p^{2}n(n-1)\sum_{x=2}^{n}\binom{n-2}{x-2}p^{x-2}q^{n-x}\\&= p^{2}n(n-1)(p+q)^{n-2}=p^2n(n-1)
		\end{split}
	\end{equation*}
	Now we apply the result above to the definition of variance:
	$$\sigma^2_{X}=E(X^2)-\mu^2_{X}=E(X(X-1))+E(X)-\mu^2_{X}=p^2n(n-1)+np-n^2p^2=npq$$
\end{proof}
\begin{proof} There is another subtle way to prove \textbf{Corollary 4.1.2.1}, if $X\sim\mathscr{B}(x;n,p)$ then:
	$$X=I_{1}+I_{2}+\cdots+I_{n}$$
	where each $I_{i}\sim\mathscr{I}(x;1,p)$. Since they are \textbf{independent random variables}, we can apply some useful results:
	$$\mu_{X}=E(I_{1})+E(I_{2})+\cdots+E(I_{n})=np$$
	$$\sigma^2_{X}=\sigma^2_{I_{1}}+\sigma^2_{I_{2}}+\cdots+\sigma^2_{I_{n}}=npq$$
\end{proof}
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=14cm]{17.jpg}
	\caption{The pdf of Binomial distribution}
	\label{Figure 17}
\end{figure}
You should note that the position of the \textbf{mean value} is very close to the value of the random variable $X$ at which $P(X=x)$ reaches its \textbf{highest value}. Or in other words, $\mu_{X}$ is very close to the \textbf{peak} of the pdf graph.
\subsection{Poisson Distribution}
\subsubsection{Using Bell-shaped Curve to approx the Binomial Distribution}
Let's begin with a specific example: $X\sim\mathscr{B}(x;100,0.6)$. Now we can see that the mean value $\mu_{X}=np=60$ is relatively close to the center $X=50$.
\begin{figure}[h]
	\centering
	\includegraphics[width=14cm]{18.jpg}
	\caption{$\mathscr{B}(x;100,0.6)$ graph}
	\label{Figure 18}
\end{figure}
\\Because $np=60$ value is medium, neither too high nor low compared to the center value $X=50$; and plotting $n=100$ points is high enough for us to connect all of them to obtain a relatively smooth curve. This smooth curve \textbf{can be approximated} by the \textbf{bell-shaped curve}, so we will perform operations on it instead of the original curve.
\\ In this subsection, we will focus on mathematical concepts rather than delving into calculation methods (you can try it yourself if you want). We will return to specific calculation methods in the next chapter.
\begin{theorem} If $X\sim\mathscr{B}(x;n,p)$ with $np$ value is medium compared to the center value, then:
	$$\text{Original Curve}\approx\textbf{Bell-shaped Curve: }\frac{1}{\sqrt{2\pi}\sigma}\text{exp}\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$
	where: $\mu=np$ and $\sigma^2=npq$
\end{theorem}
Now let's compare $2$ methods: caculating directly and using approximation.
\begin{equation*}
	\begin{split}
		P(X\leq 70)&=1-P(X\geq 71)=1-\sum_{x=71}^{100}\binom{100}{x}0.6^{x}0.4^{100-x}=0.985\\
		P(X\leq 70)&=P\left(Z<\frac{70-100.0.6+0.5}{\sqrt{100.0.6.0.4}}\right)=P(Z<2.143)= 0.983
	\end{split}
\end{equation*}
Or we can also try:
\begin{equation*}
	\begin{split}
		P(50\leq X\leq 65)&=P(X\leq 65)-P(X\leq 49)\\&=P\left(Z<\frac{65-100.0.6+0.5}{\sqrt{100.0.6.0.4}}\right)-P\left(Z<\frac{49-100.0.6+0.5}{\sqrt{100.0.6.0.4}}\right)\\&=P(Z<1.122)-P(Z<-2.143)\\&=P(Z<1.122)-1+P(Z<2.143)\\&=0.868-1+0.983=0.851\\
		P(50\leq X\leq 65)&=\sum_{x=50}^{65}\binom{100}{x}0.6^{x}0.4^{100-x}=0.852
	\end{split}
\end{equation*}
The approximation method works very well and yields resutls very close to calculating directly.
But how about the $X\sim\mathscr{B}(x;100,0.05)$ case? Can we still use the \textbf{bell-shaped curve}? Let's check it!
\begin{figure}[h]
	\centering
	\includegraphics[width=14cm]{19.jpg}
	\caption{$\mathscr{B}(x;100,0.05)$ graph}
	\label{Figure 19}
\end{figure}
\\For example:
\begin{equation*}
	\begin{split}
		P(X\leq 7)&=\sum_{x=0}^{7}\binom{100}{x}0.05^{x}0.95^{100-x}=0.872\\
		P(X\leq 7)&=P\left(Z<\frac{7-100.0.05+0.5}{\sqrt{100.0.05.0.95}}\right)=0.9292\\
	\end{split}
\end{equation*}
The approximate result differs from the actual result nearly $6\%$. Therefore, for cases where $\mu_{X}=np$ is located very far from the center, the bell-shaped curve approximation method is no longer suitable.
\newpage
\subsubsection{Poisson Distribution}
How do we handle extreme value cases of $np$? We must find better way to approximate original curve. Because $n>>p$, so we can perform some equivalent transformations with assumptions $n\to+\infty$ and $p\to 0$. We begin with $X\sim\mathscr{B}(x;n,p)$:
\begin{equation*}
	\begin{split}
		P(X=x)&=\mathscr{B}(x;n,p)=\binom{n}{x}p^{x}q^{n-x}=\frac{n!}{(n-x)!x!}p^{x}q^{n-x}\\
		&=\frac{n(n-1)(n-2)\cdots(n-x+1)}{n^{x}}\frac{(np)^{x}}{x!}(1-p)^{n-x}\\
	\end{split}
\end{equation*}
Because $n\to+\infty$ and $p\to 0$:
\begin{equation*}
	\begin{split}
		\lim_{n\to +\infty}\frac{n(n-1)(n-2)\cdots(n-x+1)}{n^{x}}=1\\
	\end{split}
\end{equation*}
Our approximation curve will be more accurate if we only care about \textbf{the first end} values, it means $n>>x$:
\begin{equation*}
	\begin{split}
		\lim_{n\to +\infty}(1-p)^{n-x}=(1-p)^{n}
	\end{split}
\end{equation*}
By the definition of the constant $e$:
\begin{equation*}
	\begin{split}
		\lim_{p\to 0}(1-p)^{\frac{-1}{p}}= e
		\Rightarrow \lim_{p\to 0}(1-p)=e^{-p} \Rightarrow (1-p)\approx e^{-p}
	\end{split}
\end{equation*}
Now we obtain: $$(1-p)^{n}\approx e^{-np}$$
Finally we have:
$$P(X=x)=\frac{(np)^{x}}{x!}e^{-np}$$
Or in shortened form with $\mu=np$:
$$P(X=x)=\frac{\mu^{x}e^{-\mu}}{x!}$$
Test the new function with the case $X\sim\mathscr{B}(x;100,0.05)$:
$$P(X\leq 7)=\sum_{x=0}^{7}\frac{(100.0.05)^{x}e^{-100.0.05}}{x!}=0.866$$
Obviously, the approximated result is very close to the actual result $0.872$.
\\ How about the case $X\sim\mathscr{B}(x;100,0.9)$? Since $p$ is clearly close to $1$, not zero, so we must \textbf{inverse the role} of $p$ and $q$. We define the new random variable $Y$ as the \textbf{number of failures} to satisfy the condition of valid approximation.
\begin{equation*}
	\begin{split}
		P(X\geq 90)&=\sum_{90}^{100}\binom{100}{x}0.9^{x}0.1^{100-x}=0.583\\
		P(X\geq 90)&=P(Y\leq 10)=\sum_{y=0}^{10}\frac{(100.0.1)^{x}e^{-100.0.1}}{y!}=0.583\\
	\end{split}
\end{equation*}
\\ So relatively speaking, we conclude the following, if $\mu=np$ position is very \textbf{far} from the \textbf{left side} of the center, then the Binomial distribution can be approximated by the formula:
$$P(X=x)=\mathscr{B}(x;n,p)\approx \frac{\mu^{x}e^{-\mu}}{x!}$$
Because of two main constraints $n>>p$ and $p$ is very close to $0$, the probability of a success event is relatively \textbf{low}. You can think about $p$ as the \textbf{average number} of outcomes per unit time, distance or volume; and $n$ as a given \textbf{time interval} or \textbf{specified region}. In practice, to avoid being confused with Binomial distribution and emphasize the fact that $p\approx 0$, we usually change our notations to:
$$p\leftrightarrow \lambda$$ $$n\leftrightarrow t$$
\begin{definition} The \textbf{Poisson process} is the \textbf{Bernoulli process} where $n>>p$ and $p\approx 0$.
\end{definition}
\begin{definition} A \textbf{Poisson event} is an event with a \textbf{low} probability of occurring.
\end{definition}
\begin{theorem} The pdf of the \textbf{Poisson random variable} $X$, representing the number of outcomes occurring in a \textbf{given time interval} or \textbf{specified region} denoted by $t$, is:
	$$\mathscr{P}(x;\mu)=\frac{\mu^{x}e^{-\mu}}{x!}\quad(x=0,1,2,\cdots)$$
	where $\mu=\lambda t$, $\lambda$ is the \textbf{average} number of outcomes per unit time, distance area or volume.
\end{theorem}
\begin{theorem} If $X\sim\mathscr{B}(x;n,p)$, when $n\to+\infty$, $p\to 0$ and $np \to\mu$ remains constant: $$\mathscr{B}(x;n,p)\to\mathscr{P}(x;\mu)$$
\end{theorem}
\begin{corollary} If $X\sim\mathscr{P}(x;\mu)$, then:
	$$\mu_{X}=\mu$$
	$$\sigma^2_{X}=\mu$$
\end{corollary}
\begin{proof} For the mean:
	\begin{equation*}
		\begin{split}
			\mu_{X}&=\sum_{x}xf(x)=\sum_{x=0}^{n}x\frac{\mu^{x}e^{-\mu}}{x!}=\sum_{x=1}^{n}\frac{\mu^{x}e^{-\mu}}{(x-1)!}=\mu e^{-\mu}\sum_{x=1}^{n}\frac{\mu^{x-1}}{(x-1)!}=\mu e^{-\mu}e^{\mu}=\mu
		\end{split}
	\end{equation*}
	For the variance, perform the same trick:
	\begin{equation*}
		\begin{split}
			E(X(X-1))&=\sum_{x}x(x-1)f(x)=\sum_{x=1}^{n}x(x-1)\frac{\mu^{x}e^{-\mu}}{x!}\\
			&=\mu^{2}e^{-\mu}\sum_{x=2}^{n}\frac{\mu^{x-2}}{(x-2)!}=\mu^{2}e^{-\mu}e^{\mu}\\
			&=\mu^2
		\end{split}
	\end{equation*}
	Apply directly our previous result:
	$$\sigma^2_{X}=E(X^2)-\mu^2_{X}=E(X(X-1))+E(X)-\mu^2_{X}=\mu^2+\mu-\mu^2=\mu$$
\end{proof}
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{20.jpg}
	\caption{The pdf of Poisson distribution}
	\label{Figure 20}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{21.jpg}
	\caption{A $4\times 4$ grid}
	\label{Figure 21}
\end{figure}
Back to our mung bean seeds, now you will see how Poisson distribution can be applied in many situations in real life. If you take some seeds from the packet and throw them on a $4\times 4$ grid and you observe that there are $10$ seeds in the $2\times 2$ square closet to you, what are the probabilities of:
\begin{enumerate}
	\item There are a total of $40$ seeds on that $4\times 4$ grid.
	\item Suppose you knew the exact number of seeds that you had thrown is $40$. Evaluate the chance of observing $10$ seeds in the $2\times 2$ square.
\end{enumerate}
The random variable $X$ is defined as the number of seeds on a $4\times 4$ grid. Using the formula above, we have: $$\mu=\lambda t=\frac{10}{4} .16=40$$
$$\Rightarrow P(X=40)=\frac{e^{-\mu}\mu^{x}}{x!}=\frac{e^{-40}40^{40}}{40!}=0.0629$$
The random variable $Y$ is defined as the number of seeds in the $2\times 2$ square. Similarly, we have: $$\mu=\lambda t=\frac{40}{16}.4=10$$
$$\Rightarrow P(Y=10)=\frac{e^{-10}10^{10}}{10!}=0.125$$
So the chance of observing $10$ seeds in the $2\times 2$ square if you throw $40$ seeds on a $4\times 4$ grid is pretty low, just $12.5\%$.
\newpage
\section{Negative Binomial and Geometric Distributions}
\subsection{Negative Binomial Distribution}
What is the probability of tossing a coin $x$ times \textbf{until} getting $k$ heads, if we know that the probability of heads appearing is $p$ and $X$ is a random variable, defined as the number of tosses? This question is very easy so I will write down the answer:
$$P(X=x)=\binom{x-1}{k-1}p^{k}q^{x-k}$$
The function above is a \textbf{Negative Binomial Distribution} formula. Formally, we state that:
\begin{theorem}
	If repeated \textbf{Bernoulli trials} can result in a success with probability $p$ and a failure with probability $q=1-p$, then the pdf of random variable $X$, defined as the \textbf{number of the trial} on which the \textbf{kth success occurs} is:
	$$\mathscr{B}^*(x;k,p)=\binom{x-1}{k-1}p^{k}q^{x-k}\quad(x=k,k+1,k+2,\cdots)$$
\end{theorem}
\subsection{Geometric Distribution}
If we only care about the probability of tossing a coin $x$ times until getting \textbf{first} heads, it means $k=1$ now and we can rewrite our formula as:
$$P(X=x)=pq^{x-1}$$
The $k=1$ case of \textbf{Negative Binomial Distribution} can also be called as \textbf{Geometric Distribution}.
\begin{theorem} If repeated \textbf{Bernoulli trials} can result in a success with probability $p$ and a failure with probability $q=1-p$, then the pdf of random variable $X$, defined as the \textbf{number of trials} on which the \textbf{first success occurs} is:
	$$\mathscr{G}^{*}(x;p)=pq^{x-1}\quad(x=1,2,3,\cdots)$$
\end{theorem}
\begin{corollary}
	If $X\sim\mathscr{G}^{*}(x;p)$, then:
	$$\mu_{X}=\frac{1}{p}$$
	$$\sigma^2_{X}=\frac{q}{p^2}$$
\end{corollary}
\begin{proof} For the mean:
	\begin{equation*}
		\begin{split}
			\mu_{X}=\sum_{x}xf(x)=p\sum_{x=1}^{+\infty}xq^{x-1}=p\sum_{x=1}^{+\infty}x(1-p)^{x-1}
		\end{split}
	\end{equation*}
	Now we use the Geometric series and take the derivative once with respect to $p$:
	\begin{equation*}
		\begin{split}
			\sum_{x=0}^{+\infty}(1-p)^{x}&=\frac{1}{1-(1-p)}=\frac{1}{p}\\
			\Leftrightarrow
			\left(\sum_{x=0}^{+\infty}(1-p)^{x}\right)^{'}&=\left(\frac{1}{p}\right)^{'}\\
			\Leftrightarrow -\sum_{x=0}^{+\infty}x(1-p)^{x-1}&=\frac{-1}{p^2}\Rightarrow \sum_{x=1}^{+\infty}x(1-p)^{x-1}=\frac{1}{p^2}\\
		\end{split}
	\end{equation*}
	After substituting our result to the previous equation, we obtain:
	\begin{equation*}
		\mu_{X}=p\sum_{x=1}^{+\infty}x(1-p)^{x-1}=p.\frac{1}{p^2}=\frac{1}{p}
	\end{equation*}
	Take the second derivative with respect to $p$ from the previous equation:
	\begin{equation*}
		\left(\sum_{x=1}^{+\infty}x(1-p)^{x-1}\right)^{'}=\left(\frac{1}{p^2}\right)^{'}\Leftrightarrow \sum_{x=2}^{+\infty}x(x-1)(1-p)^{x-2}=\frac{2}{p^3}
	\end{equation*}
	We want to determine:
	$$E(X(X-1))=\sum_{x}x(x-1)f(x)=p\sum_{x=1}^{+\infty}x(x-1)(1-p)^{x-1}=\frac{2p(1-p)}{p^3}=\frac{2q}{p^2}$$
	For the variance:
	$$\sigma^2_{X}=E(X^2)-\mu^2_{X}=E(X(X-1))+E(X)-\frac{1}{p^2}=\frac{2q}{p^2}+\frac{1}{p}-\frac{1}{p^2}=\frac{1-p}{p^2}=\frac{q}{p^2}$$
\end{proof}
\section{Hypergeometric Distribution}
Suppose you now have $N$ mung bean seeds in the packet, of which you know $N$ of them are spoiled. If you randomly select a handful of $k$ seeds, what is the probability of getting $x$ spoiled seeds?
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{22.jpg}
	\caption{Illustration of Hypergeometric distribution}
	\label{Figure 22}
\end{figure}
\\ Intuitively, we can see that: $$P(X=x)=\frac{\binom{k}{x}\binom{N-k}{n-x}}{\binom{N}{n}}$$
\begin{theorem} The pdf of the \textbf{hypergeometric} random variable $X$, the number of successes in a random sample of size $n$ selected from $N$ items of which $k$ are labeled \textbf{success} and $N-k$ labeled \textbf{failure} is:
	$$\mathscr{H}(x;N,k,n)=\frac{\binom{k}{x}\binom{N-k}{n-x}}{\binom{N}{n}}\quad(x=0,1,2,\cdots,k)$$
\end{theorem}
\newpage
\chapter{Some Continuous Probability Distributions}
Unlike discrete quantities, which are only formed through human counting, couninuous quantities appear naturally. You can easily see that most of physical quantities in reality are continuous, such as mass, length, time,$\cdots$ Therefore, this chapter plays crucial foundational role for the rest of this book.
\section{Uniform Distribution}
As we discussed above, the pdf of choosing randomly a number within the range $(0,1)$ experiment is:
\begin{equation*}
	f(x)=\begin{cases}
		1\quad(0<x<1)            \\
		0\quad\text{(elsewhere)} \\
	\end{cases}
\end{equation*}
The probability of our selected number within $(0.3,0.7)$ interval is:
$$P(0.3<x<0.7)=\int_{-\infty}^{+\infty}f(x)dx=\int_{0.3}^{0.7}dx=0.4$$
Suppose normal seeds will typically germinate within $3-5$ days of being placed on a wet towel. They will never germinate before $3$ days or after $5$ days. Now we form the pdf of this experiment:
\begin{equation*}
	f(x)=\begin{cases}
		\frac{1}{2}\quad(3<x<5)  \\
		0\quad\text{(elsewhere)} \\
	\end{cases}
\end{equation*}
We can obtain some useful result such as:
\begin{equation*}
	P(X<x)=F(x)=\int_{-\infty}^{x}f(t)dt=\begin{cases}
		0\quad(x\leq 3)         \\
		\frac{x}{2}\quad(3<x<5) \\
		1\quad(x\geq 5)
	\end{cases}
\end{equation*}
But why do $f(x)=\frac{1}{2}$ with $x\in (3,5)$? Well because every pdf must satisfy these criteria:
\begin{equation*}
	\begin{cases}
		f(x)\geq 0\text{ (for all $x\in\mathbb{R}$)} \\
		\int_{-\infty}^{+\infty}f(x)dx=1             \\
		P(a<x<b)=\int_{a}^{b}f(x)dx                  \\
	\end{cases}
\end{equation*}
Verifying yourself that the pdf of germinating seeds experiment satisfies all of them.
\begin{definition}
	The pdf of the continuous \textbf{uniform random variable} $X$ on the interval $(A,B)$ is:
	$$\mathscr{U}(x;a,b)=\begin{cases} \frac{1}{b-a}\quad(a<x<b)\\ 0\quad\text{(elsewhere)}\end{cases}$$
\end{definition}
\begin{corollary} If $X\sim\mathscr{U}(x;a,b)$ then:
	\begin{equation*}
		\begin{split}
			\mu_{X}&=\frac{b+a}{2}\\
			\sigma^2_{X}&=\frac{(b-a)^2}{12}\\
		\end{split}
	\end{equation*}
\end{corollary}
\begin{proof} For the mean:
	\begin{equation*}
		\mu_{X}=\int_{-\infty}^{+\infty}xf(x)dx=\frac{1}{b-a}\int_{a}^{b}xdx=\frac{1}{b-a}.\frac{b^2-a^2}{2}=\frac{b+a}{2}
	\end{equation*}
	For the variance:
	\begin{equation*}
		\begin{split}
			\sigma^2_{X}&=E(X^2)-\mu^2_{X}=\int_{-\infty}^{+\infty}x^2f(x)dx-\frac{(b+a)^2}{4}=\frac{1}{b-a}\int_{a}^{b}x^2dx-\frac{(b+a)^2}{4}\\
			&=\frac{1}{b-a}\frac{b^3-a^3}{3}-\frac{(b+a)^2}{4}=\frac{(b-a)^2}{12}
		\end{split}
	\end{equation*}
\end{proof}
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{23.jpg}
	\caption{The pdf of Uniform distribution}
	\label{Figure 23}
\end{figure}
\section{Normal Distribution}
\subsection{The Idea behind the Normal Distribution}
This section is one of the most important part of this book. Now we shall 'watch' very closely to the question "Is everything normal because they are?" and partially answer my question from  the \textbf{Preface}. Let's temporarily forget everything we were taught about those concepts and now rebuild them from scratch. \\ \\ \\
\indent You are watching the rain outside. You observe that the raindrops are all falling \textbf{randomly} onto a circular courtyard. You might wonder where a raindrop is most likely to fall? Perhaps at the \textbf{center} of the circle? Or at the \textbf{edge}? Or neither? Intuitively, you might think the raindrop would concentrate at the center, but are you sure about that when we can not blindly trust intuition? Remember that ProbStat is a paradoxical subject. It tricks our brains with the fact \textbf{that not everything can be absolutely certain, even this statement}; while human brain is always naturally seeking for the certainty. The only thing we can trust is the use of mathematical language to describe the uncertainty.
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{24.jpg}
	\caption{A circular courtyrad with raindrops falling inside}
	\label{Figure 24}
\end{figure}
\\  You define the pdf of an event "A single raindrop falls at the position with coordinate $(x,y)$" as $f(x,y)$. Since raindrops fall completely randomly and you surely know: $$f(x|y)=f(x)$$ $$f(y|x)=f(y)$$
Since the events "A raindrop falls on a point with coordinate $x$" and "A raindrop falls on a point with coordinate $y$" are \textbf{independent}, we can conclude:
$$f(x,y)=f(x)f(y)$$
Applying Pythagorean theorem shows that: $$f(x,y)=f(\sqrt{x^2+y^2})$$
Substituting to the previous formula yields: $$f(\sqrt{x^2+y^2})=f(x)f(y)$$
Now we have to find the solution of this functional equation. Firstly, we define a new auxiliary function $g(x)$ as follows: $$f(x)=g(x^2)$$
Our functional equation can be rewritten as:
$$g(x^2+y^2)=g(x^2)g(y^2)$$
Setting $y^2=x^2$ and $t=x^2$ yields: $$g(2t)=g^2(t)$$
We can observe the pattern:
$$g(3t)=g(2t+t)=g(2t)g(t)=g^{3}(t)$$
Now we want to prove this result by using induction: $$g(nt)=g^{n}(t)\quad(n\in\mathbb{N}^*)$$
With special cases $n=1,2,3$, our equation is correct, now we assume it can be still correct until $n=k$ with $k>3$:
$$g(kt)=g^{k}(t)\quad(k>3)$$
For the case $n=k+1$, our equation is still correct:
$$g((k+1)t)=g(kt)g(t)=g^{k+1}(t)$$
By induction we can conclude: $$g(nt)=g^{n}(t)\quad(n\in\mathbb{N}^*)$$
Setting $t=1$ and $g(1)$ as a constant $C$ yields:
$$g(n)=g^{n}(1)=C^{n}\quad(n\in\mathbb{N}^{*})$$
Constant $C$ can also be written in an exponential form: $$g(n)=C^{n}=(e^{c})^n\quad(n\in\mathbb{N}^*) $$
Now we guess the form of $g(x)$ might be: $$g(x)=e^{cx}\quad(\forall x\in\mathbb{R})$$
Or equivalent to: $$f(x)=g(x^2)=e^{cx^2}\quad(\forall x\in\mathbb{R})$$
Test our assumption with an arbitrary constant $c$:
$$f(\sqrt{x^2+y^2})=e^{c(x^2+y^2)}=e^{cx^2}e^{cy^2}=f(x)f(y)$$
So the solution of our functional equation is: $$f(x)=e^{cx^2}\quad(\forall x,c\in\mathbb{R})$$
Due to \textbf{Chebyshev's Inequality}, both ends of a valid pdf must be \textbf{flattened}, so we choose our solution is: $$f(x)=e^{-cx^2}\quad(c>0)$$
The final step is choosing an appropriate constant $c$ to satisfy the criteria of pdf:
$$\int_{-\infty}^{+\infty}f(x)dx=1$$
To avoid the constant $c$ ($c$ is just a scale factor), we consider the special case where $c=1$ and calculate its integral value:
$$I=\int_{-\infty}^{+\infty}e^{-x^2}dx$$
Since $e^{-x^2}$ is not an elementary function, so its antiderivative can not be found. Again, we must perform a brilliant trick here:
\begin{equation*}
	\begin{split}
		I^{2}&=\int_{-\infty}^{+\infty}e^{-x^2}dx\int_{-\infty}^{+\infty}e^{-y^2}dy=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{-(x^2+y^2)}dxdy=\int_{0}^{2\pi}\int_{0}^{+\infty}e^{-r^2}rdrd\theta=\pi
	\end{split}
\end{equation*}
I do not go deeply into the changing variables (from Cartesian to Polar coordinates) and using Jacobian transformation steps since they are very easy; but you can see here, the idea of squaring $I$ is so amazing! Now we obtain: $$I=\int_{-\infty}^{+\infty}e^{-x^2}dx=\sqrt{\pi}$$
Back to our pdf, now we can change the variable and perform integration:
$$\int_{-\infty}^{+\infty}e^{-cx^2}dx=\frac{1}{\sqrt{c}}\int_{-\infty}^{+\infty}e^{-(\sqrt{c}x)^2}d(\sqrt{c}x)=\sqrt{\frac{\pi}{c}}$$
After choosing $c=\pi$, finally our pdf is: $$f(x)=e^{-\pi x^2}$$
Sketching the pdf, and as you can see here, the chance of raindrops falling around the \textbf{center} is \textbf{higher} than the \textbf{edge} or anywhere esle.
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{25.jpg}
	\caption{The probability of raindrops falling around the center is higher than anywhere else}
	\label{Figure 25}
\end{figure}
\newpage
\subsection{Standard Normal Distribution}
For convenience, we usually choose $c=\frac{1}{2}$ to be standard scale factor and define:
\begin{definition} A \textbf{standard normal distribution} function is defined as:
	$$f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}=\frac{1}{\sqrt{2\pi}}\text{exp}\left(-\frac{x^2}{2}\right)$$
\end{definition}
In the case of \textbf{standard normal distribution}, we often denote $Z$ as our random variable. This is a common convention, and you will understand why later.
\begin{definition} If $Z\sim \mathscr{N}(z;0,1)$, then:
	$$Z\sim f(z)=\frac{1}{\sqrt{2\pi}}\text{exp}\left(-\frac{z^2}{2}\right)$$
\end{definition}
\begin{corollary}
	If $Z\sim\mathscr{N}(z;0,1)$, then: $$\int_{-\infty}^{+\infty}f(z)dz=1$$
\end{corollary}
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{26.jpg}
	\caption{The pdf of Standard Normal distribution}
	\label{Figure 26}
\end{figure}
We are very interested in determining cdf $F(z)$ is defined as:
$$F(z)=P(Z<z)=\int_{-\infty}^{z}f(t)dt$$
But because $f(z)$ is \textbf{not an elementary function}, so the antiderivative $F(z)$ can not be found exactly. An alternative way is using Taylor-Maclaurin expansion to obtain the \textbf{approximate} version of $F(z)$:
\begin{equation*}
	\begin{split}
		F(z)&=P(Z<z)=\int_{-\infty}^{z}f(t)dt=\int_{-\infty}^{0}f(t)dt+\int_{0}^{z}f(t)dt=\frac{1}{2}+\frac{1}{\sqrt{2\pi}}\int_{0}^{z}\text{exp}\left(-\frac{t^2}{2}\right)dt\\
		&=\frac{1}{2}+\frac{1}{\sqrt{2\pi}}\int_{0}^{z}\sum_{k=0}^{+\infty}\frac{(-1)^{k}t^{2k}}{2^k k!}dt=\frac{1}{2}+\frac{1}{\sqrt{2\pi}}\sum_{k=0}^{+\infty}\frac{(-1)^{k}z^{2k+1}}{(2k+1)2^{k}k!}
	\end{split}
\end{equation*}
In practice, we usually do not use the approximation of $F(z)$; instead, we always look for our desired value in the $Z$ score table. In this book, $Z$ score table is the \textbf{Appendix A}.\\ Let's try comparing some values of $F(z)$ when using the table and using the approximation function:
$$F(Z<0.76)\approx\frac{1}{2}+\frac{1}{\sqrt{2\pi}}\sum_{k=0}^{10}\frac{(-1)^{k}(0.76)^{2k+1}}{(2k+1)2^k k!}=0.7763$$
$$F(Z<1.65)\approx\frac{1}{2}+\frac{1}{\sqrt{2\pi}}\sum_{k=0}^{10}\frac{(-1)^{k}(1.65)^{2k+1}}{(2k+1)2^k k!}=0.9505$$
Our results are perfectly matched with results in the $Z$ score table. There are many ways to approximate $F(z)$, and using Taylor-Maclaurin series is the simplest one; although it is accurate and acceptable, but the complexity is \textbf{not optimized}. The topic of optimizing approximation of $F(z)$ is beyond the scope of this book, and as I said in practice we always look for our desired value in the $Z$ score table; but I hope now we have a better understanding of the mystery behind how the $Z$ score table is formed, rather than just simply looking up at it.
\subsection{Normal Distribution}
\begin{figure}[h]
	\centering
	\includegraphics[width=14cm]{27.jpg}
	\caption{Visualizing how the general normal distribution formula is formed}
	\label{Figure 27}
\end{figure}
As you can see the general normal distribution function can be formed in two steps: shifting the standard normal distribution curve and scaling it. Now the roles of the \textbf{mean} $\mu_{X}$ and the \textbf{standard deviation} $\sigma_{X}$ are very clear. The \textbf{mean} acts as the \textbf{axis of symmetry}, while the \textbf{standard deviation} acts as the scale factor of the general normal distribution graph. Formally, we define:
\begin{definition} The pdf of the \textbf{normal random variable} X, with \textbf{mean} $\mu_{X}$ and \textbf{variance} $\sigma^2_{X}$ is: $$\mathscr{N}(x;\mu_{X},\sigma^2_{X})=\frac{1}{\sqrt{2\pi}\sigma_{X}}\text{exp}\left(-\frac{(x-\mu_{X})^2}{2\sigma^2_{X}}\right)$$
\end{definition}
\begin{theorem} If $X\sim\mathscr{N}(x;\mu_{X},\sigma^2_{X})$ and a new random variable $Z$ (or $Z$ score) is defined as: $$Z=\frac{X-\mu_{X}}{\sigma_{X}}$$
	then: $$Z\sim\mathscr{N}(z;0,1)$$
\end{theorem}
This is an extremely useful result. We will prove it rigorously later in the next chapter, but you can see the relationship between $X$ and $Z$ through graph transformations is very clear and intuitive. These random variables are \textbf{linearly mapped}. Now instead of performing directly operations on $X$, now we do the smarter choice: transforming from $X$ to $Z$ score first, then processing $Z$ later; since we want to avoid $2$ parameters $\mu_{X}$ and $\sigma_{X}$ in our calculation and we already knew the $Z$ score table.
\\ For example, if $X\sim\mathscr{N}(x;3,0.5^2)$, then:
\begin{equation*}
	\begin{split}
		P(3.5<X<4)&=P\left(\frac{3.5-3}{0.5}<Z<\frac{4-3}{0.5}\right)\\&=P\left(1<Z<2\right)\\&=P(Z<2)-P(Z<1)\\&=0.9773-0.8413=0.135
	\end{split}
\end{equation*}
Finally, this section will conclude with one of the most powerful theorems, which I introduced in the previous chapter.
\begin{theorem} Let $X\sim\mathscr{B}(x;n,p)$, if $np$ is medium compared to the center value, then:
	$$P(X\leq x)\approx P\left(Z<\frac{x-np+\mathbf{0.5}}{\sqrt{npq}}\right)$$
	where $Z$ is a \textbf{standard normal} random variable.
\end{theorem}
What is the meaning of $0.5$? Why do we have to pad $0.5$ unit? I think the reason is padding $0.5$ to increase the \textbf{resolution} from \textbf{discrete} to \textbf{continuous} version of our approximate line.
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{28.jpg}
	\caption{Normal approximation to the binomial}
	\label{Figure 28}
\end{figure}
\\Now you should review \textbf{Chapter 4: Some Discrete Probability Distributions} and try applying this theorem.
\section{Exponential, Gamma and Chi-Squared Distributions}
\subsection{Exponential Distribution}
Back to our weather forecasting problem from \textbf{Chapter 0: Introduction to Probability and Statistics}; suppose in December, there are average $2$ rainy days annualy. Applying directly Poisson distribution, you can calculate the probabilities of only $1$, or $2$, $3$, or more rainy days in December.
$$P(X=1)=\frac{e^{-2}2^{1}}{1!}=0.2706$$
$$P(X=2)=\frac{e^{-2}2^{2}}{2!}=0.2706$$
$$P(X=3)=\frac{e^{-2}2^{3}}{3!}=0.1804$$
But our concern is not just the number of rainy days in December, we actually want to know when it will rain (and also plan for the picnic!). For example, if today is the 3rd and you have not seen a drop of rain since the beginning of the month, what is the probability of it raining within the next $4$ days? Or what is the probability that the first $10$ days of this month will have no rain? Or there will be at least one rainy day? A lot of information can be gathered just from the data "On average, there are $2$ rainy days in December.". \\ \\ \\
\indent Firstly, let's find the answer for the easiest question: "What is the probability that the first $10$ days of this month will have no rain?". Again, you can just applying directly Poisson distribution without hesitation. The random variable $Y$ is defined as the number of rainy days within the first $10$ days:
$$\mu=\lambda t=\frac{2}{31}.10=\frac{20}{31}$$
$$\Rightarrow P(Y=0)=\frac{e^{-\mu}\mu^0}{0!}=e^{-\mu}=0.5245$$
Looking from a different perspective, you define the random variable $X$ as \textbf{the waiting time} for the \textbf{first} rain:
$$P(Y=0)=P(X>10)=e^{-\mu}=0.5245$$
Now the probability of at least one rainy day in the first $10$ days is:
$$P(X<10)=1-P(X>10)=1-0.5245=0.4755$$
Generally, if $x$ denote the number of days:
$$P(Y=0)=P(X>x)=e^{-\mu}=e^{-\lambda x}$$
$$\Rightarrow P(X<x)=1-P(X>x)=1-e^{-\lambda x}$$
From our previous results, now we can derive the pdf of random variable $X$, is defined as the waiting time for the \textbf{first} rain:
$$F(x)=P(X<x)=1-e^{\lambda x}\Rightarrow f(x)=\frac{dF(x)}{dx}=\lambda e^{-\lambda x}$$
Rainy day is an example of a \textbf{Poisson event} because its probability of occuring is \textbf{very low}. Since $f(x)$ is an exponential function, so $X$ can also be called an \textbf{exponential random variable}.
\begin{definition}
	The random variable $X$ representing the \textbf{distance} (in most cases, it is time) between \textbf{Poisson events} has pdf:
	\begin{equation*}
		\mathscr{E}(x;\lambda x)=
		\begin{cases}
			\lambda e^{-\lambda x}\quad(x\geq 0) \\
			0\quad(\text{elsewhere})             \\
		\end{cases}
	\end{equation*}
\end{definition}
\begin{corollary} If $X\sim\mathscr{E}(x;\lambda x)$, then:
	$$\mu_{X}=\frac{1}{\lambda}$$
	$$\sigma^2_{X}=\frac{1}{\lambda^2}$$
\end{corollary}
\begin{proof} For the mean:
	\begin{equation*}
		\begin{split}
			\mu_{X}&=\int_{-\infty}^{+\infty}xf(x)dx=\frac{1}{\lambda}\int_{0}^{+\infty}(\lambda x)e^{-\lambda x}d(\lambda x)=\frac{1}{\lambda}\int_{0}^{+\infty}te^{-t}dt=\frac{-1}{\lambda}\int_{0}^{+\infty}td(e^{-t})\\
			&=\frac{-1}{\lambda}\left(\left. te^{-t} \right|_{0}^{+\infty}-\int_{0}^{+\infty}e^{-t}dt\right)=\frac{1}{\lambda}
		\end{split}
	\end{equation*}
	For the variance:
	\begin{equation*}
		\begin{split}
			\sigma_{X}^2&=E(X^2)-\mu^2_{X}=\int_{-\infty}^{+\infty}x^2f(x)dx-\frac{1}{\lambda ^2}=\int_{0}^{+\infty}x^2\lambda e^{-\lambda x}dx-\frac{1}{\lambda ^2}=\frac{1}{\lambda^2}\int_{0}^{+\infty}(\lambda x)^2e^{-\lambda x}d(\lambda x)-\frac{1}{\lambda^2}\\
			&=\frac{1}{\lambda^2}\int_{0}^{+\infty}t^2e^{-t}dt-\frac{1}{\lambda^2}=\frac{-1}{\lambda^2}\int_{0}^{+\infty}t^2d(e^{-t})-\frac{1}{\lambda^2}=\frac{-1}{\lambda^2}\left(\left . t^2e^{-t} \right|_{0}^{+\infty}-2\int_{0}^{+\infty}te^{-t}dt\right)-\frac{1}{\lambda^2}\\
			&=\frac{2}{\lambda^2}-\frac{1}{\lambda^2}=\frac{1}{\lambda^2}
		\end{split}
	\end{equation*}
\end{proof}
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{29.jpg}
	\caption{Illustration of Exponential distribution}
	\label{Figure 29}
\end{figure}
From the illustration, now we can see that from 0:00 on December 1st (denoted $O$), we have to wait $x_{1}$ days until the first rain. The random variable $X_{1}$ is defined as \textbf{the waiting time} for the first rain \textbf{since the origin}: $$X_{1}\sim\mathscr{E}(x_{1};\lambda x_{1})$$
Right after the first rain, we have to wait $x_{2}$ days until the second rain. The random variable $X_{2}$ is defined as \textbf{the waiting time} for the second rain \textbf{since the first rain}. Similarly, we see that: $$X_{2}\sim\mathscr{E}(x_{2},\lambda x_{2})$$
The random variables $X_{1}$, $X_{2}$, $X_{3},\cdots$ are \textbf{statistically independent} since they do not have any relationship to the others.
\\ \\ \\
\indent Now to answer our second question: "If the first 3 days are not rainy, what is the probability of it raining within the next 4 days?", you should note that this is a classic conditional probability problem:
$$P(X<7|X>3)=1-P(X>7|X>3)=1-\frac{P(X>7)}{P(X>3)}=1-\frac{e^{-7.\frac{2}{31}}}{e^{-3.\frac{2}{31}}}=1-e^{-4.\frac{2}{31}}=0.2274$$
Generally, we can see the \textbf{memoryless property} of \textbf{exponential distribution}: $$P(X>x+x_{0}|X>x)=\frac{P(X>x+x_{0})}{P(X>x)}=\frac{e^{-\lambda (x+x_{0})}}{e^{-\lambda x}}=e^{-\lambda x_{0}}=P(X>x_{0})$$

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{30.jpg}
	\caption{The pdf of Exponential distribution and its memoryless property}
	\label{Figure 30}
\end{figure}
\begin{theorem} If $X\sim\mathscr{E}(x;\lambda x)$, then it has \textbf{memoryless property}:
	$$P(X>x+x_{0}|X>x)=P(X>x_{0})$$
\end{theorem}
\subsection{Gamma Distribution}
\subsubsection{Gamma Function}
Before exploring the \textbf{Gamma Distribution}, let me introduce you to an odd but very powerful function:
\begin{definition}
	For every $\alpha>0$, the \textbf{gamma function} is defined by:
	$$\Gamma(\alpha)=\int_{0}^{+\infty}x^{\alpha-1}e^{-x}dx$$
\end{definition}
From now on, the \textbf{gamma function} will appear almost everywhere in ProbStat, so you should learn it carefully.
\begin{theorem} For every $\alpha>0$, the \textbf{gamma function} can be determined through a \textbf{recursion}:
	$$\Gamma(\alpha)=(\alpha-1)\Gamma(\alpha-1)$$
\end{theorem}
\begin{proof}
	\begin{equation*}
		\begin{split}
			\Gamma(\alpha)&=\int_{0}^{+\infty}x^{\alpha-1}e^{-x}dx=-\int_{0}^{+\infty}x^{\alpha-1}d(e^{-x})=-\left(x^{\alpha-1}e^{-x}\left.\right|_{0}^{+\infty}-(\alpha-1)\int_{0}^{+\infty}x^{\alpha-2}e^{-x}dx\right)\\
			&=(\alpha-1)\int_{0}^{+\infty}x^{\alpha-2}e^{-x}dx=(\alpha-1)\Gamma(\alpha-1)
		\end{split}
	\end{equation*}
\end{proof}
\begin{corollary} $$\Gamma(1)=1$$
\end{corollary}
\begin{proof}
	$$\Gamma(1)=\int_{0}^{+\infty}x^{0}e^{-x}dx=\int_{0}^{+\infty}e^{-x}dx=1$$
\end{proof}
\begin{corollary} For any positive integer $n$:
	$$\Gamma(n)=(n-1)!$$
\end{corollary}
\begin{proof} For any positive integer $n$:
	$$\Gamma(n)=(n-1)\Gamma(n-1)=(n-1)(n-2)\Gamma(n-2)=(n-1)(n-2)\cdots2.\Gamma(1)=(n-1)!$$
\end{proof}
\begin{corollary}
	$$\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$$
\end{corollary}
\begin{proof}
	$$\Gamma\left(\frac{1}{2}\right)=\int_{0}^{+\infty}\frac{e^{-x}}{\sqrt{x}}dx$$
	Change variable by setting $t=\sqrt{x}\Rightarrow 2tdt=dx$:
	$$\Gamma\left(\frac{1}{2}\right)=\int_{0}^{+\infty}\frac{e^{-t^2}}{t}2tdt=2\int_{0}^{+\infty}e^{-t^2}dt=\sqrt{\pi}$$
\end{proof}
Using recursion property of Gamma function, we can deduce several useful results:
$$\Gamma\left(\frac{3}{2}\right)=\frac{1}{2}\Gamma\left(\frac{1}{2}\right)=\frac{\sqrt{\pi}}{2}$$
$$\Gamma\left(\frac{5}{2}\right)=\frac{3}{2}\Gamma\left(\frac{3}{2}\right)=\frac{3\sqrt{\pi}}{4}$$
\subsubsection{Gamma Distribution}
The pdf of Gamma distribution is a little bit messy, so we focus on understanding how it can be applied rather than deriving from scratch.
\begin{definition}
	The continuous random variable $X$ has a \textbf{gamma distribution}, with 2 parameters $\alpha$ and $\beta$ are both positive, if its pdf is given by:
	\begin{equation*}
		\mathscr{G}(x;\alpha,\beta)=
		\begin{cases}
			\frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha-1}e^{-x/\beta}\quad(x>0) \\
			0\quad(\text{elsewhere})                                                 \\
		\end{cases}
	\end{equation*}
\end{definition}
\begin{corollary}
	If $X\sim\mathscr{G}(x;\alpha,\beta)$, then:
	\begin{equation*}
		\begin{split}
			\mu_{X}&=\alpha\beta\\
			\sigma^2_{X}&=\alpha\beta^2\\
		\end{split}
	\end{equation*}
\end{corollary}
\begin{proof} For the mean:
	\begin{equation*}
		\begin{split}
			\mu_{X}&=\int_{-\infty}^{+\infty}xf(x)dx=\int_{0}^{+\infty}\frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha}e^{-x/\beta}dx=\alpha\beta\int_{0}^{+\infty}\frac{1}{\Gamma(\alpha+1)\beta^{\alpha+1}}x^{\alpha}e^{-x/\beta}dx\\
			&=\alpha\beta\int_{0}^{+\infty}\mathscr{G}(x;\alpha+1,\beta)dx=\alpha\beta
		\end{split}
	\end{equation*}
	For the variance:
	\begin{equation*}
		\begin{split}
			\sigma^2_{X}&=E(X^2)-\mu^2_{X}=\int_{-\infty}^{+\infty}x^2f(x)dx-\mu^2_{X}=\int_{0}^{+\infty}\frac{1}{\Gamma(\alpha)\beta^{\alpha}}x^{\alpha+1}e^{-x/\beta}dx-(\alpha\beta)^2\\
			&=(\alpha+1)\alpha\beta^2\int_{0}^{+\infty}\frac{1}{\Gamma(\alpha+2)\beta^{\alpha+2}}x^{\alpha+1}e^{-x/\beta}dx-(\alpha\beta)^2\\
			&=(\alpha+1)\alpha\beta^2\int_{0}^{+\infty}\mathscr{G}(x;\alpha+2,\beta)dx-(\alpha\beta)^2\\
			&=(\alpha+1)\alpha\beta^2-\alpha^2\beta^2=\alpha\beta^2
		\end{split}
	\end{equation*}
\end{proof}
Exponential distribution is just a \textbf{special case} of Gamma distribution, you shoud notice that: $$\mathscr{E}(x;\lambda x)=\mathscr{G}\left(x;1,\frac{1}{\lambda}\right)$$
So what does it mean? The relationship between them can be described through $2$ parameters $\alpha$ and $\beta$, with $\alpha$ as the \textbf{number of Poisson events} that you want to observe in a time interval, $\beta$ is just the \textbf{inverse} of $\lambda$. For example, if $X$ is the random variabe, defined as the \textbf{waiting time until the second rain} from the \textbf{beginning of December} (on average, there are $2$ rainy days in December), then:
$$X\sim\mathscr{G}\left(x;2,\frac{31}{2}\right)$$
The probability of there are \textbf{at least} $2$ rainy days within the first $10$ days in December is:
$$P(X<10)=\int_{0}^{10}\frac{1}{\Gamma(2)\left(31/2\right)^2}xe^{-2x/31}dx=0.1369$$
\subsection{Chi-Squared Distribution}
Chi-Squared distribution is a special case of Gamma distribution, where $v$ is a positive integer: $$\alpha=\frac{v}{2},\;\beta=2$$
You will understand its vital role in \textbf{Statistics} in the next chapters.
\begin{definition} The continuous random variable $X$ has a \textbf{chi-squared distribution}, with $v$ \textbf{degrees of freedom}, if its pdf is given by:
	\begin{equation*}
		\mathscr{C}(x;v)=\begin{cases}
			\frac{1}{2^{v/2}\Gamma\left(v/2\right)}x^{v/2-1}e^{-x/2}\quad(x>0) \\
			0\quad(\text{elsewhere})                                           \\
		\end{cases}
	\end{equation*}
	where $v$ is a positive integer.
\end{definition}
\begin{corollary}
	If $X\sim\mathscr{C}(x;v)$, then:
	\begin{equation*}
		\begin{split}
			\mu_{X}&=v\\
			\sigma^2_{X}&=2v\\
		\end{split}
	\end{equation*}
\end{corollary}
\begin{proof} For the mean: $$\mu_{X}=\alpha\beta=v$$  For the variance:
	$$\sigma^2_{X}=\alpha\beta^2=2v$$
\end{proof}
\chapter{Functions of Random Variables}
From \textbf{Chapter 3: Mathematical Expectation}, we discovered many cool properties of \textbf{mean} and \textbf{variance} of random variables. For example, given $2$ \textbf{independent}  random variables $X_{1}\sim\mathscr{P}(x_{1};\mu_{1})$ and $X_{2}\sim\mathscr{P}(x_{2};\mu_{2})$, if we define new random variable $Y=X_{1}+X_{2}$, we can confidently conclude: $$\mu_{Y}=\mu_{X_{1}}+\mu_{X_{2}}=\mu_{1}+\mu_{2}$$
$$\sigma^2_{Y}=\sigma_{X_{1}}^2+\sigma^2_{X_{2}}=\mu_{1}^2+\mu_{2}^2$$
But we have not answered the question "How is the random variable $Y$ distributed?". Is $Y$ distributed \textbf{normally}, or \textbf{exponentially}, or neither? The only information we have known so far is just \textbf{expected value} and \textbf{variance} of $Y$. In this chapter, we will find an answer for our question; and I think it should not be skipped (like many other textbooks) just because there is too much Calculus. All of results derived here plays vital roles in \textbf{Statistics}.
\section{Transformations of Variables}
\subsection{Linear Transformations}
Suppose that the discrete random variable $X\sim f(x)$, and we want to find the pdf of $Y\sim g(y)$ if we knew both $X$ and $Y$ have a linear relationship: $Y=u(X)$. How can we do that? Firstly, we find the \textbf{inverse function} of $u(X)$: $$X=w(Y)$$
Now from the definition of pdf, we have:
$$g(y)=P(Y=y)=P(X=w(y))=f(w(y))$$
\begin{theorem} Suppose that $X$ is a \textbf{discrete} random variable with pdf $f(x)$. Let $Y=u(X)$ define a \textbf{linear transformation} so that its \textbf{inverse function} $X=w(Y)$ can be found; then the pdf of $Y$ is: $$g(y)=f(w(y))$$
	If $X$ is a \textbf{continuous} random variable with pdf $f(x)$, then:
	$$g(y)=f(w(y))|w'(y)|$$
\end{theorem}
A very common mistake is forgetting the $|w'(y)|$ term. To avoid it, you can treat $|w'(y)|$ role like $dx$ as the differential part of integration and remember it is always \textbf{positive}.
\\ For example, if $X\sim\mathscr{B}(x;10,0.5)$ and $Y=u(X)=X+3$, then the inverse function of $u(X)$ is: $$X=w(Y)=Y-3$$
Finally, we can conclude: $$g(y)=f(w(y))=\mathscr{B}(y-3;10,0.5)=\binom{10}{y-3}0.5^{y-3}0.5^{13-y}\quad (y=3,4,5,\cdots,13)$$
Here is a harder example, if $X\sim\mathscr{U}(x;0,1)$ and $Y=u(X)=-2\ln{X}$, then the inverse function of $u(X)$ is: $$X=w(Y)=e^{-Y/2}$$
Applying directly formula yields:
$$g(y)=f(w(y))|w'(y)|=\mathscr{U}(e^{-y/2};0,1)\left|\frac{-1}{2}e^{-y/2}\right|=\frac{1}{2}e^{-y/2}\mathscr{U}(e^{-y/2};0,1)$$
From the definition of \textbf{uniform distribution}:
\begin{equation*}
	\mathscr{U}(e^{-y/2};0,1)=\begin{cases}
		1\quad(0<e^{-y/2}<1)     \\
		0\quad(\text{elsewhere}) \\
	\end{cases}
\end{equation*}
The interval $(0<e^{-y/2}<1)$ is equivalent to $y>0$, we can conclude:
\begin{equation*}
	g(y)=\begin{cases}
		\frac{1}{2}e^{-y/2}\quad(y>0) \\
		0\quad(\text{elsewhere})      \\
	\end{cases} = \mathscr{C}(y;2)
\end{equation*}
Now we have a powerful tool to prove rigorously the relationship between $X$ and $Z$.
\begin{corollary} If $X\sim\mathscr{N}(x;\mu_{X},\sigma^2_{X})$ and the random variable $Z$ is defined as: $$Z=\frac{X-\mu_{X}}{\sigma_{X}}$$ then: $$Z\sim\mathscr{N}(z;0,1)$$
\end{corollary}
\begin{proof}
	$$Z=u(X)=\frac{X-\mu_{X}}{\sigma_{X}}\Rightarrow X=w(Z)=\sigma_{X}Z+\mu_{X}$$
	\begin{equation*}
		\begin{split}
			g(z)&=f(w(z))|w'(z)|=\frac{1}{\sqrt{2\pi}\sigma_{X}}\text{exp}\left(-\frac{z^2}{2}\right)\sigma_{X}=\frac{1}{\sqrt{2\pi}}\text{exp}\left(-\frac{z^2}{2}\right)=\mathscr{N}(z;0,1)
		\end{split}
	\end{equation*}
\end{proof}
In the case of \textbf{joint pdf}, we can also use the theorem as follows:
\begin{theorem} Suppose that $X_{1}$ and $X_{2}$ are \textbf{discrete} random variable with joint pdf $f(x_{1},x_{2})$. Let $Y_{1}=u_{1}(X_{1},X_{2})$ and $Y_{2}=u_{2}(X_{1},X_{2})$ define a \textbf{linear transformation} so that their \textbf{inverse functions} can be found:
	$$X_{1}=w_{1}(Y_{1},Y_{2});\;X_{2}=w_{2}(Y_{1},Y_{2})$$
	then the joint pdf of $(Y_{1},Y_{2})$ is:
	$$g(y_{1},y_{2})=f(w_{1}(y_{1},y_{2}),w_{2}(y_{1},y_{2}))$$
	If $(X_{1},X_{2})$ are \textbf{continuous} random variables with joint pdf $f(x_{1},x_{2})$, then:
	$$g(y_{1},y_{2})=f(w_{1}(y_{1},y_{2}),w_{2}(y_{1},y_{2}))|J|$$
	where the Jacobian is the $2\times2$ determinant:
	\begin{equation*}
		J=\begin{vmatrix}
			\frac{\partial x_{1}}{\partial y_{1}} &  & \frac{\partial x_{1}}{\partial y_{2}} \\
			\frac{\partial x_{2}}{\partial y_{1}} &  & \frac{\partial x_{2}}{\partial y_{2}} \\
		\end{vmatrix}
	\end{equation*}
\end{theorem}
Jacobian determinants knowledge belongs to Calculus 2, so I do not want to explain in detail here. You can think the role of the Jacobian simply as a transformation between coordinates. \textbf{Theorem 6.1.2} can yield several significant results.
\\ For example, let's find out the answer for our question from the beginning of this chapter: if $X_{1}\sim\mathscr{P}(x_{1};\mu_{1})$ and $X_{2}\sim\mathscr{P}(x_{2};\mu_{2})$ are $2$ independent random variables, what is the pdf of $Y=X_{1}+X_{2}$? We define $2$ new functions $u_{1}(X_{1},X_{2}),u_{2}(X_{1},X_{2})$ and determine their inverse functions:
$$Y_{1}=u_{1}(X_{1},X_{2})=X_{1}+X_{2};\;Y_{2}=u_{2}(X_{1},X_{2})=X_{2}$$
Like random variables, the functions $u(X_{1},X_{2})$ can be arbitrarily chosen, as long as they are \textbf{linear}; and we should choose the \textbf{smartest} and \textbf{most convenient} choices. The inverse functions of $u(X_{1},X_{2})$ are:
$$X_{1}=w_{1}(Y_{1},Y_{2})=Y_{1}-Y_{2};\;X_{2}=w_{2}(Y_{1},Y_{2})=Y_{2}$$
Since $X_{1}$ and $X_{2}$ are \textbf{independent}, we can conclude:
$$f(x_{1},x_{2})=f(x_{1})f(x_{2})=\frac{e^{-\mu_{1}}\mu_{1}^{x_{1}}}{x_{1}!}.\frac{e^{-\mu_{2}}\mu_{2}^{x_{2}}}{x_{2}!}=\frac{e^{-(\mu_{1}+\mu_{2})}\mu_{1}^{x_{1}}\mu_{2}^{x_{2}}}{x_{1}!x_{2}!}$$
Now we obtain the joint pdf of $g(y_{1},y_{2})$:
\begin{equation*}
	\begin{split}
		g(y_{1},y_{2})=f(w_{1}(y_{1},y_{2}),w_{2}(y_{1},y_{2}))=f(y_{1}-y_{2},y_{2})=\frac{e^{-(\mu_{1}+\mu_{2})}\mu_{1}^{y_{1}-y_{2}}\mu_{2}^{y_{2}}}{(y_{1}-y_{2})!y_{2}!}
	\end{split}
\end{equation*}
The pdf of $Y_{1}$ is a \textbf{marginal distribution function} of $g(y_{1},y_{2})$:
\begin{equation*}
	\begin{split}
		h(y_{1})&=\sum_{y_{2}}g(y_{1},y_{2})=\frac{e^{-(\mu_{1}+\mu_{2})}}{y_{1}!}\sum_{y_{2}=0}^{y_{1}}\frac{y_{1}!}{(y_{1}-y_{2})!y_{2}!}\mu_{1}^{y_{1}-y_{2}}\mu_{2}^{y_{2}}=\frac{e^{-(\mu_{1}+\mu_{2})}}{y_{1}!}\sum_{y_{2}=0}^{y_{1}}\binom{y_{1}}{y_{2}}\mu_{1}^{y_{1}-y_{2}}\mu_{2}^{y_{2}}\\
		&=\frac{e^{-(\mu_{1}+\mu_{2})}}{y_{1}!}(\mu_{1}+\mu_{2})^{y_{1}}=\mathscr{P}(y_{1};\mu_{1}+\mu_{2})
	\end{split}
\end{equation*}
Our final conclusion is: $$(X_{1}+X_{2})\sim\mathscr{P}(x_{1}+x_{2};\mu_{1}+\mu_{2})$$.
Now if $X_{1}\sim\mathscr{E}(x_{1},1)$, $X_{2}\sim\mathscr{E}(x_{2},1)$ are $2$ independent random variables, what are the pdf of $Y_{1}$ and $Y_{2}$ if they are given by:
$$Y_{1}=X_{1}+X_{2}$$ $$Y_{2}=\frac{X_{1}}{X_{1}+X_{2}}$$
Similarly, we define 2 new functions:
$$Y_{1}=u_{1}(X_{1},X_{2})=X_{1}+X_{2}$$ $$Y_{2}=u_{2}(X_{1},X_{2})=\frac{X_{1}}{X_{1}+X_{2}}$$
The inverse of them are:
$$X_{1}=w_{1}(Y_{1},Y_{2})=Y_{1}Y_{2}$$ $$X_{2}=w_{2}(Y_{1},Y_{2})=Y_{1}-Y_{1}Y_{2}$$
Jacobian matrix and its determinant is:
\begin{equation*}
	J=\begin{vmatrix}
		\frac{\partial x_{1}}{\partial y_{1}} &  & \frac{\partial x_{1}}{\partial y_{2}} \\
		\frac{\partial x_{2}}{\partial y_{1}} &  & \frac{\partial x_{2}}{\partial y_{2}} \\
	\end{vmatrix}=\begin{vmatrix} y_{2}&y_{1}\\ 1-y_{2} & -y_{1}\\ \end{vmatrix}=-y_{1}
\end{equation*}
Since $X_{1}$ and $X_{2}$ are \textbf{statistically independent}, the joint pdf of them is:
$$f(x_{1},x_{2})=f(x_{1})f(x_{2})=e^{-(x_{1}+x_{2})}$$
The joint pdf $g(y_{1},y_{2})$ is:
\begin{equation*}
	\begin{split}
		g(y_{1},y_{2})&=f(w_{1}(y_{1},y_{2}),w_{2}(y_{1},y_{2}))|J|
		=f(y_{1}y_{2},y_{1}-y_{1}y_{2})y_{1}=e^{-y_{1}}y_{1}\\
	\end{split}
\end{equation*}
Notice that $y_{1}>0$ and $1>y_{2}>0$, the pdf $Y_{1}$ and $Y_{2}$ are \textbf{marginal distribution function} of $g(y_{1},y_{2})$ with respect to $y_{1}$ and $y_{2}$:
\begin{equation*}
	\begin{split}
		h_{1}(y_{1})&=\int_{0}^{1}g(y_{1},y_{2})dy_{2}=\int_{0}^{1}e^{-y_{1}}y_{1}dy_{2}=y_{1}e^{-y_{1}}=\mathscr{G}(y_{1};2,1)\\
		h_{2}(y_{2})&=\int_{0}^{+\infty}g(y_{1},y_{2})dy_{1}=\int_{0}^{+\infty}e^{-y_{1}}y_{1}dy_{1}=1=\mathscr{U}(y_{2};0,1)
	\end{split}
\end{equation*}
Our final results are: $$Y_{1}\sim\mathscr{G}(y_{1};2,1)$$ $$Y_{2}\sim\mathscr{U}(y_{2};0,1)$$
In this final example, you must \textbf{be careful} when determining the interval for random variables. Suppose $X_{1}$ and $X_{2}$ are both continuous random variables and their pdf is not a special case like our previous examples.
\begin{equation*}
	f(x_{1},x_{2})=\begin{cases}
		24x_{1}x_{2}\quad(0\leq x_{1},x_{2}\leq 1;x_{1}+x_{2}\leq 1) \\
		0\quad(\text{elsewhere})                                     \\
	\end{cases}
\end{equation*}
$Y_{1}$ and $Y_{2}$ are defined as:
$$Y_{1}=u_{1}(X_{1},X_{2})=X_{1}+X_{2};\;Y_{2}=u_{2}(X_{1},X_{2})=X_{2}$$
Their inverse functions are:
$$X_{1}=w_{1}(Y_{1},Y_{2})=Y_{1}-Y_{2};\;X_{2}=w_{2}(Y_{1},Y_{2})=Y_{2}$$
Jacobian matrix and its determinant is:
\begin{equation*}
	J=\begin{vmatrix}
		\frac{\partial x_{1}}{\partial y_{1}} &  & \frac{\partial x_{1}}{\partial y_{2}} \\
		\frac{\partial x_{2}}{\partial y_{1}} &  & \frac{\partial x_{2}}{\partial y_{2}} \\
	\end{vmatrix}=\begin{vmatrix} 1&-1\\ 0&1\\ \end{vmatrix}=1
\end{equation*}
The joint pdf $g(y_{1},y_{2})$ is:
\begin{equation*}
	\begin{split}
		g(y_{1},y_{2})&=f(w_{1}(y_{1},y_{2}),w_{2}(y_{1},y_{2}))|J|
		=f(y_{1}-y_{2},y_{2}).1=24(y_{1}-y_{2})y_{2}
	\end{split}
\end{equation*}
By replacing $x_{1}=y_{1}-y_{2}$ and $x_{2}=y_{2}$, we obtain new region:
$$(0\leq x_{1},x_{2} \leq 1,x_{1}+x_{2}\leq 1)\to\begin{cases} y_{1}-1\leq y_{2}\leq y_{1}\\ 0\leq y_{2}\leq 1\\ y_{1}\leq 1\end{cases}$$
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=15cm]{31.jpg}
	\caption{Changing the domain region}
	\label{Figure 31}
\end{figure}
By looking at the graph, now you can see that $0\leq y_{2}\leq y_{1}$, and it is not so easy to see this bounded area if you do not plot it. Now the pdf of $Y_{1}$ is:
$$h(y_{1})=\int_{-\infty}^{+\infty}g(y_{1},y_{2})dy_{2}=\int_{0}^{y_{1}}24(y_{1}-y_{2})y_{2}dy_{2}=4y_{1}^3$$
Since $0\leq y_{1}\leq 1$, now you can conclude the pdf of $Y_{1}=X_{1}+X_{2}$ is:
\begin{equation*}
	Y_{1}\sim h(y_{1})=\begin{cases} 4y_{1}^3 \quad(0\leq y_{1}\leq 1)\\ 0\quad\text{(elsewhere)}\\ \end{cases}
\end{equation*}
\subsection{Non-linear Transformations}
\begin{theorem} Suppose that $X$ is a \textbf{continuous} random variable with pdf $f(x)$. Let $Y=u(X)$ define a \textbf{non-linear} transformation. If the interval over which $X$ is defined can be partitioned into $k$ mutually disjoint sets such that each of the \textbf{inverse} functions:
	$$X_{1}=w_{1}(Y),\;X_{2}=w_{2}(Y),\cdots,X_{k}=w_{k}(Y)$$
	of $Y=u(X)$ defines a \textbf{linear} correspondence, then the pdf of $Y$ is:
	$$g(y)=\sum_{i=1}^{k}f(w_{i}(y))\left|w'_{i}(y)\right|$$
\end{theorem}
Wow, this theorem seems a bit difficult at first, but its idea is very clear. Let me explain through an example:
\\ Assume that you know the pdf of $X$ is:
$$X\sim f(x)=\begin{cases} \frac{2(x+1)}{9}\quad(-1<x<2) \\
		0\quad\text{(elsewhere)}      \\
	\end{cases}
$$
Now you want to determine the pdf of $Y=X^2$. Obviously you know the relationship between $X$ and $Y$ is \textbf{non-linear}; and using the same thiking line, you define the function $Y=u(X)$ and find its inverse function:
$$Y=u(X)=X^2$$
But there is a problem, there are \textbf{more than one} existing inverse functions $w(y)$ at the same time:
$$X=w_{1}(Y)=\sqrt{Y},\;X=w_{2}(Y)=-\sqrt{Y}$$
By partitioning the interval of $X$, you can see that:
\begin{equation*}
	X=\begin{cases}
		\sqrt{Y}\quad(0<X<2)   \\
		-\sqrt{Y}\quad(-1<X<0) \\
	\end{cases}
\end{equation*}
Since $Y=X^2$:
$$(0<X<2)\Rightarrow (0<Y<4)$$
$$(-1<X<0)\Rightarrow (0<Y<1)$$
Inside the $Y$ interval:
\begin{equation*}
	X=\begin{cases}
		\pm\sqrt{Y}\quad(0<Y<1) \\
		\sqrt{Y}\quad(1<Y<4)    \\
	\end{cases}
\end{equation*}
After splitting $Y$ interval into $2$ parts, now we apply our theorem above:
$$g(y)=\sum_{i=1}^{k}f(w_{i}(y))\left|w'_{i}(y)\right|=\frac{1}{2\sqrt{y}}(f(\sqrt{y})+f(-\sqrt{y}))=\frac{2}{9\sqrt{y}}\quad(0<y<1)$$
$$g(y)=\sum_{i=1}^{k}f(w_{i}(y))\left|w'_{i}(y)\right|=\frac{1}{2\sqrt{y}}f(\sqrt{y})=\frac{\sqrt{y}+1}{9\sqrt{y}}\quad(1<y<4)$$
Rewrite our $g(y)$ function in piecewise form:
\begin{equation*}
	g(y)=\begin{cases}
		\frac{2}{9\sqrt{y}}\quad(0<y<1)          \\
		\frac{\sqrt{y}+1}{9\sqrt{y}}\quad(1<y<4) \\
		0\quad\text{(elsewhere)}                 \\
	\end{cases}\end{equation*}
\begin{corollary} If $Z\sim\mathscr{N}(z;0,1)$ and $Y=Z^2$, then: $$Y\sim\mathscr{C}(y;1)$$
\end{corollary}
\begin{proof} This corollary is literally the backbone of \textbf{Statistics}. Now we define our function $u(Z)$ and find its inverse functions $w(Y)$: $$Y=u(Z)=Z^2\Rightarrow Z=w_{1}(Y)=\sqrt{Y},\;Z=w_{2}(Y)=-\sqrt{Y}$$
	Since $Y\in(0,+\infty)$ and $Z\in(-\infty,+\infty)$, similarly, we have:
	\begin{equation*}
		Z=\begin{cases}
			\pm\sqrt{Y}\quad(Y>0)      \\
			\text{undefined}\quad(Y<0) \\
		\end{cases}
	\end{equation*}
	Apply our theorem:
	\begin{equation*}
		\begin{split}
			g(y)&=\sum_{i=1}^{k}f(w_{i}(y))\left|w'_{i}(y)\right|=\frac{1}{2\sqrt{y}}(f(\sqrt{y})+f(-\sqrt{y}))\\
			&=\frac{1}{2\sqrt{y}}.\frac{1}{\sqrt{2\pi}}\left(\text{exp}\left(-\frac{y}{2}\right)+\text{exp}\left(-\frac{y}{2}\right)\right)\\ &=\frac{1}{y^{1/2}}.\frac{1}{\sqrt{2\pi}}\text{exp}\left(-\frac{y}{2}\right)\\
			&=\frac{1}{2^{1/2}\Gamma(1/2)}y^{1/2-1}e^{-y/2}\\&=\mathscr{C}(y;1)
		\end{split}
	\end{equation*}
\end{proof}
\section{Moment-Generating Functions}
\subsection{Definition of Moment-Generating Functions}
Every pdf have their own \textbf{mean} and \textbf{variance}:
$$\mu_{X}=E(X)$$ $$\sigma^2_{X}=E(X^2)-\mu_{X}^2$$
These functions are all characterized by $E(X^r)$ functions, you can think them as human fingerprints. $2$ pdf are the same if and only if all of their $E(X^r)$ fucntions are \textbf{matched}. But directly determining $E(X^r)$ is not so simple, especially with $r\geq 3$, therefore we take advantage of the derivative property of exponential function to develop a powerful tool that can yield desired results almost immediately.
\begin{definition} The $r$th moment about the origin of the random variable $X$ is given:
	$$\mu_{r}'=E(X^r)=\sum_{x}x^{r}f(x)\quad(\text{if $X$ is discrete})$$
	$$\mu_{r}'=E(X^r)= \int_{-\infty}^{+\infty}x^{r}f(x)dx\quad\text{(if $X$ is continuous)}$$
\end{definition}
\begin{definition} The \textbf{moment-generating function} of the random variable $X$ is:
	$$M_{X}(t)=E(e^{tX})=\sum_{x}e^{tx}f(x)\quad\text{(if $X$ is discrete)}$$
	$$M_{X}(t)=E(e^{tX})=\int_{-\infty}^{+\infty}e^{tx}f(x)dx\quad\text{(if $X$ is continuous)}$$
\end{definition}
\begin{theorem} Let $X$ be a random variable with \textbf{moment-generating function} $M_{X}(t)$, then:
	$$\left.\frac{d^{r}M_{X}(t)}{dt^r}\right|_{t=0}=\mu_{r}'$$
\end{theorem}
\begin{proof}
	We only prove the \textbf{discrete case} because the continuous case is similar:
	$$\frac{d^{r}M_{X}(t)}{dt^r}=\frac{d^{r}\left(\sum_{x}e^{tx}f(x)\right)}{dt^r}=\sum_{x}x^{r}e^{tx}f(x)$$
	After setting $t=0$, finally we obtain:
	$$\left.\sum_{x}x^{r}e^{tx}f(x)\right|_{t=0}=\sum_{x}x^{r}f(x)=\mu_{r}'$$
\end{proof}
\subsection{Some Useful Moment-Generating Functions}
\subsubsection{Discrete Probability Distribution}
\begin{corollary} If $X\sim\mathscr{B}(x;n,p)$, then: $$M_{X}(t)=(pe^{t}+q)^{n}$$
	\begin{proof}
		$$M_{X}(t)=\sum_{x}e^{tx}f(x)=\sum_{x=0}^{n}\binom{n}{x}(e^{t}p)^{x}q^{n-x}=(pe^{t}+q)^{n}$$
	\end{proof}
\end{corollary}
\begin{corollary} If $X\sim\mathscr{P}(x;\mu)$, then:
	$$M_{X}(t)=\text{exp}(\mu(e^{t}-1))$$
\end{corollary}
\begin{proof}
	\begin{equation*}
		\begin{split}
			M_{X}(t)&=\sum_{x}e^{xt}f(x)=\sum_{x=0}^{n}e^{xt}.\frac{e^{-\mu}\mu^{x}}{x!}\\&=e^{-\mu}\sum_{x=0}^{n}\frac{(e^{t}\mu)^{x}\text{exp}(-e^{t}\mu)}{x!}\text{exp}(e^{t}\mu)\\&=\text{exp}(\mu(e^{t}-1))\sum_{x=0}^{n}\mathscr{P}(x;\mu e^{t})\\&=\text{exp}(\mu(e^{t}-1))
		\end{split}
	\end{equation*}
\end{proof}
\begin{corollary} If $X\sim\mathscr{G}^*(x;p)$, then:
	$$M_{X}(t)=\frac{pe^{t}}{1-qe^{t}}$$
\end{corollary}
\begin{proof}
	\begin{equation*}
		\begin{split}
			M_{X}(t)&=\sum_{x}e^{xt}f(x)=\sum_{x=1}^{+\infty}e^{xt}pq^{x-1}=pe^{t}\sum_{x=1}^{+\infty}(e^{t}q)^{x-1}\\
			&=pe^{t}\frac{1}{1-qe^{t}}=\frac{pe^{t}}{1-qe^{t}}\\
		\end{split}
	\end{equation*}
	Notice the convergence condition of geometric sum is: $|qe^{t}|<1$
\end{proof}
\subsubsection{Continuous Probability Distribution}
\begin{corollary} If $X\sim\mathscr{N}(x;\mu_{X},\sigma^2_{X})$, then:
	$$M_{X}(t)=\text{exp}\left(\mu_{X} t+\frac{1}{2}\sigma_{X}^2 t^2\right)$$
\end{corollary}
\begin{proof}
	\begin{equation*}
		\begin{split}
			M_{X}(t)&=\int_{-\infty}^{+\infty}e^{xt}f(x)dx=\frac{1}{\sqrt{2\pi}\sigma_{X}}\int_{-\infty}^{+\infty}e^{xt}\text{exp}\left(-\frac{(x-\mu_{X})^2}{2\sigma_{X}^2}\right)dx\\
			&=\frac{1}{\sqrt{2\pi}\sigma_{X}}\int_{-\infty}^{+\infty}\text{exp}\left(xt-\frac{(x-\mu_{X})^2}{2\sigma_{X}^2}\right)dx\\
			&=\frac{1}{\sqrt{2\pi}\sigma_{X}}\int_{-\infty}^{+\infty}\text{exp}\left(\frac{-(x-(\mu_{X}+t\sigma_{X}^2))^2+2(\mu_{X}t\sigma_{X}^2+t^2\sigma_{X}^4)}{2\sigma_{X}^2}\right)dx\\ &=\text{exp}\left(\mu_{X}t+\frac{1}{2}t^2\sigma_{X}^2\right)\frac{1}{\sqrt{2\pi}\sigma_{X}}\int_{-\infty}^{+\infty}\text{exp}\left(\frac{-(x-(\mu_{X}+t\sigma_{X}))^2}{2\sigma_{X}^2}\right)dx\\
			&=\exp\left(\mu_{X}t+\frac{1}{2}\sigma_{X}^{2}t^2\right)\int_{-\infty}^{+\infty}\mathscr{N}(x;\mu_{X}+t\sigma_{X},\sigma_{X}^2)dx\\
			&=\exp\left(\mu_{X}t+\frac{1}{2}\sigma_{X}^{2}t^2\right)
		\end{split}
	\end{equation*}
\end{proof}
\begin{corollary} If $X\sim\mathscr{C}(x;v)$, then:
	$$M_{X}(t)=(1-2t)^{-v/2}$$
\end{corollary}
\begin{proof}
	\begin{equation*}
		\begin{split}
			M_{X}(t)&=\int_{-\infty}^{+\infty}e^{xt}f(x)dx=\int_{-\infty}^{+\infty}e^{xt}\frac{1}{2^{v/2}\Gamma(v/2)}x^{v/2-1}e^{-x/2}dx\\
			&=\frac{1}{2^{v/2}\Gamma(v/2)}\int_{-\infty}^{+\infty}x^{v/2-1}\text{exp}\left(-\frac{x}{2/(1-2t)}\right)dx\\
			&=\frac{(1-2t)^{-v/2}}{(2/(1-2t))^{v/2}\Gamma(v/2)}\int_{-\infty}^{+\infty}x^{v/2-1}\text{exp}\left(-\frac{x}{2/(1-2t)}\right)dx\\
			&=(1-2t)^{-v/2}\int_{-\infty}^{+\infty}\mathscr{G}\left(x;\frac{v}{2},\frac{2}{1-2t}\right)dx\\
			&=(1-2t)^{-v/2}
		\end{split}
	\end{equation*}
\end{proof}
\subsection{Linear Combinations of Random Variables}
Moment-Generating function is an extremely powerful tool, not only for finding moments, but also can be applied to prove several significant theorems related to linear combinations of random variables, like \textbf{Central Limit Theorem}. But in this subsection we only focus on $4$ main results, which are directly deduced from moment-generating function; all of them are the foundation for the \textbf{Statistics}.
\begin{theorem} $X$ and $Y$ are 2 random variables with moment-generating functions $M_{X}(t)$ and $M_{Y}(t)$, respectively, if: $$M_{X}(t)=M_{Y}(t)\quad(\forall t)$$
	then $X$ and $Y$ have the same pdf.
\end{theorem}
Since proving rigorously is beyond the scope of this book, and perhaps it is not necessary because this theorem is too obvious. Like I said before, if $2$ random variables have the same moments ("fingerprints"), certainly they share the same pdf.
\begin{theorem} If $X_{1},X_{2},\cdots,X_{n}$ are independent random variables with moment-generating functions $M_{X_{1}}(t),M_{X_{2}}(t),\cdots,M_{X_{n}}(t)$, respectively, and $Y=X_{1}+X_{2}+\cdots+X_{n}$, then:
	$$M_{Y}(t)=M_{X_{1}}(t)M_{X_{2}}(t)\cdots M_{X_{n}}(t)$$
\end{theorem}
\begin{proof} It's very easy to see that:
	\begin{equation*}
		\begin{split}
			M_{Y}(t)&=E(e^{tY})=E(e^{t(X_{1}+X_{2}+\cdots+X_{n})})=E(e^{tX_{1}}e^{tX_{2}}\cdots e^{tX_{n}})\\
			&=E(e^{tX_{1}})E(e^{tX_{2}})\cdots E(e^{tX_{n}})=M_{X_{1}}(t)M_{X_{2}}(t)\cdots M_{X_{n}}(t)\\
		\end{split}
	\end{equation*}
\end{proof}
\begin{corollary} If $X_{1}, X_{2},\cdots,X_{n}$ are independent random variables having \textbf{normal distributions} with \textbf{means} $\mu_{X_{1}},\mu_{X_{2}},\cdots,\mu_{X_{n}}$ and \textbf{variances} $\sigma^2_{X_{1}},\sigma^2_{X_{2}},\cdots,\sigma^2_{X_{n}}$, respectively, then the random variable $Y=X_{1}+X_{2}+\cdots+X_{n}$ has a \textbf{normal distribution}.
\end{corollary}
\begin{proof}
	The moment-generating function of $Y$ is:
	\begin{equation*}
		\begin{split}
			M_{Y}(t)&=M_{X_{1}}(t)M_{X_{2}}(t)\cdots M_{X_{n}}(t)\\
			&=\text{exp}\left(\mu_{X_{1}}t+\frac{1}{2}\sigma^2_{X_{1}}t^2\right)\text{exp}\left(\mu_{X_{2}}t+\frac{1}{2}\sigma^2_{X_{2}}t^2\right)\cdots\text{exp}\left(\mu_{X_{n}}t+\frac{1}{2}\sigma^2_{X_{n}}t^2\right)\\
			&=\text{exp}\left((\mu_{X_{1}}+\mu_{X_{2}}+\cdots\mu_{X_{n}})t+\frac{1}{2}(\sigma^2_{X_{1}}+\sigma_{X_{2}}^2+\cdots+\sigma^2_{X_{n}})t^2\right)
		\end{split}
	\end{equation*}
	Because $M_{Y}(t)$ has a normal distribution form, so we conclude $Y$ has a \textbf{normal distribution}.
\end{proof}
\begin{corollary} If $X_{1}, X_{2},\cdots,X_{n}$ are independent random variables having \textbf{chi-squared distributions} with $v_{1},v_{2},\cdots,v_{n}$ \textbf{degrees of freedom}, then the random variable $Y=X_{1}+X_{2}+\cdots+X_{n}$ has a \textbf{chi-squared distribution}.
\end{corollary}
\begin{proof}
	The moment-generating function of $Y$ is:
	\begin{equation*}
		\begin{split}
			M_{Y}(t)&=M_{X_{1}}(t)M_{X_{2}}(t)\cdots M_{X_{n}}(t)\\
			&=(1-2t)^{-v_{1}/2}(1-2t)^{-v_{2}/2}\cdots(1-2t)^{-v_{n}/2}\\
			&=(1-2t)^{-(v_{1}+v_{2}+\cdots+v_{n})/2}
		\end{split}
	\end{equation*}
	Because $M_{Y}(t)$ has a chi-squared distribution form, so we conclude $Y$ has a \textbf{chi-squared distribution} with $v=v_{1}+v_{2}+\cdots+v_{n}$ degrees of freedom.
\end{proof}
\chapter{Fundamentals of Statistics}
Going back to the 19th and 20th centuries, all the brilliant minds in the field of Biology, such as George Mendel or Ronald Fisher, were masters of Statistics. This is no coincidence, because they not only had to conduct experiments, but also had to process the result. Have you ever wondered how Mendel could derive his theory of heredity just from growing peas? Of course, he could not observe all the peas on Earth; he could only observe some of the peas in his garden. He had to use \textbf{rules of statistical inference} to draw conclusions from the \textbf{sample set} (his garden) to the entire \textbf{population} (the Earth). But now, what if you repeat his experiment again in your own garden and do not get the same result? Is something wrong? The answer is more complex that that. We have to consider many factors:
\begin{itemize}
	\item First, are your seeds good and purebred quality?
	\item Second, are you sure that your seeds were randomly selected? Otherwise, your result would be meaningless.
	\item Ultimately, what is the probability of that result occurring? If it is less than $5\%$, you can ignore it as an exception.
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[width=6cm]{M.png}
	\caption{Why was Mendel so certain about $3:1$ ratio?}
	\label{Figure 90}
\end{figure}
\newpage
\section{The Big Picture of Statistics}
\subsection{Populations and Samples}
The germination rate of mung bean seeds in considered to be $p=0.8$, and germination occurs within $3-5$ days. How do you know if that is true or not? Like Mendel did, we have to perform an experiment to test it. After buying a packet of mung bean seeds in the agriculture store, you randomly choose $6$ \textbf{samples}, with each containing exactly $10$ seeds. Place each sample on a separate petri dish, lined with wet tissue inside, and avoid placing them in direct sunlight.
\begin{figure}[h]
	\centering
	\includegraphics[width=15cm]{32.jpg}
	\caption{Sampling process}
	\label{Figure 32}
\end{figure}
\\Our goal is instead of testing all seeds in the packet (\textbf{population}), we observe $6$ petri dishes \textbf{samples} with each containing $10$ seeds (\textbf{observations}) and draw conclusions based on our observed data. \begin{definition} A \textbf{population} consists of the totality of the \textbf{observations} with which we are concerned.
\end{definition}
\begin{definition} A \textbf{sample} is a \textbf{subset} of a population.
\end{definition}
Conceptually, you can relate these terms with sample space, events and sample points.
\begin{equation*}
	\begin{split}
		\text{Sample Points}& \subseteq \text{Event}\subseteq\text{Sample Space}\\
		\text{Observations}&\subseteq\text{Sample}\subseteq\text{Population}\\
	\end{split}
\end{equation*}
Now consider a dish and define the random variable $X$ as germination time of a single seed. There are $10$ seeds per dish, so $X_{1},X_{2},\cdots X_{10}$ are the germination time of the 1st, 2nd,$\cdots$,10th seed. Since they are \textbf{independent} random variables, each having the same pdf $f(x)$, so we can deduce:
$$f(x_{1},x_{2},\cdots,x_{10})=f(x_{1})f(x_{2})\cdots f(x_{10})$$
\begin{definition} Let $X_{1},X_{2}\cdots X_{n}$ be $n$ \textbf{independent} random variables, each having the same pdf $f(x)$. The \textbf{sample} they belong to can also be called a \textbf{random sample} of size $n$ from the \textbf{population} $f(x)$, and has the joint pdf:
	$$f(x_{1},x_{2},\cdots,x_{n})=f(x_{1})f(x_{2})\cdots f(x_{n})$$
\end{definition}
\subsection{Sample Mean and Sample Variance}
After $5$ waiting days and with the aid of time-lapsed camera, you can collect $60$ values of $X$, but now let's take a look at a single dish (\textbf{sample}) with $10$ values of $X_{i}$.
\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		$i$     & $1$    & $2$    & $3$    & $4$    & $5$    & $6$    & $7$    & $8$    & $9$   & $10$   \\
		\hline
		$x_{i}$ & $4.66$ & $4.27$ & $4.29$ & $4.96$ & $3.49$ & $3.04$ & $2.87$ & $3.53$ & $5.0$ & $2.67$ \\
		\hline
	\end{tabular}
\end{center}
Now we define several important quantities, which describe measures of sample such as \textbf{sample mean}, \textbf{sample variance},$\cdots$
\begin{definition} The \textbf{sample mean} of a sample set containing $n$ observations is defined as:
	$$\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$$
\end{definition}
The \textbf{mean} of our sample is:
$$\overline{x}=\frac{1}{10}\sum_{i=1}^{10}x_{i}=3.878$$
\begin{definition} The \textbf{sample variance} of a sample set containing $n$ observations is defined as:
	$$S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^2$$
\end{definition}
The \textbf{variance} of our sample is:
$$s^2=\frac{1}{10-1}\sum_{i=1}^{10}(x_{i}-\overline{x})^2=0.757$$
The reason of using $n-1$ as a divisor rather than $n$ will be explained in the next chapter. Since calculating the \textbf{sample variance} directly is quiet easy to make miscalculations, so in many cases we always apply this theorem belows:
\begin{theorem} The \textbf{sample variance} of a sample set containing $n$ observations is:
	$$S^{2}=\frac{1}{n(n-1)}\left(n\sum_{i=1}^{n}X^{2}_{i}-\left(\sum_{i=1}^{n}X_{i}\right)^{2}\right)$$
\end{theorem}
\begin{proof} By definition:
	\begin{equation*}
		\begin{split}
			S^{2}&=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^2=\frac{1}{n-1}\left(\sum_{i=1}^{n}X_{i}^2-2\overline{X}\sum_{i=1}^{n}X_{i}+n\overline{X}^2\right)\\
			&=\frac{1}{n-1}\left(\sum_{i=1}^{n}X_{i}^2-\frac{1}{n}\left(\sum_{i=1}^{n}X_{i}\right)^2\right)\\
			&=\frac{1}{n(n-1)}\left(n\sum_{i=1}^{n}X_{i}^2-\left(\sum_{i=1}^{n}X_{i}\right)^2\right)
		\end{split}
	\end{equation*}
\end{proof}
Applying directly this theorem, now the \textbf{variance} of our sample is:
$$s^2=\frac{1}{9.10}\left(10\sum_{i=1}^{10}x_{i}^2-\left(\sum_{i=1}^{10}x_{i}\right)^2\right)=0.757$$
You should note that $\overline{X}$ and $S$ are $2$ random variables constituting from a \textbf{random sample}, and they are typical examples of \textbf{statistics}.
\begin{definition}
	Any function of the random variables constituting a \textbf{random sample} is called a \textbf{statistic}.
\end{definition}
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{33.jpg}
	\caption{Inferring from $(\overline{x},s^2)\to(\mu,\sigma^2)$}
	\label{Figure 33}
\end{figure}
After conducting the experiment, you obtain $6$ values for $\overline{x}$ and $s^2$. But how do you process them? Do they belong to any probability distribution, or are they simply random numbers? These questions will be answered in the remainder of this chapter!
\section{Sampling Distribution of Means}
\subsection{Central Limit Theorem}
Before delving into the general case, let's consider a special case of Central Limit Theorem (CLT for short) first.
\begin{theorem} (Special case of CLT) If $\overline{X}$ is the \textbf{mean} of a \textbf{random sample} of size $n$ from a \textbf{normal population} with mean $\mu$ and variance $\sigma^2$, then the pdf of $\overline{X}$ is:
	$$\overline{X}\sim\mathscr{N}\left(\overline{x};\mu,\frac{\sigma^2}{n}\right)$$
\end{theorem}
\begin{proof} $\overline{X}$ is just a \textbf{linear combinations} of $X_{1},X_{2},\cdots,X_{n}$:
	$$\overline{X}=\frac{1}{n}(X_{1}+X_{2}+\cdots+X_{n})$$
	Since all of $X_{i}$ are normally distributed, from our previous results from \textbf{Chapter 3: Mathematical Expectation} and \textbf{Chapter 6: Functions of Random Variables}, we know the random varibale $\overline{X}$ has a \textbf{normal distribution} with its \textbf{mean} and \textbf{variance}:
	\begin{equation*}
		\begin{split}
			\mu_{\overline{X}}&=\frac{1}{n}(\mu+\mu+\cdots+\mu)=\frac{1}{n}.n\mu=\mu\\
			\sigma^{2}_{\overline{X}}&=\frac{1}{n^2}(\sigma^2+\sigma^2+\cdots+\sigma^2)=\frac{1}{n^2}.n\sigma^2=\frac{\sigma^2}{n}\\
		\end{split}
	\end{equation*}
	Finally, we conclude: $$\overline{X}\sim\mathscr{N}\left(\overline{x};\mu,\frac{\sigma^2}{n}\right)$$
\end{proof}
\begin{theorem} (General case of CLT) If $\overline{X}$ is the \textbf{mean} of a \textbf{random sample} of size $n$ from an \textbf{arbitrary population} with mean $\mu$ and variance $\sigma^2$, then the pdf of $\overline{X}$ is:
	$$\overline{X}\sim\mathscr{N}\left(\overline{x};\mu,\frac{\sigma^2}{n}\right)$$
\end{theorem}
This theorem is so amazing, it states that every pdf will \textbf{converge} to a \textbf{normal distribution}, regardless of its origin. This is the true power of CLT: the \textbf{normal distribution} is the \textbf{ultimate goal} of everything and  not a magical distrbution which drops from the sky.
\begin{proof} Proving this theorem is not so easy, but we will go step-by-step. Assume that the random variables $X_{1},X_{2},\cdots,X_{n}$ have their moments-generating functions $M_{X_{1}}(t),M_{X_{2}}(t),\cdots ,M_{X_{n}}(t)$. Since $n\overline{X}$ is just a linear combinations of $X_{i}$ and they are in the same \textbf{population}:
	$$\overline{X}=\frac{1}{n}(X_{1}+X_{2}+\cdots X_{n})\Rightarrow M_{n\overline{X}}(t)=(M_{X}(t))^n$$
	We restrict our constrains $t\approx 0$ and $n\to +\infty$ to use Taylor-Maclaurin approximation:
	\begin{equation*}
		\begin{split}
			(M_{X}(t))^{n}&=\left(\sum_{k=0}^{+\infty}\frac{d^{k}M_{X}(0)}{k!}t^{k}\right)^{n}\\
			&=\left(1+\mu t+\frac{1}{2}\mu'_{2}t^{2}+\cdots\right)^{n}\\
			&\approx\left(1+\mu t+\frac{\sigma^{2}+\mu^2}{2}t^{2}\right)^{n}\\
			&\approx\left(1+\mu t+\frac{\sigma^{2}t^{2}}{2}\right)^{n}\quad\quad\left(\text{because }\mu t>>\frac{1}{2}\mu^{2}t^{2}\right)\\
			&\approx\left(\text{exp}\left(\mu t+\frac{\sigma^{2}t^{2}}{2}\right)\right)^{n}\\
			&=\text{exp}\left(n\mu t+\frac{n\sigma^{2}t^{2}}{2}\right)
		\end{split}
	\end{equation*}
	From the moments-generating function property:
	$$M_{n\overline{X}}(t)=E(e^{nt\overline{X}})=M_{\overline{X}}(nt)$$
	Recall that the moment-generating fucntion of \textbf{normal distribution} is:
	$$\text{exp}\left(\mu t+\frac{\sigma^{2}t^{2}}{2}\right)$$
	If: $$\overline{X}\sim\mathscr{N}\left(\overline{x};\mu,\frac{\sigma^2}{n}\right)$$ then we can deduce: $$M_{\overline{X}}(t)=\text{exp}\left(\mu t+\frac{\sigma^{2}t^{2}}{2n}\right)\Rightarrow M_{\overline{X}}(nt)=\text{exp}\left(n\mu t+\frac{\sigma^{2}t^{2}n^{2}}{2n}\right)=\text{exp}\left(n\mu t+\frac{n\sigma^{2}t^{2}}{2}\right)=(M_{X}(t))^{n}$$
	Finally, we obtain our result is: $$\overline{X}\sim\mathscr{N}\left(\overline{x};\mu,\frac{\sigma^{2}}{n}\right)
\end{proof}
You should note that the condition of our approximation is $n\to+\infty$. In practice, we always choose $n>30$ as the criterion for using CLT. You can clearly see that we \textbf{can not use CLT} to process our data since each dish (sample) just has $10$ seeds (observations) inside and $n=10<30$.
\begin{corollary} If
	$$\overline{X}\sim\mathscr{N}\left(\overline{x};\mu,\frac{\sigma^2}{n}\right)$$
	then: $$Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\sim\mathscr{N}(z;0,1)$$
\end{corollary}
\subsection{t-Distribution}
We have to change our strategy since our number of observations is pretty low (just $10$ seeds per dish). Before developing a new tool, let me introduce some important results which play crucial roles first.
\begin{corollary} If $X_{1},X_{2},\cdots,X_{n}$ have \textbf{normal distribution} with the \textbf{same} mean and variance, then the sum:
	$$\sum_{i=1}^{n}\left(\frac{X_{i}-\mu}{\sigma}\right)^{2}$$
	has \textbf{chi-squared distribution} with $n$ degrees of freedom.
\end{corollary}
\begin{proof} We have already known that if the random variable $Z$ is defined as:
	$$Z_{i}=\frac{X_{i}-\mu}{\sigma}$$
	then $Z^2\sim\mathscr{C}(z;1)$. Using the \textbf{linear property} of \textbf{chi-squared distribution}, you can conclude the sum:
	$$\sum_{i=1}^{n}Z_{i}^2=\sum_{i=1}^{n}\left(\frac{X_{i}-\mu}{\sigma}\right)^{2}$$
	also has \textbf{chi-squared distribution} with $n$ degrees of freedom.
\end{proof}
\begin{theorem} If $S^2$ is the \textbf{variance} of a random sample of size $n$ taken from a \textbf{normal population} having the variance $\sigma^2$, then the \textbf{statistic}:
	$$\chi^{2}=\frac{S^{2}(n-1)}{\sigma^{2}}=\sum_{i=1}^{n}\left(\frac{X_{i}-\overline{X}}{\sigma}\right)^2\sim\mathscr{C}(\chi^{2};n-1)$$
\end{theorem}
\begin{proof} From the sum:
	\begin{equation*}
		\begin{split}
			\sum_{i=1}^{n}(X_{i}-\mu)^2&=\sum_{i=1}^{n}(X_{i}-\overline{X}+\overline{X}-\mu)^2\\
			&=\sum_{i=1}^{n}(X_{i}-\overline{X})^2+2\sum_{i=1}^{n}(X_{i}-\overline{X})(\overline{X}-\mu)+\sum_{i=1}^{n}(\overline{X}-\mu)^2\\
			&=\sum_{i=1}^{n}(X_{i}-\overline{X})^2+\sum_{i=1}^{n}(\overline{X}-\mu)^2\\
			\Rightarrow \sum_{i=1}^{n}\left(\frac{X_{i}-\mu}{\sigma}\right)^2&=\sum_{i=1}^{n}\left(\frac{X_{i}-\overline{X}}{\sigma}\right)^2+\sum_{i=1}^{n}\left(\frac{\overline{X}-\mu}{\sigma}\right)^2\\
			&=\sum_{i=1}^{n}\left(\frac{X_{i}-\overline{X}}{\sigma}\right)^2+\left(\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\right)^2\\
			&=\chi^2+\left(\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\right)^2\\
		\end{split}
	\end{equation*}
	From the \textbf{linear property} of chi-squared distribution, we can can conclude the random variable $\chi^2$ has chi-squared distribution with $v=n-1$ degrees of freedom.
\end{proof}
\begin{theorem} Let $X_{1}\sim\mathscr{N}(x_{1};0,1)$, $X_{2}\sim\mathscr{C}(x_{2};v)$ and they are both \textbf{independent}, then:
	$$Y_{1}=\frac{X_{1}}{\sqrt{X_{2}/v}}\sim h(y)=\frac{1}{\sqrt{\pi v}}.\frac{\Gamma((v+1)/2)}{\Gamma(v/2)}\left(1+\frac{y_{1}^2}{v}\right)^{-(v+1)/2}$$
\end{theorem}
\begin{proof}
	Since $X_{1}$ and $X_{2}$ are $2$ \textbf{statisitcally independent} random variables, the joint pdf $f(x_{1},x_{2})$ is:
	\begin{equation*}
		\begin{split}
			f(x_{1},x_{2})&=f(x_{1})f(x_{2})=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x_{1}^2}{2}\right)\frac{1}{2^{v/2}\Gamma(v/2)}x_{2}^{v/2-1}\exp\left(-\frac{x_{2}}{2}\right)\\
			&=\frac{1}{\sqrt{2\pi}}.\frac{1}{2^{v/2}\Gamma(v/2)}\exp\left(-\frac{x_{1}^2+x_{2}}{2}\right)x_{2}^{v/2-1}\\
		\end{split}
	\end{equation*}
	Set $2$ auxiliary random variables $Y_{1}$ and $Y_{2}$ as:
	$$Y_{1}=u_{1}(X_{1},X_{2})=\frac{X_{1}}{\sqrt{X_{2}/v}}$$ $$Y_{2}=u_{2}(X_{1},X_{2})=X_{2}$$
	The inverse functions of $u(X_{1},X_{2})$ are:
	$$X_{1}=w_{1}(Y_{1},Y_{2})=\frac{Y_{1}\sqrt{Y_{2}}}{\sqrt{v}}$$
	$$X_{2}=w_{2}(Y_{1},Y_{2})=Y_{2}$$
	The Jacobian of this transfomation is:
	\begin{equation*}
		J=\begin{vmatrix}
			\frac{\partial x_{1}}{\partial y_{1}} &  & \frac{\partial x_{1}}{\partial y_{2}} \\
			\frac{\partial x_{2}}{\partial y_{1}} &  & \frac{\partial x_{2}}{\partial y_{2}} \\
		\end{vmatrix}=\begin{vmatrix} \sqrt{\frac{y_{2}}{v}}&\frac{y_{1}}{2\sqrt{vy_{2}}}\\ 0&1\\ \end{vmatrix}=\sqrt{\frac{y_{2}}{v}}
	\end{equation*}
	Now the joint pdf of $Y_{1}$ and $Y_{2}$ is:
	\begin{equation*}
		\begin{split}
			g(y_{1},y_{2})&=f(w_{1}(y_{1},y_{2}),w_{2}(y_{1},y_{2}))|J|\\
			&=f\left(\frac{y_{1}\sqrt{y_{2}}}{\sqrt{v}},y_{2}\right).\frac{\sqrt{y_{2}}}{\sqrt{v}}\\
			&=\frac{1}{\sqrt{2\pi}}.\frac{1}{2^{v/2}\Gamma(v/2)}\exp\left(-\frac{1}{2}.\left(\frac{y_{1}^{2}y_{2}}{v}+y_{2}\right)\right)y_{2}^{v/2-1}\frac{\sqrt{y_{2}}}{\sqrt{v}}\\
			&=\frac{1}{\sqrt{2\pi v}2^{v/2}\Gamma(v/2)}\exp\left(-\frac{y_{1}^{2}y_{2}+y_{2}v}{2v}\right).y_{2}^{((v+1)/2)-1}\\
			&=\frac{1}{\sqrt{2\pi v}2^{v/2}\Gamma(v/2)}\exp\left(-\frac{y_{2}}{2v/(y_{1}^2+v)}\right).y_{2}^{((v+1)/2)-1}\\
		\end{split}
	\end{equation*}
	Hence the pdf of $Y_{1}$ is the marginal distribution of $g(y_{1},y_{2})$:
	\begin{equation*}
		\begin{split}
			h(y_{1})&=\int_{-\infty}^{+\infty}g(y_{1},y_{2})dy_{2}
			=\frac{1}{\sqrt{2\pi v}2^{v/2}\Gamma(v/2)}\int_{-\infty}^{+\infty}\exp\left(-\frac{y_{2}}{2v/(y_{1}^2+v)}\right).y_{2}^{((v+1)/2)-1}dy_{2}\\
			&=\frac{1}{\sqrt{2\pi v}2^{v/2}\Gamma(v/2)}\int_{-\infty}^{+\infty}\exp\left(-\frac{y_{2}}{2v/(y_{1}^2+v)}\right).y_{2}^{((v+1)/2)-1}dy_{2}\\
			&=\frac{(2v/(y_{1}^{2}+v))^{(v+1)/2}\Gamma((v+1)/2)}{\sqrt{2\pi v}2^{v/2}\Gamma(v/2)}.\\ &\int_{-\infty}^{+\infty}\frac{1}{(2v/(y_{1}^{2}+v))^{(v+1)/2}\Gamma((v+1)/2)}\exp\left(-\frac{y_{2}}{2v/(y_{1}^2+v)}\right).y_{2}^{((v+1)/2)-1}dy_{2}\\
			&=\frac{(2v/(y_{1}^{2}+v))^{(v+1)/2}\Gamma((v+1)/2)}{\sqrt{2\pi v}2^{v/2}\Gamma(v/2)}.\int_{-\infty}^{+\infty}\mathscr{G}\left(y_{2};\frac{v+1}{2},\frac{2v}{y_{1}^2+v}\right)dy_{2}\\
			&=\frac{(2v/(y_{1}^{2}+v))^{(v+1)/2}\Gamma((v+1)/2)}{\sqrt{2\pi v}2^{v/2}\Gamma(v/2)}\\
			&= \frac{1}{\sqrt{\pi v}}.\frac{\Gamma((v+1)/2)}{\Gamma(v/2)}.\left(\frac{y_{1}^{2}}{v}+1\right)^{-(v+1)/2}
		\end{split}
	\end{equation*}
\end{proof}
\textbf{Theorem 7.2.4} is completely a mess, but its idea is very clear. We can rephrase this theorem to our convenience.
\begin{theorem} Let $Z$ be standard \textbf{normal random variable} and $V$ is a \textbf{chi-squared} random variable with $v$ degrees of freedom and they are \textbf{independent}, then the distribution of the random variable $T$ is given by:
	$$T=\frac{Z}{\sqrt{V/v}}\sim\mathscr{T}(t;v)=\frac{1}{\sqrt{\pi v}}.\frac{\Gamma((v+1)/2)}{\Gamma(v/2)}.\left(\frac{t^{2}}{v}+1\right)^{-(v+1)/2}\quad(-\infty<t<+\infty)$$
	This pdf is known as the \textbf{t-distribution} with $v$ degrees of freedom.
\end{theorem}
Relating to our previous results, we already have:
$$Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\sim\mathscr{N}(z;0,1)$$ $$V=\chi^2=\frac{S^{2}(n-1)}{\sigma^2}\sim\mathscr{C}(v;n-1)$$
Rewriting the theorem above yields an interesting result:
$$T=\frac{Z}{\sqrt{V/v}}=\frac{(\overline{X}-\mu)/(\sigma/\sqrt{n})}{S/\sigma}=\frac{\overline{X}-\mu}{S/\sqrt{n}}\sim\mathscr{T}(t;n-1)$$
\begin{theorem} If $\overline{X}$ is the \textbf{mean} of a \textbf{random sample} of size $n$ from a \textbf{normal population} with mean $\mu$, \textbf{unknown variance} $\sigma^2$, then the pdf of random variable $T$ is:
	$$T=\frac{\overline{X}-\mu}{S/\sqrt{n}}\sim\mathscr{T}(t;n-1)$$
	where $S^2$ is the \textbf{variance} of the sample in which $\overline{X}$ belongs to.
\end{theorem}
Compared to CLT, you can see that in t-distribution \textbf{deriving process}, we do not even have to touch the constraint $n\to +\infty$, or at least $n>30$; so t-distribution is a great tool when dealing with small sample sets where $n<30$. But trade-off with small sample sets is the constraint \textbf{population must be normally distributed}; so t-distribution should not be used if you are not certain if the population is normal.
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{34.jpg}
	\caption{t-distribution and Standard Normal distribution}
	\label{Figure 34}
\end{figure}
\\The relationship between t-distribution and standard normal distribution is remarkable. Both of them have \textbf{bell-shaped} curves, and logically from their usage (large or small sample), you can describe their relationship as useful approximations:
\begin{theorem} If $v\to +\infty$, then:
	$$\mathscr{T}(t;v)\to\mathscr{N}(z;0,1)$$
\end{theorem}
In practice, if $v$ degrees of freedom is greater than $30$ (or equivalent to sample size $n>31$ because $v=n-1$), we can use standard normal instead of t-distribution.
\begin{corollary}
	If a random sample has size $n>31$, then:
	$$t\approx z\Rightarrow s\approx \sigma$$
\end{corollary}
\section{Sampling Distribution of Variances}
After gathering $6$ values of $S^2$ from our experiment, now we wish to know what the probability distribution they belong to. Let me recall an useful result which we have just proved in the previous section:
\begin{corollary} If $S^2$ is the \textbf{variance} of a random sample of size $n$ taken from a \textbf{normal population} having the variance $\sigma^2$, then the statistic:
	$$\chi^{2}=\frac{S^{2}(n-1)}{\sigma^2}\sim\mathscr{C}(\chi^2;n-1)$$
\end{corollary}
The criterion \textbf{normal population} is very important; and our conclusions will not be corrected if it is violated.
\section{Case Study: Seed Germination Time}
It is time to summarize all the theorems that we have developed so far and try applying them as tools in a real experiment. There are $3$ main tools and they can be called as Z-test, t-test and Chi-test, respectively:
\begin{enumerate}
	\item CLT (General case): In a \textbf{large} $(n>30)$ random sample from an \textbf{arbitrary} population with mean $\mu$ and \textbf{known} variance $\sigma^2$:
	      $$\overline{X}\sim\mathscr{N}\left(\overline{x};\mu,\frac{\sigma^2}{n}\right)\Rightarrow Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\sim\mathscr{N}(z;0,1)$$
	\item t-distribution: In a \textbf{small} $(n<30)$ random sample from a \textbf{normal} population with mean $\mu$ and \textbf{unknown} variance $\sigma^2$:
	      $$T=\frac{\overline{X}-\mu}{S/\sqrt{n}}\sim\mathscr{T}(t;n-1)$$
	\item Chi-distribution: In a random sample (size does not matter) from a \textbf{normal} population:
	      $$\chi^{2}=\frac{(n-1)S^{2}}{\sigma^{2}}\sim\mathscr{C}(\chi^{2},n-1)$$
\end{enumerate}
The first step of all statistical problems is always \textbf{choosing our test} (as tool). Now which one should we choose?
\begin{itemize}
	\item[-] Since we know nothing about the distribution of our population $f(x)$, so ensure the accuracy, Z-test is the best option; but our sample size is very small (just $n=10$ seeds per dish), and the population variance $\sigma^2$ is a mystery.
	\item[-] t-test is a potential choice, since it does not require $\sigma^2$ value; but the trade-off is the population must be \textbf{normally distributed}.
	\item[-] Chi-test is a very powerful tool too; but the trade-off is still remaining, the population must be \textbf{normally distributed}.
\end{itemize}
We hit the dead end. All of our testing tools have their own pros and cons; and we can not be absolutely certain for our conclusions.
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=15cm]{35.jpg}
	\caption{Experiment's measured data}
	\label{Figure 35}
\end{figure}
\\Here is our measured data after $5$ days conducting experiment:
\begin{center}
	\begin{tabular}{ |c| c| c|c|c|c|c| }
		\hline
		$i$                & $1$     & $2$     & $3$     & $4$     & $5$     & $6$     \\
		\hline
		$\overline{x}_{i}$ & $3.878$ & $4.127$ & $3.965$ & $3.569$ & $4.256$ & $4.441$ \\
		\hline
		$s^{2}_{i}$        & $0.757$ & $0.69$  & $0.732$ & $0.673$ & $0.871$ & $0.657$ \\
		\hline
	\end{tabular}
\end{center}
Of course ensuring everything correct is very \textbf{impossible}, so \textbf{trading-off} our unknown distribution $f(x)$ with \textbfP{normal assumption} is a promising idea; and we can determine the \textbf{interval} of $\mu$ or $\sigma^{2}$ may be fallen in with up to $95\%$ certainty. As I mentioned before, because of our small sample size, we can not use Z-test; t-test and Chi-test are  the better choices in our situation.
\subsection{Estimating the Mean using t-test}
The key idea of using test statistics is we know exactly the \textbf{interval} where $95\%$ of the $Z$, $t$ or $\chi^{2}$ values are inside. Let me explain via the graph of t-distribution:
\begin{figure}[h]
	\centering
	\includegraphics[width=13cm]{36.jpg}
	\caption{t-distribution with $95\%$ confidence interval}
	\label{Figure 36}
\end{figure}
\\ Let $t_{\alpha}$ represent the $t$ value above which we find an area of $\alpha$. Similarly to others test statistics, same meaning can also be applied for $z_{\alpha}$ and $\chi^{2}_{\alpha}$. So $t_{0.025}$ and $-t_{0.025}$ are the values that we find the \textbf{red area} above is $0.025$ (because $\mathscr{T}(t;v)$ graph is symmetrical). The \textbf{white area} bounded by two ends is $0.95$:
$$-t_{0.025}<T<t_{0.025}$$
Intuitively, you can see that $95\%$ of t-values focusing inside the bounded \textbf{white area}. Back to our problem, now you obtain:
$$-t_{0.025}<T<t_{0.025}\Rightarrow -t_{0.025}<\frac{\overline{X}-\mu}{S/\sqrt{n}}<t_{0.025}\Rightarrow \overline{X}-t_{0.025}\frac{S}{\sqrt{n}}<\mu<\overline{X}+t_{0.025}\frac{S}{\sqrt{n}}$$
After substituting the actual values of $\overline{x}_{i}$ and $s_{i}$, you can find the interval of $\mu$ with up to $95\%$ certainty. It means you can confidently state that the \textbf{actual} $\mu$ value lies somewhere inside our interval with a $5\%$ chance of error. The final problem is determining $t_{0.025}$ value. In practice, instead of using t-distribution formula, we always use t-distribution table for simplicity, which is \textbf{Appendix B}. When searching for desired $t$ value, you must check the parameter $v$ degrees of freedom first to ensure the accuracy. As you can see here, our sample size is $n=10$, so:
$$v=n-1=10-1=9\;\text{(degrees of freedom)}$$
Searching for $t_{0.025}$ value with $v=9$ yields $t_{0.025}=2.262$. Finally we conclude:
$$\overline{x}_{i}-t_{0.025}\frac{s_{i}}{\sqrt{n}}<\mu<\overline{x}_{i}+t_{0.025}\frac{s_{i}}{\sqrt{n}}$$
Substituting each pairs $(\overline{x}_{i},s_{i})$ yields $6$ intervals:
\begin{equation*}
	\begin{split}
		\mu&\in(3.68,4.07)\\
		\mu&\in(3.93,4.31)\\
		\mu&\in(3.77,4.15)\\
		\mu&\in(3.38,3.75)\\
		\mu&\in(4.04,4.46)\\
		\mu&\in(4.25,4.62)\\
	\end{split}
\end{equation*}
Finally, we take average of all the starting and ending points of the intervals:
$$\mu\in(3.841,4.226) \text{ ($95\%$ certainty)}$$
We can conclude that $95\%$ of the seeds germinate within the range of $(3.841,4.226)$ day.
\subsection{Estimating the Variance using Chi-test}
Using similar line of thinking, we have to find the interval that $95\%$ of $\chi^2$ values fall inside. The only difference is $\mathscr{C}(\chi^{2};v)$ is asymmetrical, so you must search for both of $\chi_{0.975}^{2}$ and $\chi_{0.025}^{2}$ in chi-squared distribution table, which is \textbf{Appendix C}.
\begin{figure}[h]
	\centering
	\includegraphics[width=13cm]{37.jpg}
	\caption{Chi-Squared distribution with $95\%$ confidence interval}
	\label{Figure 37}
\end{figure}
\begin{equation*}
	\chi^{2}_{0.975}<\chi^{2}<\chi^{2}_{0.025}\Rightarrow \chi^{2}_{0.975}<\frac{S^{2}(n-1)}{\sigma^{2}}<\chi_{0.025}^{2}\Rightarrow \frac{S^{2}(n-1)}{\chi_{0.025}^{2}}<\sigma^{2}<\frac{S^{2}(n-1)}{\chi_{0.975}^{2}}
\end{equation*}
With $v=9$ degrees of freedom, $\chi_{0.025}^{2}=19.023$ and $\chi_{0.975}^{2}=2.7$. Finally we conclude:
$$\frac{s_{i}^{2}(n-1)}{\chi_{0.025}^{2}}<\sigma^{2}<\frac{s_{i}^{2}(n-1)}{\chi_{0.975}^{2}}$$
Substituting each $s_{i}$ values yields $6$ interval:
\begin{equation*}
	\begin{split}
		\sigma^{2}&\in(0.35,2.52)\\
		\sigma^{2}&\in(0.32,2.30)\\
		\sigma^{2}&\in(0.34,2.44)\\
		\sigma^{2}&\in(0.31,2.24)\\
		\sigma^{2}&\in(0.41,2.90)\\
		\sigma^{2}&\in(0.31,2.19)\\
	\end{split}
\end{equation*}
We take average of all starting points and ending points of the intervals:
$$\sigma^{2}\in(0.34,2.431)\text{ ($95\%$ certainty)}$$
Finally, we can conclude if $f(x)$ is \textbf{normally distributed}, then:
$$f(x)\sim\mathscr{N}(x;\mu,\sigma^2)$$
with $\mu\in(3.841,4.226)$ and $\sigma^{2}\in(0.34,2.431)$ with $95\%$ certainty.
\chapter{Classical Methods of Estimation}
\section{Definition of Unbiased Estimator}
In the previous chapter, we used \textbf{sample mean} $\overline{X}$ to estimate \textbf{population mean} ${\mu}$; and \textbf{sample variance} $S^{2}$ to estimate \textbf{population variance} $\sigma^{2}$. Both $\overline{X}$ and $S^{2}$ are \textbf{unbiased estimator}; intuitively, now we can form the definition of \textbf{unbiased estimator} as follows:
\begin{definition} A statistic $\widehat \Theta$ is said to be an \textbf{unbiased estimator} of the parameter $\theta$ if: $$\mu_{\widehat\Theta}=E(\hat\Theta)=\theta$$
\end{definition}
Mathematically, in a sample set, there are many quantites like \textbf{sample mean} $\overline{X}$, \textbf{sample variance} $S^{2}$, \textbf{proportion} $\widehat P,\cdots$ and logically, we can not arbitrarily use $\overline{X}$ to estimate $\sigma^{2}$. The definition above acts like "key-lock" mechanism to guarantee there will be no ambiguity here.
\begin{theorem} $\overline{X}$ is an \textbf{unbiased estimator} of the parameter $\mu$.
\end{theorem}
\begin{proof} From CLT, we know: $$\overline{X}\sim\mathscr{N}\left(\overline{x};\mu,\frac{\sigma^2}{n}\right)$$
	From the definition of \textbf{unbiased estimator}, we have to show that a statistic $\overline{X}$ has its mean equals to $\mu$:
	$$\mu_{\overline{X}}=E(\overline{X})=\mu$$
	Obviously, it is true.
\end{proof}
\begin{theorem} ${S}^2$ is an \textbf{unbiased estimator} of the parameter $\sigma^{2}$.
\end{theorem}
\begin{proof} From the definition of \textbf{sample variance}, we have
	\begin{equation*}
		\begin{split}
			E(S^{2})&=\frac{1}{n-1}E\left(\sum_{i=1}^{n}({X}_{i}-\overline{X})^{2}\right)\\
			&=\frac{1}{n-1}E\left(\sum_{i=1}^{n}(X_{i}-\mu+\mu-\overline{X})^{2}\right)\\
			&=\frac{1}{n-1}E\left(\sum_{i=1}^{n}(X_{i}-\mu)^{2}+2\sum_{i=1}^{n}(X_{i}-\mu)(\mu-\overline{X})+\sum_{i=1}^{n}(\mu-\overline{X})^{2}\right)\\
			&=\frac{1}{n-1}E\left(\sum_{i=1}^{n}(X_{i}-\mu)^{2}+2(\mu-\overline{X})(n\overline{X}-n\mu)+n(\mu-\overline{X})^{2}\right)\\
			&=\frac{1}{n-1}E\left(\sum_{i=1}^{n}(X_{i}-\mu)^{2}-n(\mu-\overline{X})^{2}\right)\\
		\end{split}
	\end{equation*}
	Since this sum has chi-squared distribution with $n$ degrees of freedom, then: $$E\left(\sum_{i=1}^{n}\left(\frac{X_{i}-\mu}{\sigma}\right)^{2}\right)=n\Rightarrow E\left(\sum_{i=1}^{n}(X_{i}-\mu)^{2}\right)=n\sigma^{2}$$
	And this sum has chi-squared distribution with $1$ degree of freedom:
	$$E\left(\left(\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\right)^{2}\right)=1\Rightarrow E((\overline{X}-\mu)^{2})=\frac{\sigma^{2}}{n}$$
	Finally we have: $$\mu_{S^{2}}=E(S^{2})=\frac{1}{n-1}(n\sigma^{2}-\sigma^{2})=\sigma^{2}$$
\end{proof}
Now you understand \textbf{why we divide by $n-1$ rather than $n$} in sample variance formula to guarantee $S^{2}$ is an unbiased estimator. In this chapter, we define a new statistic quantity $\widehat P$ (proportion) and prove that it is an unbiased estimator.
\begin{definition} A statistic $\widehat P$ is defined as: $$\widehat P=\frac{X}{n}$$ where $X$ is a \textbf{Bernoulli random variable}.
\end{definition}
You can refer to \textbf{Chapter 0.1: Coin Tossing} to read a typical example of the quantity $\widehat P$ in a practical experiment. In \textbf{Statistics}, the terms "probability" and "proportion" can be used interchangeably, depending on the context.
\begin{theorem} $\widehat P$ is an \textbf{unbiased estimator} of the parameter $p$.
\end{theorem}
\begin{proof} Because $X\sim\mathscr{B}(x;n,p)$ we have:
	$$E(\widehat P)=E\left(\frac{X}{n}\right)=\frac{1}{n}E(X)=\frac{np}{n}=p$$
\end{proof}
It is very easy to indicate that:
$$\sigma_{\widehat P}^{2}=\frac{1}{n^{2}}\sigma^{2}_{X}=\frac{npq}{n^{2}}=\frac{pq}{n}$$
\begin{corollary} The statistic $\widehat P$ also has \textbf{Bernoulli distribution} with mean and variance:
	\begin{equation*}
		\begin{split}
			E(\widehat P)&=p\\ \sigma_{\widehat P}^{2}&=\frac{pq}{n}\\
		\end{split}
	\end{equation*}
\end{corollary}
\begin{proof} Obviously the random variable $\widehat P$ has Bernoulli distribution since it is just a scaled version of $X$.
\end{proof}
\begin{corollary} If $n\to+\infty$, then:
	$$Z=\frac{\widehat P-p}{\sqrt{pq/n}}\sim\mathscr{N}(z;0,1)$$
\end{corollary}
The most confusing point you need to be aware of in this chapter is the correct use of these notations $(\widehat\Theta,\hat\theta,\theta)$:
\begin{itemize}
	\item[-] $\widehat\Theta$: Statistic.
	\item[-] $\hat\theta$: A single value of Statistic.
	\item[-] $\theta$: Parameter of population need to be estimated.
\end{itemize}
So far we have just found $3$ typical examples of these triples $(\widehat\Theta,\hat\theta,\theta)$ are $(\overline{X},\overline{x},\mu)$, $(S^{2},s^{2},\sigma^{2})$ and $(\widehat P,\hat p, p)$ are \textbf{unbiased} estimator. In the final chapter of this book, we will prove $2$ more triples sharing the same characteristics are $(B_{0},b_{0},\beta_{0})$ and $(B_{1},b_{1},\beta_{1})$ also \textbf{unbiased} too.
\section{Determining Statistical Intervals Like A Pro: An Ultimate Guide}
\subsection{Selecting Your Test Statistic}
The very first step of all statistical problems is \textbf{selecting test statistic}. To choose the most appropriate test you must deeply understand their uses and \textbf{trade-off}. Remember nothing can be absolutely perfect!
\begin{itemize}
	\item If your sample size is relatively large ($n>30$) and you want to estimate the \textbf{mean} $\mu$, choose Z-test without any doubts.
	\item If your sample size is small ($n<30$) and you want to estimate the \textbf{mean} $\mu$, choose t-test.
	\item Sample size does not matter when you want to estimate the \textbf{variance} $\sigma^2$, choose Chi-test.
	\item If you want to estimate the \textbf{proportion} $p$, your sample size must be \textbf{large}, and choose Z-test.
\end{itemize}
Since all of our testing tools are based on the \textbf{normal population} assumption (except CLT), so you have to be careful when you do not know how your population is distributed. A very common approach is assuming that the entire population follows a \textbf{normal distribution}, sacrificing the precision of $f(x)$ to simplify our problem. From this point onward in this book, \textbf{all populations are assumed to be normal}.
\\ After choosing your desired test among $3$ tests, write down your testing formula based on the distribution of the quantity that you want to estimate, for example:
\begin{equation*}
	\begin{split}
		\overline{X}\sim\mathscr{N}\left(\overline{x};\mu,\frac{\sigma^2}{n}\right)&\Rightarrow Z=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\sim\mathscr{N}(z;0,1)\\
		(\overline{X}_{1}-\overline{X}_{2})\sim\mathscr{N}\left(\overline{x_{1}}-\overline{x_{2}};\mu_{1}-\mu_{2},\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}\right)&\Rightarrow Z=\frac{\overline{X_{1}}-\overline{X_{2}}-(\mu_{1}-\mu_{2})}{\sqrt{(\sigma_{1}^{2}/n_{1})+(\sigma_{2}^{2}/n_{2})}}\sim\mathscr{N}(z;0,1)\\
		\frac{S^{2}(n-1)}{\sigma^2}&=\chi^{2}\sim\mathscr{C}(\chi^{2};n-1)\\
		\frac{\overline{X}-\mu}{S/\sqrt{n}}&=T\sim\mathscr{T}(t;n-1)\\
	\end{split}
\end{equation*}
$Z$, $t$ and $\chi^{2}$ are 3 new \textbf{normalized estimators} $\widehat\Theta$, and we use them to determine confidence interval.
\newpage
\subsection{Establishing The Confidence Interval}
\subsubsection{Two-Sided Confidence Bounds}
The idea of establishing the confidence interval is very simple and intuitive, we did it (but sill do not give it a name) in the previous chapter. Before learning the methods, let me introduce some general notations first:
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{38.jpg}
	\caption{Two-sided confidence bounds}
	\label{Figure 38}
\end{figure}
\\ $\hat\theta_{L}$ and $\hat\theta_{U}$ stand for the lower and upper values of our boundaries; $100(1-\alpha)\%$ is the \textbf{confidence interval}, where \textbf{exact} $100(1-\alpha)\%$ values of $\theta$ inside. Obviously you can see that the value of $\hat\theta$ must be chosen to satisfies the white area it bounded is $1-\alpha$. Because standard normal and t-distribution curves are both symmetrical, so we often choose:
$$\hat\theta_{L}=-t_{\alpha/2},\;\hat\theta_{U}=t_{\alpha/2}\text{ (for t-test)}$$
$$\hat\theta_{L}=-z_{\alpha/2},\;\hat\theta_{U}=z_{\alpha/2}\text{ (for Z-test)}$$
But chi-distribution curve is asymmetrical, you have to be careful here:
$$\hat\theta_{L}=\chi^{2}_{1-\alpha/2},\;\hat\theta_{U}=\chi^{2}_{\alpha/2}\;\text{ (for Chi-test)}$$
General interval has been established with $100(1-\alpha)\%$ certainty:
$$\hat\theta_{L}<\widehat\Theta<\hat\theta_{U}$$
Substituting everything down here yields your final interval, for example:
$$-t_{\alpha/2}<T<t_{\alpha/2}\Rightarrow -t_{\alpha/2}<\frac{\overline{X}-\mu}{s/\sqrt{n}}<t_{\alpha/2}\Rightarrow \overline{X}-t_{\alpha/2}\frac{s}{\sqrt{n}}<\mu<\overline{X}+t_{\alpha/2}\frac{s}{\sqrt{n}}$$
From your actual experiment data, set the random variable $\overline{X}$ be one of your result:
$$\overline{x}-t_{\alpha/2}\frac{s}{\sqrt{n}}<\mu<\overline{x}+t_{\alpha/2}\frac{s}{\sqrt{n}}$$
Using the same line of thinking, you can find every two-sided confidence bounds intervals, with an arbitrary $\alpha$ value. In practice, we often choose $\alpha=0.01,0.05$ or even $\alpha=0.1$. I do not want to give too many formulas here, since they share the same logic, and you can derive them yourself easily. Before moving on to the next part, you should pay attention to this subtle formula:
$$-z_{\alpha/2}<Z<z_{\alpha/2}\Rightarrow -z_{\alpha/2}<\frac{\widehat P-p}{\sqrt{pq/n}}<z_{\alpha/2}\Rightarrow \widehat P-z_{\alpha/2}\sqrt{\frac{pq}{n}}<p<\widehat P+z_{\alpha/2}\sqrt{\frac{pq}{n}}$$
Directly estimating $p$ seems very hard now, because it appears in both sides, but is not impossible (you should try). Since $n\to +\infty$ so we can approx $\hat p\approx p$, then:
$$\widehat P-z_{\alpha/2}\sqrt{\frac{\hat p\hat q}{n}}<p<\widehat P+z_{\alpha/2}\sqrt{\frac{\hat p\hat q}{n}}$$
Set the random variable $\widehat P$ be one of your experimental result:
$$\hat p-z_{\alpha/2}\sqrt{\frac{\hat p\hat q}{n}}<p<\hat p+z_{\alpha/2}\sqrt{\frac{\hat p\hat q}{n}}$$
\subsubsection{One-Sided Confidence Bounds}
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{39.jpg}
	\caption{Lower one-sided bound}
	\label{Figure 39}
\end{figure}
Similarly, you can establish lower one-side bounds interval with some slightly changes:
$$P(\widehat\Theta<\hat\theta_{U})=100(1-\alpha)\%$$
For exmaple, estimating the population variance with $100(1-\alpha)\%$ certainty:
$$\chi^{2}<\chi^{2}_{\alpha}\Rightarrow \frac{S^{2}(n-1)}{\sigma^{2}}<\chi^{2}_{\alpha}\Rightarrow \frac{S^{2}(n-1)}{\chi^{2}_{\alpha}}<\sigma^{2}$$
Substituting one of your sample variance $s$ yields lower one-side bound of $\sigma^{2}$:
$$\frac{s^{2}(n-1)}{\chi^{2}_{\alpha}}<\sigma^{2}$$
\newpage
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{40.jpg}
	\caption{Upper one-sided bound}
	\label{Figure 40}
\end{figure}
$$P(\theta_{L}<\widehat\Theta)=100(1-\alpha)\%$$
\\Using the same steps, we can derive the confidence interval with $100(1-\alpha)\%$ certainty for the mean in a large population:
$$-z_{\alpha}<Z\Rightarrow -z_{\alpha}<\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\Rightarrow \mu<\overline{X}+z_{\alpha}\frac{\sigma}{\sqrt{n}} $$
\begin{comment}
\appendix
\chapter{Areas under the Normal Curve}
\begin{figure}[h]
	\centering
	\includegraphics[width=15cm]{z.png}
	\caption{Areas under the Normal Curve}
	\label{Figure 60}
\end{figure}
\end{comment}
\begin{comment}
\appendix
\chapter{Critical Values of the t-Distribution}
\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{T.jpg}
	\caption{Critical Values of the t-Distribution}
	\label{Figure 61}
\end{figure}
\appendix
\chapter{Critical Values of the Chi-Squared Distribution}
\begin{figure}[h]
	\centering
	\includegraphics[width=15cm]{C.png}
	\caption{Critical Values of the Chi-Squared Distribution}
	\label{Figure 62}
\end{figure}
\end{comment}
\end{document}

